{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRxeI7EOx9k_"
   },
   "source": [
    "# Beat The Bookies: Predicting EPL Matches\n",
    "_Team C_\n",
    "\n",
    "__Mohammad Ali Syed, Abdul Al-Fahim, Dylan Hoi, Henry Chen, Chris Wong & Yolanne Lee__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOcQFZzsgHWK"
   },
   "source": [
    "**Contents:**\n",
    "\n",
    "- [Section 1](#section1): Introduction\n",
    "\n",
    "- [Section 2](#section2): Data Import\n",
    "\n",
    "- [Section 3](#section3): Data Transformation & Exploration\n",
    ">- [Section 3.1](#section31): Initial Data Exploration\n",
    ">>- [Section 3.1.1](#section311): Relationship Between Attributes\n",
    ">>- [Section 3.1.2](#section312): Initial Data Preprocessing\n",
    ">>- [Section 3.1.3](#section313): Training model on entire featureset\n",
    ">>- [Section 3.1.4](#section314): Random Forest Tree for entire featureset\n",
    ">>- [Section 3.1.5](#section315): Training model without Referee\n",
    ">>- [Section 3.1.6](#section316): Random Forest Tree without Referee\n",
    ">>- [Section 3.1.7](#section317): Training model without Date\n",
    ">>- [Section 3.1.8](#section318): Random Forest Tree without Date\n",
    ">>- [Section 3.1.9](#section319): Training model on only in-game stats\n",
    ">>- [Section 3.1.10](#section3110): Visualising selected features\n",
    ">- [Section 3.2](#section32): Priors Feature Construction\n",
    ">>- [Section 3.2.1](#section321): Data Cleaning\n",
    ">>- [Section 3.2.2](#section322): Cumulative Full-time W/L Ratio\n",
    ">>- [Section 3.2.3](#section323): Cumulative Half-time W/L Ratio\n",
    ">>- [Section 3.2.4](#section324): Cumulative Full-Time goals scored\n",
    ">>- [Section 3.2.5](#section325): Cumulative Half-time W/L Ratio\n",
    ">>- [Section 3.2.6](#section326): Previous shots on target\n",
    ">>- [Section 3.2.7](#section327): Computing previous fouls\n",
    ">>- [Section 3.2.8](#section328): Computing previous corners\n",
    ">>- [Section 3.2.9](#section329): Computing previous goals before half-time\n",
    ">>- [Section 3.2.10](#section3210): Compute previous goals after half-time\n",
    ">>- [Section 3.2.11](#section3211): Computing previous goals conceded before half-time\n",
    ">>- [Section 3.2.12](#section3212): Computing previous goals conceded after half-time\n",
    ">>- [Section 3.2.13](#section3213): Matches Played\n",
    ">- [Section 3.3](#section33): Additional Features\n",
    ">>- [Section 3.3.1](#section331): Distance Travelled for Away Teams\n",
    ">>- [Section 3.3.2](#section332): Average shots on goal in the past 3 matches\n",
    ">>- [Section 3.3.3](#section333): WL Performance of past 3 matches\n",
    ">>- [Section 3.3.4](#section334): Cumulative Full Time Goal Difference\n",
    ">>- [Section 3.3.5](#section335): Goalkeeper Stats\n",
    ">- [Section 3.4](#section34): Derive Priors\n",
    ">- [Section 3.5](#section35): Final Data Preprocessing\n",
    ">>- [Section 3.5.1](#section351): Split Data\n",
    ">- [Section 3.6](#section36): Scale Data\n",
    "\n",
    "\n",
    "- [Section 4](#section4): Methodology Overview\n",
    "\n",
    "- [Section 5](#section5): Model Training & Validation\n",
    ">- [Section 5.1](#section51): Base Models\n",
    ">>- [Section 5.1.1](#section511): Gaussian Naive Bayes\n",
    ">>- [Section 5.1.2](#section512): Generic SVM\n",
    ">>- [Section 5.1.3](#section513): Logistic Regression\n",
    ">- [Section 5.2](#section52): Boosting Models\n",
    ">>- [Section 5.2.1](#section521): XGBoost\n",
    ">>- [Section 5.2.2](#section522): AdaBoost\n",
    ">>- [Section 5.2.3](#section523): GradientBoost\n",
    ">>- [Section 5.2.4](#section524): LightGBM\n",
    ">- [Section 5.3](#section53): Neural Network Models\n",
    ">>- [Section 5.3.1](#section531): Vanilla Neural Network\n",
    ">>- [Section 5.3.2](#section532): Deep Neural Network\n",
    ">>- [Section 5.3.3](#section533): Recurrent Neural Network\n",
    ">>- [Section 5.3.4](#section534): Gated Recurrent Neural Network\n",
    ">>- [Section 5.3.5](#section535): Long Short-Term Memory Neural Network\n",
    ">>- [Section 5.3.6](#section536): Convolutional Neural Network\n",
    ">- [Section 5.4](#section54): Time Series Models\n",
    ">>- [Section 5.4.1](#section541): Prophet\n",
    ">>- [Section 5.4.2](#section542): Arima\n",
    "\n",
    "- [Section 6](#section6): Results\n",
    ">- [Section 6.1](#section61): Calculate Train and Test Accuracies\n",
    ">- [Section 6.2](#section62): Accuracy Comparison Plot\n",
    ">>- [Section 6.2.1](#section621): Individual Training Accuracy\n",
    ">>- [Section 6.2.2](#section622): Individual Testing Accuracy\n",
    ">>- [Section 6.2.3](#section623): Training Accuracy Grouped by Model Type\n",
    ">>- [Section 6.2.4](#section624): Testing Accuracy Grouped by Model Type\n",
    ">- [Section 6.3](#section63): Confusion Matrix Plots\n",
    ">- [Section 6.4](#section64): Comparison of Best Models from Each Category\n",
    "\n",
    "\n",
    "- [Section 7](#section7): Final Predictions on Test Set\n",
    ">- [Section 7.1](#section71): Data Cleaning\n",
    ">- [Section 7.2](#section72): Priors Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "<a name='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-EqnUuI8gWm"
   },
   "source": [
    "## 2. Data Import\n",
    "<a name='section2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import seaborn as sns\n",
    "from collections import Counter, deque\n",
    "\n",
    "#!pip install geopy\n",
    "#!pip install sklearn\n",
    "\n",
    "#For Computing Priors\n",
    "from geopy.distance import geodesic \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report,confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "#For Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "\n",
    "#For Model Selection\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#For creating Tensorflow models\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout, SimpleRNN, GRU, LSTM, Conv1D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "# EPL Training Data\n",
    "dirName = 'Data_Files/'\n",
    "filePath = dirName + 'epl-training.csv'\n",
    "data = pd.read_csv(filePath)\n",
    "\n",
    "#EPL Test Dataset\n",
    "dirName = 'Data_Files/'\n",
    "filePath = dirName + 'epl-test.csv'\n",
    "testData = pd.read_csv(filePath)\n",
    "\n",
    "# Additional EPL Training Data\n",
    "# downloaded from www.football-stats.co.uk and concatenated from seasons 2000-2008.\n",
    "# Reformatted to suit our current data architecture, additional 3,047 rows x 22 columns\n",
    "filePath = dirName + 'epl-training-extra.csv'\n",
    "extraData = pd.read_csv(filePath)\n",
    "data = extraData.append(data, ignore_index = True) #append additional data of seasons 2000-2008\n",
    "\n",
    "# Additional EPL Training Data\n",
    "# downloaded from www.football-stats.co.uk and concatenated from seasons 2021.\n",
    "# Reformatted to suit our current data architecture, additional 158 rows x 22 columns\n",
    "filePath = dirName + 'epl-training-updated.csv'\n",
    "updatedData = pd.read_csv(filePath)\n",
    "\n",
    "# Additional EPL Stadium Location Data\n",
    "filePath = dirName + 'epl-stadium.csv'\n",
    "positionalData = pd.read_csv(filePath)\n",
    "\n",
    "# Additional EPL Goalkeeper Data\n",
    "filePath = dirName + 'epl-goalkeeping.csv'\n",
    "GKData = pd.read_csv(filePath)\n",
    "\n",
    "#Remove empty nan columns at the end\n",
    "data = data.iloc[:, 0:22]\n",
    "pd.set_option('display.max_columns', None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b63a_ejYMVK"
   },
   "source": [
    "## 3. Data Transformation & Exploration\n",
    "<a name='section3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "def corr_matrix(X, feature):\n",
    "    corr= X.corr()\n",
    "    corr_y = abs(corr[feature])\n",
    "    highest_corr = corr_y[corr_y >0.2]\n",
    "    highest_corr.sort_values(ascending=True)\n",
    "    return highest_corr\n",
    "\n",
    "def rf_model(X_train, X_test, y_train, y_test):\n",
    "    rf=RandomForestClassifier(random_state = 42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    accuracy = calc_accuracy(preds, y_test)\n",
    "    return rf, preds, accuracy\n",
    "\n",
    "def feat_importances(X_train, rf):\n",
    "    feature_importances = list(zip(X_train, rf.feature_importances_))\n",
    "    feature_importances_ranked = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    return feature_importances_ranked\n",
    "\n",
    "def select_feat(X_train, y_train):\n",
    "    feature_selector = SelectFromModel(RandomForestClassifier(random_state = 42)).fit(X_train, y_train)\n",
    "    selected_feat= X_train.columns[(feature_selector.get_support())]\n",
    "    return selected_feat\n",
    "\n",
    "def calc_accuracy(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds) * 100\n",
    "    return accuracy\n",
    "\n",
    "def rf_tree_visualiser(rf, featuresetName, feature_names):\n",
    "    tree = rf.estimators_[10]  #Take 10th random tree\n",
    "    export_graphviz(tree, out_file = featuresetName + '.dot', feature_names = list(feature_names),\n",
    "                    rounded = True, proportion = False, \n",
    "                    precision = 2, filled = True, max_depth = 3)\n",
    "    call(['dot', '-Tpng', featuresetName + '.dot', '-o', featuresetName + '.png'],shell=True)\n",
    "    return featuresetName + '.png'\n",
    "\n",
    "def scatter(data, title, xlabel, ylabel):\n",
    "    # Assume data is an array of tuples\n",
    "    x, y = zip(*data)\n",
    "    # s is the area of the circles in the plot\n",
    "    plt.scatter(x, y, s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "# https://towardsdatascience.com/stop-one-hot-encoding-your-time-based-features-24c699face2f\n",
    "def transformation(column):\n",
    "    max_value = column.max()\n",
    "    sin_values = [math.sin((2*math.pi*x)/max_value) for x in list(column)]\n",
    "    cos_values = [math.cos((2*math.pi*x)/max_value) for x in list(column)]\n",
    "    return sin_values, cos_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Intial Data Exploration\n",
    "<a name='section31'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################################# Feature Visualisation\n",
    "# # Visualise correlations between different statistics\n",
    "# from pandas.plotting import scatter_matrix\n",
    "\n",
    "# # Sort data by teams\n",
    "# teams = {}\n",
    "# referees = {}\n",
    "# for i in data.groupby('HomeTeam').mean().T.columns:\n",
    "#     teams[i] = []\n",
    "# for i in data.groupby('Referee').mean().T.columns:\n",
    "#     referees[i] = []\n",
    "\n",
    "# # Team Summary Statistics\n",
    "# home_team_stats = pd.DataFrame()\n",
    "# away_team_stats = pd.DataFrame()\n",
    "\n",
    "# teams = pd.unique(data[[\"HomeTeam\"]].values.ravel())\n",
    "\n",
    "# for team in teams:\n",
    "#     # Compute summary stats as home team\n",
    "#     team_stats = data[(data[\"HomeTeam\"] == team)]\n",
    "#     team_stats = team_stats.iloc[:, [3, 6, 10, 12, 14, 16, 18, 20]]\n",
    "#     team_stats = team_stats.sum()\n",
    "\n",
    "#     performance = data[(data[\"HomeTeam\"] == team)].iloc[:, 5]\n",
    "#     num_vals = len(performance)\n",
    "    \n",
    "#     performance = performance.value_counts()\n",
    "#     performance_keys = performance.keys()\n",
    "#     performance_values = performance.values\n",
    "#     performance = zip(performance.keys(), performance.values)\n",
    "    \n",
    "#     for key, value in performance:\n",
    "#         metric = value/num_vals\n",
    "        \n",
    "#         if key == \"H\":\n",
    "#             team_stats[\"Win Rate\"] = metric\n",
    "            \n",
    "#         elif key == \"A\":\n",
    "#             team_stats[\"Lose Rate\"] = metric\n",
    "        \n",
    "#         else:\n",
    "#             team_stats[\"Draw Rate\"] = metric\n",
    "\n",
    "#     home_team_stats[team] = pd.DataFrame(team_stats) ##causing problems\n",
    "\n",
    "#     # Compute summary stats as away team\n",
    "#     team_stats = data[(data[\"AwayTeam\"] == team)]\n",
    "#     team_stats = team_stats.iloc[:, [4, 7, 11, 13, 15, 17, 19, 21]]\n",
    "#     team_stats = team_stats.sum()\n",
    "\n",
    "#     performance = data[(data[\"AwayTeam\"] == team)].iloc[:, 5]\n",
    "#     num_vals = len(performance)\n",
    "\n",
    "#     performance = performance.value_counts()\n",
    "#     performance_keys = performance.keys()\n",
    "#     performance_values = performance.values\n",
    "#     performance = zip(performance.keys(), performance.values)\n",
    "    \n",
    "#     for key, value in performance:\n",
    "#         metric = value/num_vals\n",
    "        \n",
    "#         if key == \"A\":\n",
    "#             team_stats[\"Win Rate\"] = metric\n",
    "            \n",
    "#         elif key == \"H\":\n",
    "#             team_stats[\"Lose Rate\"] = metric\n",
    "        \n",
    "#         else:\n",
    "#             team_stats[\"Draw Rate\"] = metric\n",
    "\n",
    "\n",
    "#     away_team_stats[team] = pd.DataFrame(team_stats)\n",
    "\n",
    "# # Sort by strongest to weakest team, by win rate\n",
    "# home_team_stats = home_team_stats.sort_values(by='Win Rate', axis=1, ascending=False)\n",
    "# away_team_stats = away_team_stats.sort_values(by='Win Rate', axis=1, ascending=False)\n",
    "# home_team_stats\n",
    "# #Interesting to note, Man U ranked lower on every metric except fouls and yellow cards compared to Chelsea but had higher win rate -> could suggest the more aggressive the team, the higher the win rate\n",
    "# # print(home_team_stats.iloc[:, 0])\n",
    "# # print(away_team_stats.iloc[:, 0])\n",
    "# # print(np.array(home_team_stats.iloc[:, 0]) - np.array(away_team_stats.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Relationship Between Attributes\n",
    "<a name='section311'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix between full time goals and other features\n",
    "highest_corr = corr_matrix(data, \"FTHG\")\n",
    "print(\"FTHG: \\n\" + str(highest_corr))\n",
    "\n",
    "highest_corr = corr_matrix(data, \"FTAG\")\n",
    "print(\"FTAG: \\n\" + str(highest_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into input and output data\n",
    "\n",
    "#Output variable\n",
    "y = data.iloc[:, 5:6]\n",
    "#Reformat y to make it suitable for LabelEncoder\n",
    "\n",
    "y = np.array(y).reshape(len(y))\n",
    "# #Encode y\n",
    "# y = LabelEncoder().fit_transform(y) #################this needs to be done separately for train/test\n",
    "\n",
    "#Input variables\n",
    "#Remove give away columns such as goals scored\n",
    "data_filtered = data.drop(labels = data.columns[[3, 4, 5, 6, 7, 8]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Initial Data Preprocessing\n",
    "<a name='section312'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "\n",
    "#Dates\n",
    "data_filtered['Date'] = pd.to_datetime(data_filtered['Date'])\n",
    "#year has been removed as we need to predict future results -> https://towardsdatascience.com/machine-learning-with-datetime-feature-engineering-predicting-healthcare-appointment-no-shows-5e4ca3a85f96\n",
    "year = data_filtered['Date'].dt.year\n",
    "data_filtered['Month'] = data_filtered['Date'].dt.month\n",
    "data_filtered['Week'] = data_filtered['Date'].dt.isocalendar().week\n",
    "data_filtered['Day'] = data_filtered['Date'].dt.day\n",
    "#Extract encoded dates\n",
    "dates_split = data_filtered.iloc[:, 16:19]\n",
    "#Remove encoded dates and original date column\n",
    "data_filtered = data_filtered.drop(labels = data_filtered.columns[[0, 16, 17, 18]], axis = 1)\n",
    "\n",
    "#Encode categorical data\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#Teams\n",
    "home_t = data_filtered.iloc[:, 0:1]\n",
    "home_t = encoder.fit_transform(home_t) #################does this need to be done separately?\n",
    "\n",
    "away_t = data_filtered.iloc[:, 1:2]\n",
    "away_t = encoder.fit_transform(away_t) #################does this need to be done separately?\n",
    "data_filtered = data_filtered.drop(labels = data_filtered.columns[[0,1]], axis = 1)\n",
    "\n",
    "#Referees \n",
    "ref = data_filtered.iloc[:, 0:1]\n",
    "ref = encoder.fit_transform(ref)       #################does this need to be done separately?\n",
    "data_filtered = data_filtered.drop(labels = data_filtered.columns[[0]], axis = 1)\n",
    "\n",
    "#Re-stack columns\n",
    "data_filtered = data_filtered.join(pd.DataFrame(ref.toarray()), rsuffix = '_ref')\n",
    "data_filtered = data_filtered.join(pd.DataFrame(home_t.toarray()), rsuffix = '_home')\n",
    "data_filtered = data_filtered.join(pd.DataFrame(away_t.toarray()), rsuffix = '_away')\n",
    "data_filtered = dates_split.join(data_filtered)\n",
    "data_filtered.columns = data_filtered.columns.astype(str)\n",
    "data_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Training model on entire featureset\n",
    "<a name='section313'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train model on entire featureset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "rf, preds, base_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on entire featureset: \" + str(base_accuracy) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Random Forest Tree for entire featureset\n",
    "<a name='section314'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print rf tree N.B. may not work without importing graphviz, random forest images will be on GitHub\n",
    "Image(filename = rf_tree_visualiser(rf, 'featureSetTree', data_filtered.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Training model without Referee\n",
    "<a name='section315'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model without Referee feature\n",
    "data_filtered_no_ref = data_filtered.iloc[:, 0:15].join(data_filtered.iloc[:, 58:])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_no_ref, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy without Referee: \" + str(accuracy) + \"%\")\n",
    "print(\"Difference from before: \" + str(accuracy - base_accuracy) + \"%\")\n",
    "#Ref is having negative impact so remove\n",
    "data_filtered = data_filtered_no_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 Random Forest Tree without Referee\n",
    "<a name='section316'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print rf tree (no ref)\n",
    "Image(filename = rf_tree_visualiser(rf, 'featureSetTreeNoRef', data_filtered_no_ref.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7 Training model without Date\n",
    "<a name='section317'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model without Date feature\n",
    "data_filtered_no_date = data_filtered.iloc[:, 3:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_no_date, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy without Dates: \" + str(accuracy) + \"%\")\n",
    "print(\"Difference from before: \" + str(accuracy - base_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.8 Random Forest Tree without Date\n",
    "<a name='section318'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print rf tree (no dates)\n",
    "Image(filename = rf_tree_visualiser(rf, 'featureSetTreeNoDate', data_filtered_no_date.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.9 Training model on only in-game stats\n",
    "<a name='section319'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model on only in-game stats to identify most important ones\n",
    "data_filtered_only_game_stats = data_filtered.iloc[:, 3:15]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_only_game_stats, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, all_stats_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on all in-game stats: \" + str(all_stats_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.10 Visualising selected features\n",
    "<a name='section3110'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise and analyse initial results\n",
    "\n",
    "#Display feature importances in descending order\n",
    "feature_importances = feat_importances(X_train, rf)\n",
    "print(\"Feature Importances: \")\n",
    "[print('Feature: {:35} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(\"\\nClassification Report: \")\n",
    "print(classification_report(y_test, preds))\n",
    "#Important note: AF/HF rank higher than HC/AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise feature importance\n",
    "scatter(feature_importances, \"Feature importances\", \"Feature\", \"Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Pearson Correlation Heatmap to see the top 10 features related to the match result FTR\n",
    "\n",
    "def plotGraph(X_all, Y_all):\n",
    "\n",
    "    train_data=pd.concat([X_all,Y_all],axis=1)\n",
    "\n",
    "    #FTR correlation matrix\n",
    "    plt.figure(figsize=(12,12))\n",
    "    k = 11 # number of variables for heatmap\n",
    "    cols = abs(train_data.astype(float).corr()).nlargest(k, 'FTR')['FTR'].index\n",
    "    cm = np.corrcoef(train_data[cols].values.T)\n",
    "    sns.set(font_scale=1.25)\n",
    "    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, cmap=\"Blues\", yticklabels=cols.values, xticklabels=cols.values)\n",
    "    plt.show()\n",
    "\n",
    "attributes = data.drop(['Date','HomeTeam', 'AwayTeam', 'Referee','FTR'],1)\n",
    "attributes['HTR'] = attributes['HTR'].map({'H':1,'A':0,'D':2})\n",
    "label = data['FTR']\n",
    "label = label.map({'H':1,'A':0,'D':2})\n",
    "plotGraph(attributes,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection\n",
    "#change names and display selected features more nicely, ideally with their importance, gini impurity...\n",
    "selected_feat = select_feat(X_train, y_train)\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model on selected in-game stats only\n",
    "indexes = []\n",
    "for feat in selected_feat:\n",
    "    indexes.append(data_filtered_only_game_stats.columns.get_loc(feat))\n",
    "    \n",
    "data_filtered_filtered_game_stats = data_filtered_only_game_stats.iloc[:, indexes]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_filtered_game_stats, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, reduced_stats_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on reduced in-game stats: \" + str(reduced_stats_accuracy) + \"%\")\n",
    "print(\"Difference compared to all in-game stats: \" + str(reduced_stats_accuracy - all_stats_accuracy) + \"%\")\n",
    "\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(\"\\nClassification Report: \")\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation of new featureset/tree\n",
    "data_filtered_filtered_game_stats.plot(kind='hist', subplots=True, sharex=False, sharey=False, bins=50, layout=(2,4), figsize=(12, 6))\n",
    "data_filtered_filtered_game_stats.plot(kind='box', subplots=True, layout=(2,4), sharex=False, sharey=False, figsize=(12, 6))\n",
    "data_filtered_filtered_game_stats.plot(kind='density', subplots=True, layout=(2,4), sharex=False, sharey=False, figsize=(12, 6))\n",
    "Image(filename = rf_tree_visualiser(rf, 'selectedFeatureSetTree', data_filtered_filtered_game_stats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produce new dataset\n",
    "#Fix column names\n",
    "#Restack teams and dates\n",
    "\n",
    "#Original teams are needed to be able to compute priors\n",
    "data_new = data.iloc[:, [1, 2]].join(data_filtered_filtered_game_stats)\n",
    "data_new = dates_split.join(data_new)\n",
    "\n",
    "#Stack previously removed giveaway columns\n",
    "data_new = data_new.join(data.iloc[:, [3, 4, 5, 6, 7, 8]])\n",
    "\n",
    "#Feature engineer second half goals\n",
    "#Second half home goals\n",
    "SHHG = np.array(data.iloc[:, [3]]) - np.array(data.iloc[:, [6]])\n",
    "#Second half away goals\n",
    "SHAG = np.array(data.iloc[:, [4]]) - np.array(data.iloc[:, [7]])\n",
    "data_new['SHHG'] = pd.DataFrame(SHHG)\n",
    "data_new['SHAG'] = pd.DataFrame(SHAG)\n",
    "data_new.columns = data_new.columns.astype(str)\n",
    "data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See if second half goals have significant correlation to total goals\n",
    "highest_corr = corr_matrix(data_new, \"FTHG\")\n",
    "print(\"FTHG: \\n\" + str(highest_corr))\n",
    "\n",
    "highest_corr = corr_matrix(data_new, \"FTAG\")\n",
    "print(\"FTAG: \\n\" + str(highest_corr))\n",
    "#Second half goals do have very strong correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Priors Feature Construction\n",
    "<a name='section32'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Pearson Correlation Heatmap to extract the top 10 features \n",
    "# there are two pairs of data highly correlated (see details in report), \n",
    "# so we just pick [FTHG, FTAG, HS, AS, HR, AR] from the top 10 features,\n",
    "# additionally [Date, HomeTeam, AwayTeam, FTR], to derive our features.\n",
    "selectedAttributes = [\"Date\",\"HomeTeam\", \"AwayTeam\",\"FTR\",\"FTHG\",\"FTAG\",\"HS\",\"AS\",\"HR\",\"AR\"]\n",
    "training_data = data[selectedAttributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Data Cleaning\n",
    "<a name='section321'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive features and remove unwanted data\n",
    "def removeInvalidData(data):\n",
    "\n",
    "    # remove data which contains None\n",
    "    data.dropna(axis=0, how='any',inplace=True)\n",
    "\n",
    "    # remove data which contains NaN, infinite or overflowed number \n",
    "    indices_to_keep = ~data.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    data = data[indices_to_keep]\n",
    "\n",
    "    return data\n",
    "\n",
    "#check if there are rows containing None, NaN, infinite or overflowed values\n",
    "assert data.shape[0] == removeInvalidData(data).shape[0]\n",
    "data = removeInvalidData(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the different date formats and convert the type from str to timestamp  \n",
    "def convertDate(data):\n",
    "    if not isinstance(data.Date[0],str):\n",
    "        return\n",
    "\n",
    "    newDate = []\n",
    "    for _, matchInfo in data.iterrows():\n",
    "        if len(matchInfo.Date) == 8 :\n",
    "            newDate.append(pd.to_datetime(matchInfo.Date, format=\"%d/%m/%y\" ))\n",
    "        elif len(matchInfo.Date) == 9 :\n",
    "            newDate.append(pd.to_datetime(matchInfo.Date, format=\"%d %b %y\" ))  # the date format in test data\n",
    "        elif len(matchInfo.Date) == 10 :\n",
    "            newDate.append(pd.to_datetime(matchInfo.Date, format=\"%d/%m/%Y\" ))\n",
    "    \n",
    "    data['Date'] = pd.Series(newDate).values\n",
    "\n",
    "    return data\n",
    "\n",
    "# converted the date formats for later exploration and transformation\n",
    "# convertDate(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Cumulative Full-time W/L Ratio\n",
    "<a name='section322'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate cumulative Full-Time win-loss ratio for Home/Away teams prior to every match\n",
    "# TODO: Points-based results based on previous wins & losses \n",
    "# PHWL = Previous Home Team Win Loss Ratio\n",
    "# PAWL = Previous Away Team Win Loss Ratio\n",
    "\n",
    "def get_previousFTResults(playing_stat):\n",
    "    \n",
    "    # Create a dictionary with team names as keys\n",
    "    teams = {}\n",
    "    PHWL = []\n",
    "    PAWL = []\n",
    "    \n",
    "    for i in playing_stat.groupby('HomeTeam').mean().T.columns:\n",
    "        teams[i] = [] #Each team gets their own list\n",
    "\n",
    "    # the value corresponding to keys is a list containing the match result\n",
    "    for i in range(len(playing_stat)):\n",
    "        \n",
    "        #list of respective Home/Away team in match\n",
    "        match_ht = teams[playing_stat.iloc[i].HomeTeam]\n",
    "        match_at = teams[playing_stat.iloc[i].AwayTeam]\n",
    "        \n",
    "        #count no. of wins\n",
    "        \n",
    "        h_wins = Counter(match_ht)\n",
    "        a_wins = Counter(match_at)\n",
    "        \n",
    "        #h_wins = no. of home wins\n",
    "        #a_wins = no. of away wins\n",
    "        h_wins = h_wins['W']\n",
    "        a_wins = a_wins['W']\n",
    "        \n",
    "        #append W/L/D to respective teams\n",
    "        \n",
    "        if y[i] == 'H':\n",
    "            match_ht.append('W')\n",
    "            match_at.append('L')\n",
    "        elif y[i] == 'A':\n",
    "            match_at.append('W')\n",
    "            match_ht.append('L')\n",
    "        else:\n",
    "            match_at.append('D')\n",
    "            match_ht.append('D')\n",
    "       \n",
    "        h_wlRatio = h_wins / len(match_ht)\n",
    "        a_wlRatio = a_wins / len(match_at)\n",
    "        \n",
    "        #Home/Away cumulative WL ratios prior to every match\n",
    "        PHWL.append(h_wlRatio)\n",
    "        PAWL.append(a_wlRatio)\n",
    "        \n",
    "    playing_stat.loc[:,'PHWL'] = pd.Series(PHWL)\n",
    "    playing_stat.loc[:,'PAWL'] = pd.Series(PAWL)\n",
    "\n",
    "    return playing_stat\n",
    "\n",
    "#get_previousFTResults(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.2.3 Cumulative Half-time W/L Ratio\n",
    "<a name='section323'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate cumulative Half-Time win-loss ratio for Home/Away teams prior to every match\n",
    "# HHTR = Previous Home Half Time Results\n",
    "# AHTR = Previous Away Half Time Results\n",
    "\n",
    "def get_PreviousHTResults(playing_stat):\n",
    "    \n",
    "    # Create a dictionary with team names as keys\n",
    "    teams = {}\n",
    "    HHTR = []\n",
    "    AHTR = []\n",
    "    \n",
    "    for i in playing_stat.groupby('HomeTeam').mean().T.columns:\n",
    "        teams[i] = [] #Each team gets their own list\n",
    "\n",
    "    # the value corresponding to keys is a list containing the match result\n",
    "    for i in range(len(playing_stat)):\n",
    "        \n",
    "        #list of respective Home/Away team in match\n",
    "        match_ht = teams[playing_stat.iloc[i].HomeTeam]\n",
    "        match_at = teams[playing_stat.iloc[i].AwayTeam]\n",
    "        \n",
    "        #count no. of wins\n",
    "        \n",
    "        h_wins = Counter(match_ht)\n",
    "        a_wins = Counter(match_at)\n",
    "        \n",
    "        #h_wins = no. of home wins\n",
    "        #a_wins = no. of away wins\n",
    "        h_wins = h_wins['W']\n",
    "        a_wins = a_wins['W']\n",
    "        \n",
    "        #append W/L/D to respective teams\n",
    "        \n",
    "        if playing_stat.iloc[i].HTR == 'H':\n",
    "            match_ht.append('W')\n",
    "            match_at.append('L')\n",
    "        elif playing_stat.iloc[i].HTR == 'A':\n",
    "            match_at.append('W')\n",
    "            match_ht.append('L')\n",
    "        else:\n",
    "            match_at.append('D')\n",
    "            match_ht.append('D')\n",
    "            \n",
    "        h_wlRatio = h_wins / len(match_ht)\n",
    "        a_wlRatio = a_wins / len(match_at)\n",
    "       \n",
    "        #Home/Away cumulative WL ratios prior to every match\n",
    "        HHTR.append(h_wlRatio)\n",
    "        AHTR.append(a_wlRatio)\n",
    "        \n",
    "    playing_stat.loc[:,'HHTR'] = pd.Series(HHTR)\n",
    "    playing_stat.loc[:,'AHTR'] = pd.Series(AHTR)\n",
    "\n",
    "    return playing_stat\n",
    "\n",
    "\n",
    "#get_PreviousHTResults(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Cumulative Full-Time goals scored\n",
    "<a name='section324'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Full-Time Cumulative Goal \n",
    "# PHGS = Previous Home Goal Scored\n",
    "# PAGS = Previous Away Goal Scored\n",
    "\n",
    "def getPreviousCumulativeGoals(priorData):\n",
    "    teams = {}\n",
    "    PHGS = [] \n",
    "    PAGS = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        FTHG = priorData.iloc[i]['FTHG']\n",
    "        FTAG = priorData.iloc[i]['FTAG']\n",
    "\n",
    "        try:\n",
    "            pcgs_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pcgs_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcgs_h = 0\n",
    "            pcgs_a = 0\n",
    "\n",
    "        PHGS.append(pcgs_h)\n",
    "        PAGS.append(pcgs_a)\n",
    "#         print(PAGS)\n",
    "#         print(PHGS)\n",
    "        pcgs_h = pcgs_h + FTHG #Home team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pcgs_h)\n",
    "        pcgs_a = pcgs_a + FTAG #Away team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pcgs_a)\n",
    "\n",
    "    priorData.loc[:,'PHGS'] = pd.Series(PHGS)\n",
    "    priorData.loc[:,'PAGS'] = pd.Series(PAGS)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousCumulativeGoals(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Cumulative Half-time W/L Ratio\n",
    "<a name='section325'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Shots in the match\n",
    "# PHS = Home teams previous match Shots, totaled over season\n",
    "# PAS = Away teams previous match Shots, totaled over season\n",
    "\n",
    "def getPreviousShots(priorData):\n",
    "    teams = {}\n",
    "    PHS = [] \n",
    "    PAS = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HS = priorData.iloc[i]['HS']\n",
    "        AS = priorData.iloc[i]['AS']\n",
    "\n",
    "        try:\n",
    "            pcs_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pcs_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcs_h = 0\n",
    "            pcs_a = 0\n",
    "\n",
    "        PHS.append(pcs_h)\n",
    "        PAS.append(pcs_a)\n",
    "        pcs_h = pcs_h + HS #Home team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pcs_h)\n",
    "        pcs_a = pcs_a + AS #Away team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pcs_a)\n",
    "\n",
    "    priorData.loc[:,'PHS'] = pd.Series(PHS)\n",
    "    priorData.loc[:,'PAS'] = pd.Series(PAS)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousShots(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 Previous shots on target\n",
    "<a name='section326'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Shots on Target\n",
    "# PHSOT = Home teams Previous Shots on Target, totaled over season\n",
    "# PASOT = Away teams Previous Shots on Target, totaled over season\n",
    "\n",
    "def getPreviousShotsOnTarget(priorData):\n",
    "    teams = {}\n",
    "    PHSOT = [] \n",
    "    PASOT = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HST = priorData.iloc[i]['HST']\n",
    "        AST = priorData.iloc[i]['AST']\n",
    "\n",
    "        try:\n",
    "            pcsot_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pcsot_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcsot_h = 0\n",
    "            pcsot_a = 0\n",
    "\n",
    "        PHSOT.append(pcsot_h)\n",
    "        PASOT.append(pcsot_a)\n",
    "        pcsot_h = pcsot_h + HST #Home team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pcsot_h)\n",
    "        pcsot_a = pcsot_a + AST #Away team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pcsot_a)\n",
    "\n",
    "    priorData.loc[:,'PHSOT'] = pd.Series(PHSOT)\n",
    "    priorData.loc[:,'PASOT'] = pd.Series(PASOT)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousShotsOnTarget(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7 Computing previous fouls\n",
    "<a name='section327'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Fouls\n",
    "# PHTF = Home teams Previous Fouls, Totaled over season\n",
    "# PATF = Away teams Previous Fouls, Totaled over season\n",
    "\n",
    "def getPreviousTeamFouls(priorData):\n",
    "    teams = {}\n",
    "    PHTF = [] \n",
    "    PATF = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HF = priorData.iloc[i]['HF']\n",
    "        AF = priorData.iloc[i]['AF']\n",
    "\n",
    "        try:\n",
    "            pcf_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pcf_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcf_h = 0\n",
    "            pcf_a = 0\n",
    "\n",
    "        PHTF.append(pcf_h)\n",
    "        PATF.append(pcf_a)\n",
    "        pcf_h = pcf_h + HF #Home team's previous fouls before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pcf_h)\n",
    "        pcf_a = pcf_a + AF #Away team's previous fouls before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pcf_a)\n",
    "\n",
    "    priorData.loc[:,'PHTF'] = pd.Series(PHTF)\n",
    "    priorData.loc[:,'PATF'] = pd.Series(PATF)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousTeamFouls(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.8 Computing previous corners\n",
    "<a name='section328'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Corners\n",
    "# PHTC = Home teams Previous Corners, Totaled over season\n",
    "# PATC = Away teams Previous Corners, Totaled over season\n",
    "\n",
    "def getPreviousTeamCorners(priorData):\n",
    "    teams = {}\n",
    "    PHTC = [] \n",
    "    PATC = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HC = priorData.iloc[i]['HC']\n",
    "        AC = priorData.iloc[i]['AC']\n",
    "\n",
    "        try:\n",
    "            pcc_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pcc_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcc_h = 0\n",
    "            pcc_a = 0\n",
    "\n",
    "        PHTC.append(pcc_h)\n",
    "        PATC.append(pcc_a)\n",
    "        pcc_h = pcc_h + HC #Home team's previous corners before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pcc_h)\n",
    "        pcc_a = pcc_a + AC #Away team's previous corners before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pcc_a)\n",
    "\n",
    "    priorData.loc[:,'PHTC'] = pd.Series(PHTC)\n",
    "    priorData.loc[:,'PATC'] = pd.Series(PATC)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousTeamCorners(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.9 Computing previous goals before half-time\n",
    "<a name='section329'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Goals before half time\n",
    "# PHTHG = Home teams Previous Goals Before Half Time, Totaled over season\n",
    "# PHTAG = Away teams Previous Goals Before Half Time, Totaled over season\n",
    "\n",
    "def getPreviousHalfTimeGoalsScored(priorData):\n",
    "    teams = {}\n",
    "    PHTHG = [] \n",
    "    PHTAG = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HTHG = priorData.iloc[i]['HTHG']\n",
    "        HTAG = priorData.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            pchtg_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pchtg_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pchtg_h = 0\n",
    "            pchtg_a = 0\n",
    "\n",
    "        PHTHG.append(pchtg_h)\n",
    "        PHTAG.append(pchtg_a)\n",
    "        pchtg_h = pchtg_h + HTHG #Home team's previous first half goals scored before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pchtg_h)\n",
    "        pchtg_a = pchtg_a + HTAG #Away team's previous first half goals scored before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pchtg_a)\n",
    "\n",
    "    priorData.loc[:,'PHTHG'] = pd.Series(PHTHG)\n",
    "    priorData.loc[:,'PHTAG'] = pd.Series(PHTAG)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousHalfTimeGoalsScored(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.10 Compute previous goals after half-time\n",
    "<a name='section3210'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Second Half Time Goals in the match\n",
    "# PSHHG = Previous Second Half Time Goals scored by Home team, totaled over season\n",
    "# PSHAG = Previous Second Half Time Goals scored by Away team, totaled over season\n",
    "\n",
    "def getPreviousSecondHalfGoals(priorData):\n",
    "    teams = {}\n",
    "    PSHHG = [] \n",
    "    PSHAG = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "                \n",
    "        FTHG = priorData.iloc[i]['FTHG']\n",
    "        FTAG = priorData.iloc[i]['FTAG']\n",
    "        HTHG = priorData.iloc[i]['HTHG']\n",
    "        HTAG = priorData.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            shg_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            shg_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            shg_h = 0\n",
    "            shg_a = 0\n",
    "\n",
    "        PSHHG.append(shg_h)\n",
    "        PSHAG.append(shg_a)\n",
    "        shg_h = shg_h + (FTHG - HTHG) #Home team's previous second half goals scored before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(shg_h)\n",
    "        shg_a = shg_a + (FTAG - HTAG) #Away team's previous second half goals scored before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(shg_a)\n",
    "\n",
    "    priorData.loc[:,'PSHHG'] = pd.Series(PSHHG)\n",
    "    priorData.loc[:,'PSHAG'] = pd.Series(PSHAG)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousSecondHalfGoals(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.11 Computing previous goals conceded before half-time\n",
    "<a name='section3211'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate previous goals conceded before half-time\n",
    "# PHTHGC = Home Team Previous Goals Conceded Before Half Time, totaled over season\n",
    "# PHTAGC = Away Team Previous Goals Conceded Before Half Time, Totaled over season\n",
    "\n",
    "def getPreviousHalfTimeGoalConceded(priorData):\n",
    "    teams = {}\n",
    "    PHTHGC = [] \n",
    "    PHTAGC = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "                      \n",
    "        HTHG = priorData.iloc[i]['HTHG']\n",
    "        HTAG = priorData.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            phtgc_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            phtgc_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            phtgc_h = 0\n",
    "            phtgc_a = 0\n",
    "\n",
    "        PHTHGC.append(phtgc_h)\n",
    "        PHTAGC.append(phtgc_a)\n",
    "        phtgc_h = phtgc_h + HTAG #Home team's previous half time goals conceded before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(phtgc_h)\n",
    "        phtgc_a = phtgc_a + HTHG #Away team's previous half time goals conceded before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(phtgc_a)\n",
    "\n",
    "    priorData.loc[:,'PHTHGC'] = pd.Series(PHTHGC)\n",
    "    priorData.loc[:,'PHTAGC'] = pd.Series(PHTAGC)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousHalfTimeGoalConceded(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.12 Computing previous goals conceded after half-time\n",
    "<a name='section3212'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate previous goals conceded after half-time\n",
    "# PSHHGC = Previous second half home team goals conceded, totaled over season\n",
    "# PSHAGC = Previous second half away team goals conceded, totaled over season\n",
    "\n",
    "def getPreviousSecondHalfGoalConceded(priorData):\n",
    "    teams = {}\n",
    "    PSHHGC = [] \n",
    "    PSHAGC = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "  \n",
    "        FTHG = priorData.iloc[i]['FTHG']\n",
    "        FTAG = priorData.iloc[i]['FTAG']   \n",
    "        HTHG = priorData.iloc[i]['HTHG']\n",
    "        HTAG = priorData.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            pshhgc_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pshhgc_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pshhgc_h = 0\n",
    "            pshhgc_a = 0\n",
    "\n",
    "        PSHHGC.append(pshhgc_h)\n",
    "        PSHAGC.append(pshhgc_a)\n",
    "        pshhgc_h = pshhgc_h + (FTAG - HTAG) #Home team's previous half time goals conceded before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pshhgc_h)\n",
    "        pshhgc_a = pshhgc_a + (FTHG - HTHG) #Away team's previous half time goals conceded before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pshhgc_a)\n",
    "\n",
    "    priorData.loc[:,'PSHHGC'] = pd.Series(PSHHGC)\n",
    "    priorData.loc[:,'PSHAGC'] = pd.Series(PSHAGC)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousSecondHalfGoalConceded(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.13 Matches Played\n",
    "<a name='section3213'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors \n",
    "# Calculate previous goals conceded after half-time\n",
    "# PMPH = Previous total matches played for home team\n",
    "# PMPA = Previous total matches played for away team\n",
    "def getPreviousMatchesPlayed(priorData):\n",
    "    teams = {}\n",
    "    PMPH = [] \n",
    "    PMPA = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        try:\n",
    "            pmp_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pmp_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pmp_h = 0\n",
    "            pmp_a = 0\n",
    "\n",
    "        PMPH.append(pmp_h)\n",
    "        PMPA.append(pmp_a)\n",
    "        pmp_h = pmp_h + 1 #Home team's previous number matches played\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pmp_h)\n",
    "        pmp_a = pmp_a + 1 #Away team's previous number matches played\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pmp_a)\n",
    "\n",
    "    priorData.loc[:,'PMPH'] = pd.Series(PMPH)\n",
    "    priorData.loc[:,'PMPA'] = pd.Series(PMPA)\n",
    "    return priorData\n",
    "\n",
    "print(getPreviousMatchesPlayed(data_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Additional Features\n",
    "<a name='section33'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Distance Travelled for Away Teams\n",
    "<a name='section331'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIS\n",
    "# The positionalData contains the latitude and longitude of teams\n",
    "def getDistance(priorData):\n",
    "  array = []\n",
    "  for x in priorData.iterrows():\n",
    "   \n",
    "    home_lat = (positionalData.loc[positionalData['Team'] == x[1].HomeTeam]).Latitude\n",
    "    home_long = (positionalData.loc[positionalData['Team'] == x[1].HomeTeam]).Longitude\n",
    "    home_location = (np.float32(home_lat), np.float32(home_long))\n",
    "    \n",
    "    away_lat = (positionalData.loc[positionalData['Team'] == x[1].AwayTeam]).Latitude\n",
    "   \n",
    "    away_long = (positionalData.loc[positionalData['Team'] == x[1].AwayTeam]).Longitude\n",
    "    away_location = (np.float32(away_lat), np.float32(away_long))\n",
    "    array.append(np.float32(geodesic(home_location, away_location).km))\n",
    "  \n",
    "  \n",
    "  DIS = pd.Series(array)\n",
    "  priorData.loc[:,'DIS'] = DIS\n",
    "\n",
    "  return priorData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Average shots on goal in the past 3 matches\n",
    "<a name='section332'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average shots on goal for the past 3 matches\n",
    "# HAS, AAS\n",
    "def getPreviousShotOnGoal_3(priorData):\n",
    "    teams = {}\n",
    "    HAS = [] \n",
    "    AAS = []   \n",
    "    \n",
    "    for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = deque([None, None, None]) #[3rd, 2nd, latest priorData]\n",
    "            \n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "\n",
    "            \n",
    "        try:\n",
    "            as_h = np.mean(teams[priorData.iloc[i].HomeTeam])\n",
    "            as_a = np.mean(teams[priorData.iloc[i].AwayTeam])\n",
    "        except:\n",
    "            as_h = None\n",
    "            as_a = None\n",
    "\n",
    "        HAS.append(as_h)\n",
    "        AAS.append(as_a)\n",
    "\n",
    "        teams[priorData.iloc[i].HomeTeam].popleft()\n",
    "        teams[priorData.iloc[i].HomeTeam].append(priorData.iloc[i].HS)\n",
    "\n",
    "        teams[priorData.iloc[i].AwayTeam].popleft()\n",
    "        teams[priorData.iloc[i].AwayTeam].append(priorData.iloc[i].AS)\n",
    "\n",
    "    priorData.loc[:,'HAS'] = pd.Series(HAS)\n",
    "    priorData.loc[:,'AAS'] = pd.Series(AAS)\n",
    "\n",
    "    return priorData\n",
    "\n",
    "#getPreviousShotOnGoal_3(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 WL Performance of past 3 matches\n",
    "<a name='section333'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Home-Away teams in past 3 matches\n",
    "# HM1, AM1, HM2, AM2, HM3, AM3\n",
    "def getPerformanceOfLast3Matches(priorData):\n",
    "    HM1 = []    # performance of the last match of home team\n",
    "    AM1 = []    # performance of the last match of away team\n",
    "\n",
    "    HM2 = []    # performance of the 2nd last match of home team\n",
    "    AM2 = []\n",
    "\n",
    "    HM3 = []    # performance of the 3rd last match of home team\n",
    "    AM3 = []\n",
    "\n",
    "    teams = {}\n",
    "    \n",
    "    for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "               teams[name] = deque([None, None, None])  #[3rd, 2nd, latest priorData]\n",
    "\n",
    "    for i in range(len(priorData)):\n",
    "        \n",
    "\n",
    "        HM3.append(teams[priorData.iloc[i].HomeTeam].popleft())\n",
    "        AM3.append(teams[priorData.iloc[i].AwayTeam].popleft())\n",
    "        HM2.append(teams[priorData.iloc[i].HomeTeam][0])\n",
    "        AM2.append(teams[priorData.iloc[i].AwayTeam][0])\n",
    "        HM1.append(teams[priorData.iloc[i].HomeTeam][1])\n",
    "        AM1.append(teams[priorData.iloc[i].AwayTeam][1])\n",
    "\n",
    "        if priorData.iloc[i].FTR == 'H':\n",
    "            teams[priorData.iloc[i].HomeTeam].append('W')\n",
    "            teams[priorData.iloc[i].AwayTeam].append('L')\n",
    "        elif priorData.iloc[i].FTR == 'A':\n",
    "            teams[priorData.iloc[i].AwayTeam].append('W')\n",
    "            teams[priorData.iloc[i].HomeTeam].append('L')\n",
    "        else:\n",
    "            teams[priorData.iloc[i].AwayTeam].append('D')\n",
    "            teams[priorData.iloc[i].HomeTeam].append('D')\n",
    "\n",
    "    priorData.loc[:,'HM1'] = HM1\n",
    "    priorData.loc[:,'AM1'] = AM1\n",
    "    priorData.loc[:,'HM2'] = HM2\n",
    "    priorData.loc[:,'AM2'] = AM2\n",
    "    priorData.loc[:,'HM3'] = HM3\n",
    "    priorData.loc[:,'AM3'] = AM3\n",
    "\n",
    "    return priorData\n",
    "\n",
    "#print(getPerformanceOfLast3Matches(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Cumulative Full Time Goal Difference\n",
    "<a name='section334'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate cumulative Full-Time goal different for Home/Away teams prior to every match\n",
    "# HCGD = Home Cumulative Goal Difference\n",
    "# ACGD = Away Cumulative Goal Difference\n",
    "def getCumulativeGoalsDiff(priorData):\n",
    "    teams = {}\n",
    "    HCGD = [] \n",
    "    ACGD = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        # as the result in 3.2.1 shows that the number of matchese per season is always the same, so here we simply use i%380==0 to check if it is a new season and to initialize the feature.\n",
    "        if (i % 380 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = []\n",
    "\n",
    "        FTHG = priorData.iloc[i]['FTHG']\n",
    "        FTAG = priorData.iloc[i]['FTAG']\n",
    "\n",
    "        try:\n",
    "            cgd_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            cgd_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            cgd_h = 0\n",
    "            cgd_a = 0\n",
    "\n",
    "        HCGD.append(cgd_h)\n",
    "        ACGD.append(cgd_a)\n",
    "        cgd_h = cgd_h + FTHG - FTAG\n",
    "        teams[priorData.iloc[i].HomeTeam].append(cgd_h)\n",
    "        cgd_a = cgd_a + FTAG - FTHG\n",
    "        teams[priorData.iloc[i].AwayTeam].append(cgd_a)\n",
    "\n",
    "    priorData.loc[:,'HCGD'] = pd.Series(HCGD)\n",
    "    priorData.loc[:,'ACGD'] = pd.Series(ACGD)\n",
    "\n",
    "    return priorData\n",
    "\n",
    "#getCumulativeGoalsDiff(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors - extra features pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pickled_to_df(df,filename,column):\n",
    "    matrix = pd.read_pickle(filename)\n",
    "    matrix[2008] = np.NaN\n",
    "#     print(matrix)\n",
    "    difference = []\n",
    "    for i in range(0,len(data_new)):\n",
    "    #     print(ratings_matrix[\"mean\"].loc[data_new[\"HomeTeam\"].iloc[i]])\n",
    "        if pd.isnull(matrix[year[i]].loc[df[\"HomeTeam\"].iloc[i]]) or pd.isnull(matrix[year[i]].loc[df[\"AwayTeam\"].iloc[i]]):\n",
    "            difference.append(np.nan)\n",
    "            \n",
    "        else:\n",
    "            difference.append(matrix[year[i]].loc[df[\"HomeTeam\"].iloc[i]]-matrix[year[i]].loc[df[\"AwayTeam\"].iloc[i]])\n",
    "\n",
    "#     for i in range(0,len(difference)):\n",
    "#         if difference[i]<-0.1:\n",
    "#             difference[i]='A'\n",
    "#         elif difference[i]>0.1:\n",
    "#             difference[i]='H'\n",
    "#         else:\n",
    "#             difference[i]='D'\n",
    "\n",
    "    df[column]=difference\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have decided not to include these scraped features due to the many missing values without a reliable way to impute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# files = glob.glob(\"./Pickles/*\")\n",
    "# for file in files:\n",
    "#     name = file.split(\"\\\\\")[-1].split(\".\")[0].replace(\"DF\",\"\")\n",
    "#     data_new = add_pickled_to_df(data_new,file,name)\n",
    "\n",
    "# data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Derive Priors\n",
    "<a name='section34'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def DerivePriors(priorData):\n",
    "    #get_previousFTResults(priorData) # dont want Full time results in the test data\n",
    "    get_PreviousHTResults(priorData)\n",
    "    getPreviousCumulativeGoals(priorData)\n",
    "    getPreviousShots(priorData)\n",
    "    getPreviousShotsOnTarget(priorData)\n",
    "    getPreviousTeamFouls(priorData)\n",
    "    getPreviousTeamCorners(priorData)\n",
    "    getPreviousHalfTimeGoalsScored(priorData)\n",
    "    getPreviousSecondHalfGoals(priorData)\n",
    "    getPreviousHalfTimeGoalConceded(priorData)\n",
    "    getPreviousSecondHalfGoalConceded(priorData)\n",
    "    getPreviousMatchesPlayed(priorData)\n",
    "    getDistance(priorData)\n",
    "    getPreviousShotOnGoal_3(priorData)\n",
    "    getPerformanceOfLast3Matches(priorData)\n",
    "    getCumulativeGoalsDiff(priorData)\n",
    "    return priorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove First Initial Season\n",
    "data_new = DerivePriors(data_new).iloc[380:] #chop off first season \n",
    "y=np.delete(y,slice(0,380),axis=0)\n",
    "data_new.reset_index(drop=True, inplace=True)\n",
    "data_new "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Final Data Preprocessing\n",
    "<a name='section35'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dates are transformed into their sin/cos representation to capture their cyclic nature and reduce dimensionality, further explanation is given in the report\n",
    "dates = data_new.iloc[:, 0:3]\n",
    "month_sin = transformation(dates[\"Month\"])[0]\n",
    "month_cos = transformation(dates[\"Month\"])[1]\n",
    "week_sin = transformation(dates[\"Week\"])[0]\n",
    "week_cos = transformation(dates[\"Week\"])[1]\n",
    "day_sin = transformation(dates[\"Day\"])[0]\n",
    "day_cos = transformation(dates[\"Day\"])[1]\n",
    "\n",
    "teams = pd.DataFrame(home_t.toarray()).add_prefix(\"home_\").join(pd.DataFrame(away_t.toarray()).add_prefix(\"away_\"))\n",
    "\n",
    "# Select only columns that contain priors, can't use in-game stats to predict the future\n",
    "priors = data_new.iloc[:, 21:44]\n",
    "\n",
    "# Compute custom features\n",
    "# PHGS_PHSOT is ratio of home goals to home shots on target\n",
    "PHGS_PHSOT = np.where(priors[\"PHSOT\"] != 0, priors[\"PHGS\"]/priors[\"PHSOT\"], 0)\n",
    "# PHGS_PHSOT is ratio of away goals to away shots on target\n",
    "PAGS_PASOT = np.where(priors[\"PASOT\"] != 0, priors[\"PAGS\"]/priors[\"PASOT\"], 0)\n",
    "# PHSOT_PHS is ratio of home shots on target to home shots\n",
    "PHSOT_PHS = np.where(priors[\"PHS\"] != 0, priors[\"PHSOT\"]/ (priors[\"PHS\"] + priors[\"PHSOT\"]), 0)\n",
    "# PASOT_PAS is ratio of away shots on target to away shots\n",
    "PASOT_PAS = np.where(priors[\"PAS\"] != 0, priors[\"PASOT\"]/ (priors[\"PAS\"] + priors[\"PASOT\"]), 0)\n",
    "# PHTF_PATF is ratio of home fouls to away fouls\n",
    "PHTF_PATF = np.where(priors[\"PATF\"] != 0, priors[\"PHTF\"]/priors[\"PATF\"], 0)\n",
    "\n",
    "# Building final dataset\n",
    "X = pd.DataFrame()\n",
    "X[\"month_cos\"] = month_cos\n",
    "X[\"month_sin\"] = month_sin\n",
    "X[\"week_cos\"] = week_cos\n",
    "X[\"week_sin\"] = week_sin\n",
    "X[\"day_cos\"] = day_cos\n",
    "X[\"day_sin\"] = day_sin\n",
    "X = X.join(teams).join(priors)\n",
    "X[\"PHGS_PHSOT\"] = PHGS_PHSOT.tolist()\n",
    "X[\"PAGS_PASOT\"] = PAGS_PASOT.tolist()\n",
    "X[\"PHSOT_PHS\"] = PHSOT_PHS.tolist()\n",
    "X[\"PASOT_PAS\"] = PASOT_PAS.tolist()\n",
    "X[\"PHTF_PATF\"] = PHTF_PATF.tolist()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Split Data\n",
    "<a name='section351'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_val = np.array(y_val).reshape(len(y_val))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_val = encoder.transform(y_val)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "y_train_categorical = keras.utils.to_categorical(y_train)\n",
    "y_val_categorical = keras.utils.to_categorical(y_val)\n",
    "y_test_categorical = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try without our custom features\n",
    "rf, preds, all_stats_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on all in-game stats: \" + str(all_stats_accuracy) + \"%\")\n",
    "\n",
    "feature_importances = feat_importances(X_train, rf)\n",
    "print(\"Feature Importances: \")\n",
    "[print('Feature: {:35} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Scale data\n",
    "<a name='section36'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train.iloc[:, 92:])\n",
    "X_train_scaled = scaler.transform(X_train.iloc[:, 92:])\n",
    "X_test_scaled = scaler.transform(X_test.iloc[:, 92:])\n",
    "X_val_scaled = scaler.transform(X_val.iloc[:, 92:])\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_val = pd.DataFrame(X_val)\n",
    "\n",
    "X_train = np.array(X_train.iloc[:, 0:92])\n",
    "X_test = np.array(X_test.iloc[:, 0:92])\n",
    "X_val = np.array(X_val.iloc[:, 0:92])\n",
    "\n",
    "X_train_scaled = np.hstack((X_train, X_train_scaled))\n",
    "X_test_scaled = np.hstack((X_test, X_test_scaled))\n",
    "X_val_scaled = np.hstack((X_val, X_val_scaled))\n",
    "\n",
    "# PCA\n",
    "# https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "# pca = PCA(0.95)\n",
    "# pca.fit(X_train_scaled)\n",
    "# X_train = pca.transform(X_train_scaled)\n",
    "# X_test = pca.transform(X_test_scaled)\n",
    "\n",
    "# pca = PCA(n_components=50)\n",
    "# X = pca.fit_transform(X_scaled)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# X = TSNE(n_components=3, learning_rate='auto', init='random').fit_transform(X)\n",
    "\n",
    "# from sklearn.manifold import MDS\n",
    "# embedding = MDS(n_components=2)\n",
    "# X = embedding.fit_transform(X) -> took way too long\n",
    "\n",
    "# from sklearn.manifold import Isomap\n",
    "# embedding = Isomap(n_components=2)\n",
    "# X = embedding.fit_transform(X) -> gave terrible results\n",
    "\n",
    "# import umap.umap_ as umap\n",
    "# reducer = umap.UMAP(random_state=42,n_components=15)\n",
    "# X = reducer.fit_transform(X_scaled) -> requires outdated numpy\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=15, kernel='rbf')\n",
    "kpca.fit(X_train_scaled)\n",
    "X_train = kpca.transform(X_train_scaled)\n",
    "X_test = kpca.transform(X_test_scaled)\n",
    "X_val = kpca.transform(X_val_scaled)\n",
    "#tune hyperparams for this -> gamma\n",
    "\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# clf = LinearDiscriminantAnalysis()\n",
    "# clf.fit(X_train_scaled, y_train)\n",
    "# X_train = clf.transform(X_train_scaled)\n",
    "# X_test = clf.transform(X_test_scaled)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# X = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X)\n",
    "\n",
    "#consider combos of these eg pca then lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping data for RNN, LSTM, GRU\n",
    "X_train_test = np.reshape(X_train, (532, 10, X_train.shape[1]))\n",
    "\n",
    "y_train_test = np.reshape(y_train_categorical, (532, 10, 3))\n",
    "\n",
    "y_train_test_cnn = np.array(y_train_categorical).reshape(-1, 1, y_train_categorical.shape[1])\n",
    "\n",
    "X_val_test = np.reshape(X_val, (114, 10, X_val.shape[1]))\n",
    "\n",
    "y_val_categorical_test = np.reshape(y_val_categorical, (114, 10, 3))\n",
    "\n",
    "y_val_test_cnn = np.array(y_val_categorical).reshape(-1, 1, y_val_categorical.shape[1])\n",
    "\n",
    "X_test_test = X_test.reshape(114, 10, X_test.shape[1])\n",
    "\n",
    "X_train_cnn = np.expand_dims(X_train, axis=-1)\n",
    "\n",
    "X_val_cnn = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "X_test_cnn = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90xVEDPEp5Js"
   },
   "source": [
    "## 4. Methodology Overview\n",
    "<a name='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOxVov2mqGZ-"
   },
   "source": [
    "## 5. Model Training & Validation\n",
    "<a name='section5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to remove warning to see clearer result\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "#Import for optuna and cv\n",
    "import optuna\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Base Models\n",
    "<a name='section51'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Gaussian Naive Bayes\n",
    "<a name='section511'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Implement into pipeline ##\n",
    "# Some general comments:\n",
    "# Gaussian NB is most suitable for non-categorical classification\n",
    "# Based on diagram above (gaussian distributed density plots) the features we use are gaussian distributed however \n",
    "# the teams are not actually gaussian distributed \n",
    "# And the features we use are not conditionally independent as the statistics arent independent (e.g. shots affect\n",
    "# shots on target etc.)\n",
    "# Therefore we expect that the prediction will not be accurate and naives bayes is not suitable\n",
    "\n",
    "#prove calculations and results later\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "# y_gnb = gnb.fit(X_train, y_train).predict(X_test)\n",
    "# accuracy_score(y_test, y_gnb)\n",
    " \n",
    "\n",
    "# #Smoothing parameter scaling\n",
    "\n",
    "# cv_method = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# param_grid_nb = {\n",
    "#     'var_smoothing': np.logspace(0,-9, num=100)\n",
    "# }\n",
    "# gnb = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, cv=cv_method,verbose=1, scoring='accuracy',n_jobs=-1)\n",
    "# y_gnb = gnb.fit(X_train, y_train).predict(X_test)\n",
    "# accuracy_score(y_test, y_gnb)\n",
    " \n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# print(accuracy_score(y_test, y_gnb), \": is the accuracy score gnb\")\n",
    "\n",
    "\n",
    "#using optuna\n",
    "def objective_gnb(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"var_smoothing\": trial.suggest_float(\"var_smoothing\",1e-9,1,log=True),\n",
    "    }\n",
    "    model = GaussianNB(**param_grid)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "    preds = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, preds)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"GaussianNB\")\n",
    "func = lambda trial: objective_gnb(trial, X, y)\n",
    "study.optimize(func, n_trials=20)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = GaussianNB(**study.best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))\n",
    "\n",
    "gnb_final = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Generic SVM\n",
    "<a name='section512'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using generic SVM to estimate\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "# gammas = np.power(2, np.linspace(-15, 3, 10))\n",
    "# accuracy_validation = np.empty((5, len(gammas)))\n",
    "\n",
    "# for l, gamma in enumerate(gammas):\n",
    "#     svm = SVC(kernel='rbf', gamma=gamma, C=100)\n",
    "#     svm.fit(X_train, y_train)\n",
    "        \n",
    "#     predict_test = svm.predict(X_test)  # test\n",
    "#     print(accuracy_score(y_test, predict_test))\n",
    "\n",
    "# SVM = svm.SVC(kernel=\"linear\")   #(kernel=\"poly\", degree=3, coef0=1, C=5) (kernel=\"linear\")\n",
    "# SVM.fit(training_data,y_train)# predict the labels on validation dataset\n",
    "# predictions_SVM = SVM.predict(testing_data)# Use accuracy_score function to get the accuracy\n",
    "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)\n",
    "\n",
    "# scores = cross_val_score(SVM, X_whole, y_enc, cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=100))\n",
    "# scores.mean()\n",
    "\n",
    "# def fineTuneSVM(X_train, y_train):\n",
    "#     # define model and parameters\n",
    "#     svm = SVC()   \n",
    "#     # SVM solves an optimization problem of quadratic order \n",
    "#     # link on SVC: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "#     # The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.\n",
    "#     # Therefore, we will stick with basic kernels like linear and rbf which do the job well without sacrificing processing time.\n",
    "#     kernel = ['linear', 'rbf'] \n",
    "#     # kernel = ['poly', 'rbf', 'sigmoid'] #Advanced kernels \n",
    "#     C = [50, 10, 1.0, 0.1, 0.01]\n",
    "#     gamma = ['scale']\n",
    "    \n",
    "#     # define grid search\n",
    "#     grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
    "#     cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "#     grid_search = GridSearchCV(estimator=svm, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "#     grid_result = grid_search.fit(X_train, y_train)\n",
    "    \n",
    "#     # summarize results\n",
    "#     print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#     means = grid_result.cv_results_['mean_test_score']\n",
    "#     stds = grid_result.cv_results_['std_test_score']\n",
    "#     params = grid_result.cv_results_['params']\n",
    "#     for mean, stdev, param in zip(means, stds, params):\n",
    "#         print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# fineTuneSVM(X_train, y_train)\n",
    "\n",
    "#using optuna\n",
    "def objective_svm(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"kernel\": trial.suggest_categorical(\"kernel\", ['linear', 'rbf']),\n",
    "        \"C\": trial.suggest_categorical(\"C\", [50, 10, 1.0, 0.1, 0.01]),\n",
    "        \"gamma\": trial.suggest_categorical(\"gamma\", ['scale']),\n",
    "    }\n",
    "    model = SVC(probability = True, **param_grid)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "    preds = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, preds)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"SVM\")\n",
    "func = lambda trial: objective_svm(trial, X, y)\n",
    "study.optimize(func, n_trials=20)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = SVC(**study.best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))\n",
    "\n",
    "# \tBest value (rmse): 1.01688\n",
    "# \tBest params:\n",
    "# \t\tkernel: rbf\n",
    "# \t\tC: 1.0\n",
    "# \t\tgamma: scale\n",
    "# Training accuracy 0.5368421052631579\n",
    "# Testing accuracy 0.5236842105263158\n",
    "\n",
    "svm_final = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 Logistic Regression\n",
    "<a name='section513'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Multinomial logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# lr = LogisticRegression()\n",
    "# solvers = ['newton-cg', 'lbfgs','saga']\n",
    "# penalty = ['l2']\n",
    "# c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# multi_n = ['multinomial']\n",
    "# # define grid search\n",
    "# grid = dict(solver=solvers,penalty=penalty,C=c_values,multi_class=multi_n)\n",
    "# # cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# # cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# grid_search = GridSearchCV(estimator=lr, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# ### print all the tested results\n",
    "# # means = grid_result.cv_results_['mean_test_score']\n",
    "# # stds = grid_result.cv_results_['std_test_score']\n",
    "# # params = grid_result.cv_results_['params']\n",
    "# # for mean, stdev, param in zip(means, stds, params):\n",
    "# #     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "\n",
    "#using optuna\n",
    "def objective_lr(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", ['newton-cg', 'lbfgs','saga']),\n",
    "        \"penalty\": trial.suggest_categorical(\"penalty\", ['l2']),\n",
    "        \"multi_class\": trial.suggest_categorical(\"multi_class\", ['multinomial']),\n",
    "        \"C\": trial.suggest_categorical(\"C\", [100, 10, 1.0, 0.1, 0.01]),\n",
    "    }\n",
    "    model = LogisticRegression(**param_grid)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "    preds = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, preds)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LogisticRegression\")\n",
    "func = lambda trial: objective_lr(trial, X, y)\n",
    "study.optimize(func, n_trials=20)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = LogisticRegression(**study.best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))\n",
    "\n",
    "lr_final = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Boosting Models\n",
    "<a name='section52'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 XGBoost\n",
    "<a name='section521'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7, step=1),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, step=0.01),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 1, step=0.25),\n",
    "        \"reg_lambda\": trial.suggest_int(\"reg_lambda\", 0, 10, step=1),\n",
    "        \"scale_pos_weight\": trial.suggest_int(\"scale_pos_weight\", 1, 5, step=2),\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.5,\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(objective=\"multiclass\", **param_grid, use_label_encoder=False, eval_metric='mlogloss')\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)]\n",
    "        )\n",
    "    return model.score(X_val, y_val)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"XGBoost Classifier\")\n",
    "func = lambda trial: objective(trial, X, y)\n",
    "study.optimize(func, n_trials=20)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = xgb.XGBClassifier(objective=\"multiclass\", **study.best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
    "model.fit(X_train, y_train, eval_set=[(X_test,y_test), (X_train, y_train)])\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))\n",
    "\n",
    "xgb_final = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 AdaBoost\n",
    "<a name='section522'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimising hyperparameters for AdaBoost\n",
    "# First classified boosting algorithm\n",
    "\n",
    "# Sources used:\n",
    "# https://towardsdatascience.com/adaboost-from-scratch-37a936da3d50\n",
    "# https://analyticsindiamag.com/introduction-to-boosting-implementing-adaboost-in-python/\n",
    "# https://machinelearningmastery.com/adaboost-ensemble-in-python/\n",
    "\n",
    "# Hyperparameter types (Modified Y/N):\n",
    "# Num. of trees (Y)\n",
    "# Weak learner (N)\n",
    "# Learning rate (Y)\n",
    "# Alternate algorithm (Decision Tree/Logistic Regression)\n",
    "\n",
    "#First classified boosting algorithm\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define the model\n",
    "abc = AdaBoostClassifier()\n",
    "abc.fit(X_train, y_train)\n",
    "abc_pre_pred = abc.predict(X_test)\n",
    "print(\"Pretuning Test: %f\" % accuracy_score(y_test, abc_pre_pred))\n",
    "\n",
    "# with optuna\n",
    "def objective_abc(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 1000, step=200),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.0008, 0.0014, step=0.0002),\n",
    "    }\n",
    "    model = AdaBoostClassifier(**param_grid)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "    preds = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, preds)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"AdaBoost\")\n",
    "func = lambda trial: objective_abc(trial, X, y)\n",
    "study.optimize(func, n_trials=20)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = AdaBoostClassifier(**study.best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))\n",
    "\n",
    "# \tBest value (rmse): 1.03090\n",
    "# \tBest params:\n",
    "# \t\tn_estimators: 400\n",
    "# \t\tlearning_rate: 0.001\n",
    "# Training accuracy 0.5033834586466165\n",
    "# Testing accuracy 0.5201754385964912\n",
    "\n",
    "abc_final = model\n",
    "# abc_final = AdaBoostClassifier(n_estimators=400, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 GradientBoost\n",
    "<a name='section523'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimising hyperparameters for GradientBoost\n",
    "\n",
    "# Sources used:\n",
    "# https://www.datasciencelearner.com/gradient-boosting-hyperparameters-tuning/\n",
    "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "#define the model\n",
    "gbc = GradientBoostingClassifier(subsample = 0.8)\n",
    "gbc.fit(X_train, y_train)\n",
    "gbc_pre_pred = gbc.predict(X_test)\n",
    "print(\"Pretuning Test: %f\" % accuracy_score(y_test, gbc_pre_pred))\n",
    "\n",
    "#with optuna\n",
    "def objective_gbc(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"subsample\": trial.suggest_categorical(\"subsample\", [0.8]),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 3, 9, step=3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 6),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.1]),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 30, 50, step=5),\n",
    "    }\n",
    "    model = GradientBoostingClassifier(**param_grid)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "    preds = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, preds)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"GradientBoost\")\n",
    "func = lambda trial: objective_gbc(trial, X, y)\n",
    "study.optimize(func, n_trials=20)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = GradientBoostingClassifier(**study.best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))\n",
    "\n",
    "gbc_final = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 LightGBM\n",
    "<a name='section524'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "# https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\n",
    "\n",
    "import lightgbm\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 0.2, 0.9, step=0.1\n",
    "        ),\n",
    "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 0.2, 0.9, step=0.1\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    model = lightgbm.LGBMClassifier(objective=\"multiclass\", **param_grid)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"multi_logloss\",\n",
    "        early_stopping_rounds=100,\n",
    "        )\n",
    "    return model.score(X_val, y_val)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\n",
    "func = lambda trial: objective(trial, X, y)\n",
    "study.optimize(func, n_trials=20)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = lightgbm.LGBMClassifier(objective=\"multiclass\", **study.best_params)\n",
    "model.fit(X_train, y_train, eval_set=[(X_test,y_test), (X_train, y_train)], eval_metric='multi_logloss')\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))\n",
    "\n",
    "#Rename for results graph\n",
    "lightgbm_final = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Neural Network Models\n",
    "<a name='section53'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Vanilla Neural Network\n",
    "<a name='section531'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Hyperparam tuning for vanilla nn\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(trial.suggest_categorical('n_nodes', [32, 64, 128, 256]), input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation', ['relu', 'linear', 'tanh'])))\n",
    "    model.add(Dropout(0.1), )\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train, y_train_categorical, batch_size=50, epochs=5, validation_split=0.1, shuffle=False)\n",
    "    return model.evaluate(X_val, y_val_categorical)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Standard, baseline NN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#need to stop randomization\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "#Rename to vanilla\n",
    "model = Sequential()\n",
    "#need to replace input shape by X shape\n",
    "model.add(layers.Dense(ridge_params['n_nodes'], input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation(ridge_params['activation']))\n",
    "model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=ridge_params['optimizer'], metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val_categorical))\n",
    "\n",
    "results = model.evaluate(X_test, y_test_categorical, batch_size=50)\n",
    "_, acc = results\n",
    "print(acc)\n",
    "\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "# accuracy = accuracy_score(y_test, preds) * 100\n",
    "# print(accuracy)\n",
    "vnn_cr = classification_report(y_test, preds, output_dict=True, zero_division = True)\n",
    "\n",
    "preds = model.predict(X_train)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "vnn_train_acc = classification_report(y_train, preds, output_dict=True, zero_division = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Deep Neural Network\n",
    "<a name='section532'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparam tuning for deep nn\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(trial.suggest_categorical('n_nodes1', [32, 64, 128, 256]), input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation1', ['relu', 'linear', 'tanh'])))\n",
    "    model.add(layers.Dense(trial.suggest_categorical('n_nodes2', [32, 64, 128, 256]), input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation2', ['relu', 'linear', 'tanh'])))\n",
    "    model.add(layers.Dense(trial.suggest_categorical('n_nodes3', [32, 64, 128, 256]), input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation3', ['relu', 'linear', 'tanh'])))\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train, y_train_categorical, batch_size=50, epochs=5, validation_split=0.1, shuffle=False)\n",
    "    return model.evaluate(X_val, y_val_categorical)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Deep NN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#need to stop randomization\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "#Rename to vanilla\n",
    "model = Sequential()\n",
    "#need to replace input shape by X shape\n",
    "model.add(layers.Dense(ridge_params['n_nodes1'], input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation(ridge_params['activation1']))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(ridge_params['n_nodes2'], input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation(ridge_params['activation2']))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(ridge_params['n_nodes3'], input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation(ridge_params['activation3']))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=ridge_params['optimizer'], metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "results = model.evaluate(X_test, y_test_categorical, batch_size=50)\n",
    "_, acc = results\n",
    "print(acc)\n",
    "\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "# accuracy = accuracy_score(y_test, preds) * 100\n",
    "# print(accuracy)\n",
    "dnn_cr = classification_report(y_test, preds, output_dict=True, zero_division = True)\n",
    "\n",
    "preds = model.predict(X_train)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "dnn_train_acc = classification_report(y_train, preds, output_dict=True, zero_division = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Recurrent Neural Network\n",
    "<a name='section533'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparam tuning for rnn\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(trial.suggest_categorical('n_nodes', [32, 64, 128, 256]), input_shape = (10, 15), return_sequences = True))\n",
    "    model.add(Dropout(trial.suggest_categorical('dropout', [0.2, 0.4, 0.6, 0.8])), )\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation', ['sigmoid', 'softmax'])))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer', ['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train_test, y_train_test, epochs = 20, validation_split=0.1, shuffle=False)\n",
    "    return model.evaluate(X_val_test, y_val_categorical_test)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating RNN #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, SimpleRNN\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "# X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "# X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(ridge_params['n_nodes'], input_shape = (10, 15), return_sequences = True))\n",
    "model.add(Dropout(ridge_params['dropout']), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation(ridge_params['activation']))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = ridge_params['optimizer'], metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 20)\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "preds = preds.reshape(1140, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(accuracy)\n",
    "rnn_cr = classification_report(y_test, preds, output_dict=True, zero_division=True)\n",
    "\n",
    "rnn_train_acc = history.history['accuracy'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 Gated Recurrent Neural Network\n",
    "<a name='section534'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparam tuning for gru\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GRU(trial.suggest_categorical('n_nodes', [32, 64, 128, 256]), input_shape = (10, 15), return_sequences = True))\n",
    "    model.add(Dropout(trial.suggest_categorical('dropout', [0.2, 0.4, 0.6, 0.8])), )\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation', ['sigmoid', 'softmax'])))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train_test, y_train_test, epochs = 20, validation_split=0.1, shuffle=False)\n",
    "    return model.evaluate(X_val_test, y_val_categorical_test)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating GRU #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GRU\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "# X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "# X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(GRU(ridge_params['n_nodes'], input_shape = (10, 15), return_sequences = True))\n",
    "model.add(Dropout(ridge_params['dropout']), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation(ridge_params['activation']))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = ridge_params['optimizer'], metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 20)\n",
    "\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "preds = preds.reshape(1140, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(accuracy)\n",
    "\n",
    "gru_cr = classification_report(y_test, preds, output_dict=True, zero_division=True)\n",
    "\n",
    "gru_train_acc = history.history['accuracy'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.5 Long Short-Term Memory Neural Network\n",
    "<a name='section535'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparam tuning for LSTM\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(trial.suggest_categorical('n_nodes', [32, 64, 128, 256]), input_shape = (10, 15), return_sequences = True))\n",
    "    model.add(Dropout(trial.suggest_categorical('dropout', [0.2, 0.4, 0.6, 0.8])), )\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation', ['sigmoid', 'softmax'])))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train_test, y_train_test, epochs = 50, batch_size = 100, shuffle=False)\n",
    "    return model.evaluate(X_val_test, y_val_categorical_test)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating LSTM\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "\n",
    "# X_train_test = np.reshape(X_train, (266, 13, 15))\n",
    "\n",
    "# y_train_test = np.reshape(y_train_categorical, (266, 13, 3))\n",
    "\n",
    "# X_test_test = X_test.reshape(114, 13, 15)\n",
    "\n",
    "# X_train_lstm, y_train_categorical_lstm = lstm_data_transform(X_train, y_train_categorical, num_steps=5)\n",
    "# X_train_lstm = np.array(X_train_lstm).reshape(-1, 5, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical_lstm).reshape(-1, 5, y_train_categorical.shape[1])\n",
    "\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(ridge_params['n_nodes'], input_shape = (10, 15), return_sequences = True))\n",
    "model.add(Dropout(ridge_params['dropout']), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation(ridge_params['activation']))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = ridge_params['optimizer'], metrics = ['accuracy'])\n",
    "\n",
    "#change fit to be like rnn and gru?\n",
    "#set shuffle to false\n",
    "# add validation set here?\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 50, batch_size = 100)\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "preds = preds.reshape(1140, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(accuracy)\n",
    "\n",
    "lstm_cr = classification_report(y_test, preds, output_dict=True, zero_division=True)\n",
    "\n",
    "lstm_train_acc = history.history['accuracy'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.6 Convolutional Neural Network\n",
    "<a name='section536'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparam tuning for cnn\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(trial.suggest_categorical('n_nodes', [32, 64, 128, 256]), kernel_size=X_train.shape[1], input_shape=(None, 1)))\n",
    "    #add activation here\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation', ['sigmoid', 'softmax'])))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train_cnn, y_train_test_cnn, epochs = 20 , validation_split=0.1, shuffle=False)\n",
    "    return model.evaluate(X_val_cnn, y_val_test_cnn)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating CNN #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "# X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "# X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "# input_shape = (1, X_train.shape[1])\n",
    "model.add(Conv1D(ridge_params['n_nodes'], kernel_size=X_train.shape[1], input_shape=(None, 1)))\n",
    "#should there be an activation here?\n",
    "# model.add(Conv1D(64, kernel_size=X_train.shape[1], input_shape=(None, 1)))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation(ridge_params['activation']))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = ridge_params['optimizer'], metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_cnn, y_train_test_cnn, epochs = 20)\n",
    "\n",
    "preds = model.predict(X_test_cnn)\n",
    "preds = np.argmax(preds, axis=2)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(accuracy)\n",
    "\n",
    "cnn_cr = classification_report(y_test, preds, output_dict=True, zero_division=True)\n",
    "\n",
    "cnn_train_acc = history.history['accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ROC curves\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "n_classes = 3\n",
    "\n",
    "y_score = model.predict_proba(X_test_cnn)\n",
    "# We need to do this because for some reason cnn#predict_proba returns a 3D array which is actually a 2D array with the elements wrapped in two arrays\n",
    "y_score_fixed = []\n",
    "for i in range(len(y_score)):\n",
    "    y_score_fixed.append(y_score[i][0])\n",
    "y_score_fixed = np.array(y_score_fixed)\n",
    "\n",
    "y_test_categorical = keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_categorical[:, i], y_score_fixed[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_categorical.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Process of plotting roc-auc curve belonging to all classes.\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for all classes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Precision Recall curve\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test_categorical[:, i], y_score_fixed[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test_categorical[:, i], y_score_fixed[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n",
    "    y_test_categorical.ravel(), y_score.ravel()\n",
    ")\n",
    "average_precision[\"micro\"] = average_precision_score(y_test_categorical, y_score_fixed, average=\"micro\")\n",
    "\n",
    "_, ax = plt.subplots(figsize=(17, 8))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines, labels = [], []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "    plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "display = PrecisionRecallDisplay(\n",
    "    recall=recall[\"micro\"],\n",
    "    precision=precision[\"micro\"],\n",
    "    average_precision=average_precision[\"micro\"],\n",
    ")\n",
    "display.plot(ax=ax, name=\"Micro-average precision-recall\", color=\"gold\")\n",
    "    \n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[i],\n",
    "        precision=precision[i],\n",
    "        average_precision=average_precision[i],\n",
    "    )\n",
    "    display.plot(ax=ax, name=f\"Precision-recall for class {i}\", color=color)\n",
    "    \n",
    "# add the legend for the iso-f1 curves\n",
    "handles, labels = display.ax_.get_legend_handles_labels()\n",
    "handles.extend([l])\n",
    "labels.extend([\"iso-f1 curves\"])\n",
    "# set the legend and the axes\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.legend(handles=handles, labels=labels, loc=\"best\")\n",
    "ax.set_title(\"Precision-Recall Curves for all classes\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Time Series Models\n",
    "<a name='section54'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 Prophet\n",
    "<a name='section541'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/time-series-forecasting-with-prophet-in-python/\n",
    "\n",
    "from prophet import *\n",
    "import optuna\n",
    "#from fbprophet import Prophet\n",
    "#from fbprophet.diagnostics import cross_validation, performance_metrics\n",
    "\n",
    "def objective(trial, df):\n",
    "    seasonality = ['additive', 'multiplicative']\n",
    "    param_grid = {\n",
    "        \"changepoint_prior_scale\": trial.suggest_uniform(\"changepoint_prior_scale\", 0.001, 0.5),\n",
    "        \"seasonality_prior_scale\": trial.suggest_uniform(\"seasonality_prior_scale\", 0.01, 10),\n",
    "        \"seasonality_mode\": seasonality[trial.suggest_int(\"seasonality_mode\", 0, 1)]\n",
    "    }\n",
    "\n",
    "    m = Prophet(**param_grid)\n",
    "    m.fit(df)\n",
    "    df_cv = diagnostics.cross_validation(m, \n",
    "                             initial='3458 days', \n",
    "                             period='13 days', \n",
    "                             horizon = '13 days',\n",
    "                             parallel=\"processes\")\n",
    "    \n",
    "    df_p = diagnostics.performance_metrics(df_cv, rolling_window=1)\n",
    "    \n",
    "    return df_p['rmse'].values[0]\n",
    "\n",
    "\n",
    "ds = pd.to_datetime(data[\"Date\"])\n",
    "ds = ds.iloc[380:] # chop off first season to match current data\n",
    "\n",
    "encoder = LabelEncoder().fit(y)\n",
    "\n",
    "# Prophet expects data as (ds, y) where ds is the date and y is the observation on that date\n",
    "prophet_df = pd.DataFrame()\n",
    "prophet_df[\"ds\"] = ds\n",
    "prophet_df[\"y\"] = encoder.transform(y)\n",
    "\n",
    "# This process can take a long time so we have added a flag that can be set if you just wish to see the accuracy with precomputed hyperparameters\n",
    "FIND_BEST_PARAMS = False\n",
    "if FIND_BEST_PARAMS:\n",
    "    study = optuna.create_study()\n",
    "    func = lambda trial: objective(trial, prophet_df)\n",
    "    study.optimize(func, n_trials=20)\n",
    "    print(f\"\\tBest value (rmse): {study.best_value}\")\n",
    "    print(f\"\\tBest params:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"\\t\\t{key}: {value}\")\n",
    "    best_params = study.best_params\n",
    "else:\n",
    "    best_params = {\n",
    "        \"changepoint_prior_scale\": 0.3742218538948863,\n",
    "        \"seasonality_prior_scale\": 0.05051527961102975,\n",
    "        \"seasonality_mode\": 'multiplicative'\n",
    "    }\n",
    "\n",
    "model = Prophet(**best_params)\n",
    "\n",
    "# 3458 is the equivalent of 266 seasons if we define a season as 13 matches and reserve the remaining 114 seasons for testing\n",
    "ds_train = prophet_df.head(3458)\n",
    "ds_test = prophet_df.tail(len(prophet_df) - 3458)\n",
    "\n",
    "model.fit(ds_train)\n",
    "\n",
    "future = ds_test.drop(columns=[\"y\"])\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "accuracy = accuracy_score(ds_test[\"y\"], np.rint(forecast[\"yhat\"]))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 Arima\n",
    "<a name='section542'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arima\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "def get_accuracy(X):\n",
    "    train_size = int(len(X) * 0.70)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = SARIMAX(history)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(round(yhat))\n",
    "        history.append(test[t])\n",
    "    return accuracy_score(test, predictions)\n",
    "\n",
    "\n",
    "idx = data['Date']\n",
    "idx = idx.iloc[380:] # chop off first season to match current data\n",
    "encoder = LabelEncoder().fit(y)\n",
    "y = encoder.transform(y)\n",
    "ts = pd.Series(y, index=idx).astype('float32')\n",
    "\n",
    "# Summary:\n",
    "# This model doesn't have any hyperparameter tuning because Optuna was run for over 2 hours and never returned\n",
    "# So we resorted to using the default hyperparameters to see what kind of accuracy we could expect\n",
    "# The accuracy we obtained is one of the lowest out of all the models we tried so decided to focus on those more promising models\n",
    "\n",
    "print(get_accuracy(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_aPyBUwqMAD"
   },
   "source": [
    "## 6. Results\n",
    "<a name='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "1. Define Models\n",
    "2. Calculate Predictions for Test Data\n",
    "3. Extract Classification Report for each Model\n",
    "4. Plot results as bar charts\n",
    "5. 1 bar chart for each classification plot output\n",
    "### Definitions\n",
    "- The recall means \"how many of this class you find over the whole number of elements of this class\"\n",
    "- The precision will be \"how many are correctly classified among that class\"\n",
    "- The f1-score is the harmonic mean between precision & recall\n",
    "- The support is the number of occurence of the given class in your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Calculate Train and Test Accuracies\n",
    "<a name='section61'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Training Models and CR\n",
    "\n",
    "## Standard Models Training Accuracy##\n",
    "gnb_vis_pred_train = gnb_final.fit(X_train, y_train).predict(X_train)\n",
    "svm_vis_pred_train = svm_final.fit(X_train, y_train).predict(X_train)\n",
    "lr_vis_pred_train = lr_final.fit(X_train, y_train).predict(X_train)\n",
    "\n",
    "## Boosting Models ##\n",
    "xgb_vis_pred_train = xgb_final.fit(X_train, y_train).predict(X_train)\n",
    "abc_vis_pred_train = abc_final.fit(X_train, y_train).predict(X_train)\n",
    "gbc_vis_pred_train = gbc_final.fit(X_train, y_train).predict(X_train)\n",
    "lightgbm_vis_pred_train = lightgbm_final.predict(X_train)\n",
    "\n",
    "## NN Models ##\n",
    "# Defined in models\n",
    "\n",
    "## Time Series Modules ##\n",
    "# prophet_vis_pred_train = prophet_final.predict(X_test)\n",
    "# arima_vis_pred_train = arima_final.predict(X_test)\n",
    "\n",
    "#Save results as dict\n",
    "lr_cr_train = classification_report(y_train, lr_vis_pred_train, output_dict=True)\n",
    "gnb_cr_train = classification_report(y_train, gnb_vis_pred_train, output_dict=True)\n",
    "svm_cr_train = classification_report(y_train, svm_vis_pred_train, output_dict=True)\n",
    "xgb_cr_train = classification_report(y_train, xgb_vis_pred_train, output_dict=True)\n",
    "abc_cr_train = classification_report(y_train, abc_vis_pred_train, output_dict=True)\n",
    "gbc_cr_train = classification_report(y_train, gbc_vis_pred_train, output_dict=True)\n",
    "lightgbm_cr_train = classification_report(y_train, lightgbm_vis_pred_train, output_dict=True)\n",
    "vnn_cr_train = vnn_train_acc\n",
    "dnn_cr_train = dnn_train_acc\n",
    "# ACCURACY FOR BELOW AS VARIABLE\n",
    "# rnn_cr_train = classification_report(y_train, rnn_vis_pred, output_dict=True)\n",
    "# gru_cr_train = classification_report(y_train, gru_vis_pred, output_dict=True)\n",
    "# ltsm_cr_train = classification_report(y_train, ltsm_vis_pred, output_dict=True)\n",
    "# cnn_cr_train = classification_report(y_train, cnn_vis_pred, output_dict=True)\n",
    "#CHECK IF FIXED\n",
    "# prophet_cr_train = classification_report(y_train, prophet_vis_pred, output_dict=True)\n",
    "# arima_cr_train = classification_report(y_train, arima_vis_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Testing Models and CR\n",
    "\n",
    "## Standard Models Test Accuracy ##\n",
    "gnb_vis_pred = gnb_final.fit(X_train, y_train).predict(X_test)\n",
    "svm_vis_pred = svm_final.fit(X_train, y_train).predict(X_test)\n",
    "lr_vis_pred = lr_final.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "## Boosting Models ##\n",
    "xgb_vis_pred = xgb_final.fit(X_train, y_train).predict(X_test)\n",
    "abc_vis_pred = abc_final.fit(X_train, y_train).predict(X_test)\n",
    "gbc_vis_pred = gbc_final.fit(X_train, y_train).predict(X_test)\n",
    "lightgbm_vis_pred = lightgbm_final.predict(X_test)\n",
    "\n",
    "## NN Models ##\n",
    "# Defined in models\n",
    "\n",
    "## Time Series Modules ##\n",
    "# prophet_vis_pred = prophet_final.predict(X_test)\n",
    "# arima_vis_pred = arima_final.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, lr_vis_pred, output_dict=True))\n",
    "\n",
    "#Save results as dict\n",
    "lr_cr = classification_report(y_test, lr_vis_pred, output_dict=True)\n",
    "gnb_cr = classification_report(y_test, gnb_vis_pred, output_dict=True)\n",
    "svm_cr = classification_report(y_test, svm_vis_pred, output_dict=True)\n",
    "xgb_cr = classification_report(y_test, xgb_vis_pred, output_dict=True)\n",
    "abc_cr = classification_report(y_test, abc_vis_pred, output_dict=True)\n",
    "gbc_cr = classification_report(y_test, gbc_vis_pred, output_dict=True)\n",
    "lightgbm_cr = classification_report(y_test, lightgbm_vis_pred, output_dict=True)\n",
    "# Defined in models\n",
    "# nn_cr = classification_report(y_test, nn_vis_pred, output_dict=True)\n",
    "# vnn_cr = classification_report(y_test, vnn_vis_pred, output_dict=True)\n",
    "# dnn_cr = classification_report(y_test, dnn_vis_pred, output_dict=True)\n",
    "# rnn_cr = classification_report(y_test, rnn_vis_pred, output_dict=True)\n",
    "# gru_cr = classification_report(y_test, gru_vis_pred, output_dict=True)\n",
    "# ltsm_cr = classification_report(y_test, ltsm_vis_pred, output_dict=True)\n",
    "# cnn_cr = classification_report(y_test, cnn_vis_pred, output_dict=True)\n",
    "# CHECK IF FIXED\n",
    "# prophet_cr = classification_report(y_test, prophet_vis_pred, output_dict=True)\n",
    "# arima_cr = classification_report(y_test, arima_vis_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Accuracy Comparison Plots\n",
    "<a name='section62'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Individual Training Accuracy\n",
    "<a name='section621'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names of models for columns\n",
    "models = ['GNB','SVM','LR','XGB','ABC','GBC','L-GBM','VNN','DNN','RNN','GRU','LSTM','CNN']\n",
    "# For after fix\n",
    "# models = ['GNB','SVM','LR','XGB','ABC','GBC','L-GBM','NN','VNN','DNN','RNN','GRU','LTSM','CNN','Prophet','Arima']\n",
    "\n",
    "#Plot of models by accuracy\n",
    "accuracy = [gnb_cr_train[\"accuracy\"],\n",
    "            svm_cr_train[\"accuracy\"],\n",
    "            lr_cr_train[\"accuracy\"],\n",
    "            xgb_cr_train[\"accuracy\"],\n",
    "            abc_cr_train[\"accuracy\"],\n",
    "            gbc_cr_train[\"accuracy\"],\n",
    "            lightgbm_cr_train[\"accuracy\"],\n",
    "            vnn_cr_train[\"accuracy\"],\n",
    "            dnn_cr_train[\"accuracy\"],\n",
    "            rnn_train_acc,\n",
    "            gru_train_acc,\n",
    "            lstm_train_acc,\n",
    "            cnn_train_acc]\n",
    "# For after fix\n",
    "# accuracy = [gnb_cr[\"accuracy\"],\n",
    "#             svm_cr[\"accuracy\"],\n",
    "#             lr_cr[\"accuracy\"],\n",
    "#             xgb_cr[\"accuracy\"],\n",
    "#             abc_cr[\"accuracy\"],\n",
    "#             gbc_cr[\"accuracy\"],\n",
    "#             lightgbm_cr[\"accuracy\"],\n",
    "#             vnn_cr[\"accuracy\"],\n",
    "#             dnn_cr[\"accuracy\"],\n",
    "#             rnn_cr[\"accuracy\"],\n",
    "#             gru_cr[\"accuracy\"],\n",
    "#             ltsm_cr[\"accuracy\"],\n",
    "#             cnn_cr[\"accuracy\"],\n",
    "#             prophet_cr[\"accuracy\"],\n",
    "#             arima_cr[\"accuracy\"]]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "plt.bar(models, accuracy)\n",
    "ax.set_title('Training Accuracy of Different Models')\n",
    "ax.set_yticks([0.40, 0.5, 0.6])\n",
    "ax.set_ylim([0.40, 0.6])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Individual Testing Accuracy\n",
    "<a name='section622'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names of models for columns\n",
    "models = ['GNB','SVM','LR','XGB','ABC','GBC','L-GBM','VNN','DNN','RNN','GRU','LSTM','CNN','Prophet','Arima']\n",
    "models_no_ts = ['GNB','SVM','LR','XGB','ABC','GBC','L-GBM','VNN','DNN','RNN','GRU','LSTM','CNN']\n",
    "# For after fix\n",
    "# models = ['GNB','SVM','LR','XGB','ABC','GBC','L-GBM','NN','VNN','DNN','RNN','GRU','LTSM','CNN','Prophet','Arima']\n",
    "\n",
    "#Plot of models by accuracy\n",
    "accuracy = [gnb_cr[\"accuracy\"],\n",
    "            svm_cr[\"accuracy\"],\n",
    "            lr_cr[\"accuracy\"],\n",
    "            xgb_cr[\"accuracy\"],\n",
    "            abc_cr[\"accuracy\"],\n",
    "            gbc_cr[\"accuracy\"],\n",
    "            lightgbm_cr[\"accuracy\"],\n",
    "            vnn_cr[\"accuracy\"],\n",
    "            dnn_cr[\"accuracy\"],\n",
    "            rnn_cr[\"accuracy\"],\n",
    "            gru_cr[\"accuracy\"],\n",
    "            lstm_cr[\"accuracy\"],\n",
    "            cnn_cr[\"accuracy\"],\n",
    "            0.33,\n",
    "            0.27]\n",
    "# For after fix\n",
    "# accuracy = [gnb_cr[\"accuracy\"],\n",
    "#             svm_cr[\"accuracy\"],\n",
    "#             lr_cr[\"accuracy\"],\n",
    "#             xgb_cr[\"accuracy\"],\n",
    "#             abc_cr[\"accuracy\"],\n",
    "#             gbc_cr[\"accuracy\"],\n",
    "#             lightgbm_cr[\"accuracy\"],\n",
    "#             vnn_cr[\"accuracy\"],\n",
    "#             dnn_cr[\"accuracy\"],\n",
    "#             rnn_cr[\"accuracy\"],\n",
    "#             gru_cr[\"accuracy\"],\n",
    "#             ltsm_cr[\"accuracy\"],\n",
    "#             cnn_cr[\"accuracy\"],\n",
    "#             prophet_cr[\"accuracy\"],\n",
    "#             arima_cr[\"accuracy\"]]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "plt.bar(models, accuracy)\n",
    "ax.set_title('Testing Accuracy of Different Models')\n",
    "ax.set_yticks([0.3, 0.35, 0.4, 0.45, 0.5, 0.55])\n",
    "ax.set_ylim([0.25, 0.55])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "#Plot of models by weighted avg f1 score\n",
    "accuracy = [gnb_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            svm_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            lr_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            xgb_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            abc_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            gbc_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            lightgbm_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            vnn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            dnn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            rnn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            gru_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            lstm_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            cnn_cr[\"weighted avg\"][\"f1-score\"]]\n",
    "# For after fix\n",
    "# accuracy = [gnb_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             svm_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             lr_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             xgb_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             abc_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             gbc_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             lightgbm_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             vnn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             dnn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             rnn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             gru_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             ltsm_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             cnn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             prophet_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             arima_cr[\"weighted avg\"][\"f1-score\"]]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "plt.bar(models_no_ts, accuracy)\n",
    "ax.set_title('Weighted F1-Score of Different Models')\n",
    "ax.set_yticks([0.35, 0.4, 0.45, 0.5])\n",
    "ax.set_ylim([0.35, 0.5])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Weighted F1-Score')\n",
    "plt.show()\n",
    "\n",
    "best_accuracy = [lr_cr[\"accuracy\"],\n",
    "                 abc_cr[\"accuracy\"],\n",
    "                 vnn_cr[\"accuracy\"],\n",
    "                 0.33]\n",
    "best_models = ['LR','ABC','VNN','Prophet']\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "plt.bar(best_models, best_accuracy)\n",
    "ax.set_title('Testing Accuracy of Best Models')\n",
    "ax.set_yticks([0.3, 0.35, 0.4, 0.45, 0.5, 0.55])\n",
    "ax.set_ylim([0.25, 0.55])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Weighted F1-Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 Training Accuracy Grouped by Model Type\n",
    "<a name='section623'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Models Training Accuracy##\n",
    "gnb_vis_pred_train = gnb_final.fit(X_train, y_train).predict(X_train)\n",
    "svm_vis_pred_train = svm_final.fit(X_train, y_train).predict(X_train)\n",
    "lr_vis_pred_train = lr_final.fit(X_train, y_train).predict(X_train)\n",
    "\n",
    "## Boosting Models ##\n",
    "xgb_vis_pred_train = xgb_final.fit(X_train, y_train).predict(X_train)\n",
    "abc_vis_pred_train = abc_final.fit(X_train, y_train).predict(X_train)\n",
    "gbc_vis_pred_train = gbc_final.fit(X_train, y_train).predict(X_train)\n",
    "lightgbm_vis_pred_train = lightgbm_final.predict(X_train)\n",
    "\n",
    "## NN Models ##\n",
    "# Defined in models\n",
    "\n",
    "## Time Series Modules ##\n",
    "# prophet_vis_pred_train = prophet_final.predict(X_test)\n",
    "# arima_vis_pred_train = arima_final.predict(X_test)\n",
    "\n",
    "#Save results as dict\n",
    "lr_cr_train = classification_report(y_train, lr_vis_pred_train, output_dict=True)\n",
    "gnb_cr_train = classification_report(y_train, gnb_vis_pred_train, output_dict=True)\n",
    "svm_cr_train = classification_report(y_train, svm_vis_pred_train, output_dict=True)\n",
    "xgb_cr_train = classification_report(y_train, xgb_vis_pred_train, output_dict=True)\n",
    "abc_cr_train = classification_report(y_train, abc_vis_pred_train, output_dict=True)\n",
    "gbc_cr_train = classification_report(y_train, gbc_vis_pred_train, output_dict=True)\n",
    "lightgbm_cr_train = classification_report(y_train, lightgbm_vis_pred_train, output_dict=True)\n",
    "vnn_cr_train = vnn_train_acc\n",
    "dnn_cr_train = dnn_train_acc\n",
    "# ACCURACY FOR BELOW AS VARIABLE\n",
    "# rnn_cr_train = classification_report(y_train, rnn_vis_pred, output_dict=True)\n",
    "# gru_cr_train = classification_report(y_train, gru_vis_pred, output_dict=True)\n",
    "# ltsm_cr_train = classification_report(y_train, ltsm_vis_pred, output_dict=True)\n",
    "# cnn_cr_train = classification_report(y_train, cnn_vis_pred, output_dict=True)\n",
    "#CHECK IF FIXED\n",
    "# prophet_cr_train = classification_report(y_train, prophet_vis_pred, output_dict=True)\n",
    "# arima_cr_train = classification_report(y_train, arima_vis_pred, output_dict=True)\n",
    "\n",
    "#Collective Base Training Accuracy \n",
    "base_cr = lr_cr_train[\"accuracy\"] + gnb_cr_train[\"accuracy\"] + svm_cr_train[\"accuracy\"]\n",
    "base_cr = base_cr/3\n",
    "print(\"base:\",base_cr)\n",
    "\n",
    "#Collective Boost Training Accuracy \n",
    "boost_cr = xgb_cr_train[\"accuracy\"] + abc_cr_train[\"accuracy\"] + gbc_cr_train[\"accuracy\"] + lightgbm_cr_train[\"accuracy\"]\n",
    "boost_cr = boost_cr/4\n",
    "print(\"boost:\",boost_cr)\n",
    "\n",
    "#Collective NN Training Accuracy \n",
    "nn_cr = vnn_cr_train[\"accuracy\"] + dnn_cr_train[\"accuracy\"] + rnn_train_acc +  gru_train_acc +   lstm_train_acc + cnn_train_acc\n",
    "nn_cr = nn_cr/6\n",
    "print(\"nn:\",nn_cr)\n",
    "\n",
    "#Collective TS Testing Accuracy \n",
    "#ts_cr =  prophet_cr_train[\"accuracy\"], arima_cr_train[\"accuracy\"]\n",
    "#ts_cr = ts_cr/2\n",
    "#Hardcoded\n",
    "ts_cr=(0.33 + 0.27)/2\n",
    "print(\"ts:\",ts_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names of models for columns\n",
    "#models = ['Base','Boost']\n",
    "# For after fix\n",
    "models = ['Base','Boost','NN','TS']\n",
    "\n",
    "\n",
    "#Plot of models by accuracy\n",
    "accuracy = [base_cr,\n",
    "            boost_cr,\n",
    "            nn_cr,\n",
    "            ts_cr]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "plt.bar(models, accuracy, width=0.8)\n",
    "ax.set_title('Cumulative Training Accuracy of Models')\n",
    "ax.set_yticks([0.25, 0.5, 0.75])\n",
    "ax.set_ylim([0.25, 0.75])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.4 Testing Accuracy Grouped by Model Type\n",
    "<a name='section624'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Models Test Accuracy ##\n",
    "gnb_vis_pred = gnb_final.fit(X_train, y_train).predict(X_test)\n",
    "svm_vis_pred = svm_final.fit(X_train, y_train).predict(X_test)\n",
    "lr_vis_pred = lr_final.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "## Boosting Models ##\n",
    "xgb_vis_pred = xgb_final.fit(X_train, y_train).predict(X_test)\n",
    "abc_vis_pred = abc_final.fit(X_train, y_train).predict(X_test)\n",
    "gbc_vis_pred = gbc_final.fit(X_train, y_train).predict(X_test)\n",
    "lightgbm_vis_pred = lightgbm_final.predict(X_test)\n",
    "\n",
    "## NN Models ##\n",
    "# Defined in models\n",
    "\n",
    "## Time Series Modules ##\n",
    "# prophet_vis_pred = prophet_final.predict(X_test)\n",
    "# arima_vis_pred = arima_final.predict(X_test)\n",
    "\n",
    "#Save results as dict\n",
    "lr_cr = classification_report(y_test, lr_vis_pred, output_dict=True)\n",
    "gnb_cr = classification_report(y_test, gnb_vis_pred, output_dict=True)\n",
    "svm_cr = classification_report(y_test, svm_vis_pred, output_dict=True)\n",
    "xgb_cr = classification_report(y_test, xgb_vis_pred, output_dict=True)\n",
    "abc_cr = classification_report(y_test, abc_vis_pred, output_dict=True)\n",
    "gbc_cr = classification_report(y_test, gbc_vis_pred, output_dict=True)\n",
    "lightgbm_cr = classification_report(y_test, lightgbm_vis_pred, output_dict=True)\n",
    "# Defined in models\n",
    "# nn_cr = classification_report(y_test, nn_vis_pred, output_dict=True)\n",
    "# vnn_cr = classification_report(y_test, vnn_vis_pred, output_dict=True)\n",
    "# dnn_cr = classification_report(y_test, dnn_vis_pred, output_dict=True)\n",
    "# rnn_cr = classification_report(y_test, rnn_vis_pred, output_dict=True)\n",
    "# gru_cr = classification_report(y_test, gru_vis_pred, output_dict=True)\n",
    "# ltsm_cr = classification_report(y_test, ltsm_vis_pred, output_dict=True)\n",
    "# cnn_cr = classification_report(y_test, cnn_vis_pred, output_dict=True)\n",
    "# CHECK IF FIXED\n",
    "# prophet_cr = classification_report(y_test, prophet_vis_pred, output_dict=True)\n",
    "# arima_cr = classification_report(y_test, arima_vis_pred, output_dict=True)\n",
    "\n",
    "#Collective Base Training Accuracy \n",
    "base_cr = lr_cr[\"accuracy\"] + gnb_cr[\"accuracy\"] + svm_cr[\"accuracy\"]\n",
    "base_cr = base_cr/3\n",
    "print(\"base:\",base_cr)\n",
    "\n",
    "#Collective Boost Training Accuracy \n",
    "boost_cr = xgb_cr[\"accuracy\"] + abc_cr[\"accuracy\"] + gbc_cr[\"accuracy\"] + lightgbm_cr[\"accuracy\"]\n",
    "boost_cr = boost_cr/4\n",
    "print(\"boost:\",boost_cr)\n",
    "\n",
    "#Collective NN Training Accuracy \n",
    "nn_cr = vnn_cr[\"accuracy\"] + dnn_cr[\"accuracy\"] + rnn_cr[\"accuracy\"] + gru_cr[\"accuracy\"] +  lstm_cr[\"accuracy\"] + cnn_cr[\"accuracy\"] \n",
    "nn_cr = nn_cr/6\n",
    "print(\"nn:\",nn_cr)\n",
    "\n",
    "#Collective TS Testing Accuracy \n",
    "#ts_cr =  prophet_cr[\"accuracy\"], arima_cr[\"accuracy\"]\n",
    "#ts_cr = ts_cr/2\n",
    "#Hardcoded\n",
    "ts_cr=(0.33 + 0.27)/2\n",
    "print(\"ts:\",ts_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names of models for columns\n",
    "#models = ['Base','Boost']\n",
    "# For after fix\n",
    "models = ['Base','Boost','NN','TS']\n",
    "\n",
    "\n",
    "#Plot of models by accuracy\n",
    "accuracy = [base_cr,\n",
    "            boost_cr,\n",
    "            nn_cr,\n",
    "            ts_cr]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "plt.bar(models, accuracy, width=0.8)\n",
    "ax.set_title('Cumulative Training Accuracy of Models')\n",
    "ax.set_yticks([0.35, 0.5, 0.75])\n",
    "ax.set_ylim([0.25, 0.75])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Confusion Matrix Plots\n",
    "<a name='section63'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLot confusion matrix of all models\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def confusion_plt (model):\n",
    "    cm = confusion_matrix(y_test, model)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['A','D','H'])\n",
    "    disp.plot(cmap='Reds');\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# Plot all confusion plots choose best few models for display\n",
    "confusion_plt(lr_vis_pred)\n",
    "confusion_plt(gnb_vis_pred)\n",
    "confusion_plt(svm_vis_pred)\n",
    "confusion_plt(xgb_vis_pred)\n",
    "confusion_plt(abc_vis_pred)\n",
    "confusion_plt(gbc_vis_pred)\n",
    "confusion_plt(lightgbm_vis_pred)\n",
    "# confusion_plt(nn_vis_pred)\n",
    "# confusion_plt(vnn_vis_pred)\n",
    "# confusion_plt(dnn_vis_pred)\n",
    "# confusion_plt(rnn_vis_pred)\n",
    "# confusion_plt(gru_vis_pred)\n",
    "# confusion_plt(ltsm_vis_pred)\n",
    "# confusion_plt(cnn_vis_pred)\n",
    "# confusion_plt(prophet_vis_pred)\n",
    "# confusion_plt(arima_vis_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Comparison of Best Models from Each Category\n",
    "<a name='section64'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot best models from each category\n",
    "def output_model_comp (output):\n",
    "    best_model_comp = pd.DataFrame([['Precision', \n",
    "                                     lr_cr[output]['precision'], \n",
    "                                     abc_cr[output]['precision'], \n",
    "                                     vnn_cr[output]['precision'], \n",
    "                                     gnb_cr[output]['precision']], \n",
    "                                    ['Recall', \n",
    "                                     lr_cr[output]['recall'], \n",
    "                                     abc_cr[output]['recall'], \n",
    "                                     vnn_cr[output]['recall'], \n",
    "                                     gnb_cr[output]['recall']], \n",
    "                                    ['F1-Score',\n",
    "                                     lr_cr[output][\"f1-score\"], \n",
    "                                     abc_cr[output][\"f1-score\"], \n",
    "                                     vnn_cr[output][\"f1-score\"], \n",
    "                                     gnb_cr[output][\"f1-score\"]]], \n",
    "                  columns=['Model', 'Base', 'Boost', 'Neural Net','Time Series (TEST)'])\n",
    "    return best_model_comp\n",
    "\n",
    "yaxis = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "fig1 = output_model_comp('0').plot(x='Model', kind='bar', stacked=False, title='Away Win', figsize=(12,4), ylim=(0,1), yticks=yaxis)\n",
    "fig2 = output_model_comp('1').plot(x='Model', kind='bar', stacked=False, title='Draw', figsize=(12,4), ylim=(0,1), yticks=yaxis)\n",
    "fig3 = output_model_comp('2').plot(x='Model', kind='bar', stacked=False, title='Home Win', figsize=(12,4), ylim=(0,1), yticks=yaxis)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2XDY1oOAyEZ"
   },
   "source": [
    "## 7. Final Predictions on Test Set\n",
    "<a name='section7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Data Cleaning\n",
    "<a name='section71'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most up-to-date data of season 2021 is named 'updatedData' \n",
    "# Test set is named as 'testData'\n",
    "\n",
    "# FULL DATASET ALTERNATIVE FROM 2000 - 2021 FOR TRAINING \n",
    "updatedData = data.append(updatedData, ignore_index = True) # Comment out if not needed\n",
    "\n",
    "# only pick the columns that are presented in the training set, drop the others\n",
    "updatedData = updatedData[data.columns.tolist()]\n",
    "\n",
    "# check if there are invalid data\n",
    "assert updatedData.shape[0] == removeInvalidData(updatedData).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Priors Derivation \n",
    "<a name='section72'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the up-to-date data of season 2021 and the test set\n",
    "final_data = pd.concat([updatedData,testData],ignore_index=True,sort=False) \n",
    "\n",
    "\n",
    "attributesToDrop = [\"Date\", \"FTR\", \"FTHG\", \"FTAG\", \"HS\", \"AS\", \"HR\", \"AR\", \"FTHG\", \"FTAG\", \"FTR\", \"HTHG\", \"HTAG\", \"HTR\", \"Referee\", \"HS\", \"AS\", \"HST\", \"AST\", \"HF\", \"AF\", \"HC\", \"AC\", \"HY\", \"AY\", \"HR\", \"AR\"]\n",
    "# convert different date formats into test set format\n",
    "convertDate(final_data)\n",
    "\n",
    "#construct priors for test set\n",
    "final_data = DerivePriors(final_data)\n",
    "\n",
    "# remove intermediate data like FTR, HTR etc\n",
    "# the order of removing unwanted data is also different from the previous, so we have to pre-define them\n",
    "final_data = final_data.drop(attributesToDrop, 1)\n",
    "\n",
    "\n",
    "# Due to the lack of data at the beginning of each year, there are rows containing empty values. \n",
    "# Remove invalid or empty values.\n",
    "removeInvalidData(final_data)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_test = final_data.head()\n",
    "\n",
    "# We don't want the referee column or any of the columns after the \"DIS\" column\n",
    "columns = [i for i in range(45) if i != 9]\n",
    "final_data_test = final_data_test.iloc[:, columns]\n",
    "\n",
    "final_data_test['Date'] = pd.to_datetime(final_data_test['Date'])\n",
    "final_data_test['Month'] = final_data_test['Date'].dt.month\n",
    "final_data_test['Week'] = final_data_test['Date'].dt.isocalendar().week\n",
    "final_data_test['Day'] = final_data_test['Date'].dt.day\n",
    "\n",
    "# Dates are transformed into their sin/cos representation to capture their cyclic nature and reduce dimensionality, further explanation is given in the report\n",
    "month_sin = transformation(final_data_test[\"Month\"])[0]\n",
    "month_cos = transformation(final_data_test[\"Month\"])[1]\n",
    "week_sin = transformation(final_data_test[\"Week\"])[0]\n",
    "week_cos = transformation(final_data_test[\"Week\"])[1]\n",
    "day_sin = transformation(final_data_test[\"Day\"])[0]\n",
    "day_cos = transformation(final_data_test[\"Day\"])[1]\n",
    "\n",
    "teams = pd.DataFrame(home_t.toarray()).add_prefix(\"home_\").join(pd.DataFrame(away_t.toarray()).add_prefix(\"away_\"))\n",
    "\n",
    "# Select only columns that contain priors, can't use in-game stats to predict the future\n",
    "priors = final_data_test.iloc[:, 21:43]\n",
    "\n",
    "# Compute custom features\n",
    "# PHGS_PHSOT is ratio of home goals to home shots on target\n",
    "PHGS_PHSOT = np.where(priors[\"PHSOT\"] != 0, priors[\"PHGS\"]/priors[\"PHSOT\"], 0)\n",
    "# PHGS_PHSOT is ratio of away goals to away shots on target\n",
    "PAGS_PASOT = np.where(priors[\"PASOT\"] != 0, priors[\"PAGS\"]/priors[\"PASOT\"], 0)\n",
    "# PHSOT_PHS is ratio of home shots on target to home shots\n",
    "PHSOT_PHS = np.where(priors[\"PHS\"] != 0, priors[\"PHSOT\"]/ (priors[\"PHS\"] + priors[\"PHSOT\"]), 0)\n",
    "# PASOT_PAS is ratio of away shots on target to away shots\n",
    "PASOT_PAS = np.where(priors[\"PAS\"] != 0, priors[\"PASOT\"]/ (priors[\"PAS\"] + priors[\"PASOT\"]), 0)\n",
    "# PHTF_PATF is ratio of home fouls to away fouls\n",
    "PHTF_PATF = np.where(priors[\"PATF\"] != 0, priors[\"PHTF\"]/priors[\"PATF\"], 0)\n",
    "\n",
    "# Building final dataset\n",
    "final_data_test = pd.DataFrame()\n",
    "final_data_test[\"month_cos\"] = month_cos\n",
    "final_data_test[\"month_sin\"] = month_sin\n",
    "final_data_test[\"week_cos\"] = week_cos\n",
    "final_data_test[\"week_sin\"] = week_sin\n",
    "final_data_test[\"day_cos\"] = day_cos\n",
    "final_data_test[\"day_sin\"] = day_sin\n",
    "final_data_test = final_data_test.join(teams).join(priors)\n",
    "final_data_test[\"PHGS_PHSOT\"] = PHGS_PHSOT.tolist()\n",
    "final_data_test[\"PAGS_PASOT\"] = PAGS_PASOT.tolist()\n",
    "final_data_test[\"PHSOT_PHS\"] = PHSOT_PHS.tolist()\n",
    "final_data_test[\"PASOT_PAS\"] = PASOT_PAS.tolist()\n",
    "final_data_test[\"PHTF_PATF\"] = PHTF_PATF.tolist()\n",
    "\n",
    "scaler = StandardScaler().fit(final_data_test.iloc[:, 92:])\n",
    "final_data_test_scaled = scaler.transform(final_data_test.iloc[:, 92:])\n",
    "\n",
    "final_data_test = pd.DataFrame(final_data_test)\n",
    "final_data_test = np.array(final_data_test.iloc[:, 0:92])\n",
    "\n",
    "final_data_test_scaled = np.hstack((final_data_test, final_data_test_scaled))\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=15, kernel='rbf')\n",
    "kpca.fit(final_data_test_scaled)\n",
    "final_data_test = kpca.transform(final_data_test_scaled)\n",
    "\n",
    "print(final_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CW1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
