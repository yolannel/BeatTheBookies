{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRxeI7EOx9k_"
   },
   "source": [
    "# Beat The Bookies: Predicting EPL Matches\n",
    "_Team C_\n",
    "\n",
    "__Mohammad Ali Syed, Abdul Al-Fahim, Dylan Hoi, Henry Chen, Chris Wong & Yolanne Lee__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOcQFZzsgHWK"
   },
   "source": [
    "**Contents:**\n",
    "\n",
    "[Section 1](#section1): Introduction\n",
    "\n",
    "[Section 2](#section2): Data Import\n",
    "\n",
    "[Section 3](#section3): Data Transformation & Exploration\n",
    "\n",
    "[Section 4](#section4): Methodology Overview\n",
    "\n",
    "[Section 5](#section5): Model Training & Validation\n",
    "\n",
    "[Section 6](#section6): Results\n",
    "\n",
    "[Section 7](#section7): Final Predictions on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-EqnUuI8gWm"
   },
   "source": [
    "## 2. Data Import\n",
    "<a name='section2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import seaborn as sns\n",
    "from collections import Counter, deque\n",
    "\n",
    "#!pip install geopy\n",
    "#!pip install sklearn\n",
    "\n",
    "#For Computing Priors\n",
    "from geopy.distance import geodesic \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report,confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "#For Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "\n",
    "#For Model Selection\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "# EPL Training Data\n",
    "dirName = 'Data_Files/'\n",
    "filePath = dirName + 'epl-training.csv'\n",
    "data = pd.read_csv(filePath)\n",
    "\n",
    "# Additional EPL Training Data\n",
    "# downloaded from www.football-stats.co.uk and concatenated from seasons 2000-2008.\n",
    "# Reformatted to suit our current data architecture, additional 3,047 rows x 22 columns\n",
    "filePath = dirName + 'epl-training-extra.csv'\n",
    "extraData = pd.read_csv(filePath)\n",
    "#data = data.append(extraData, ignore_index = True) #append additional data\n",
    "\n",
    "# Additional EPL Stadium Location Data\n",
    "filePath = dirName + 'epl-stadium.csv'\n",
    "locationData = pd.read_csv(filePath)\n",
    "\n",
    "# Additional EPL Goalkeeper Data\n",
    "filePath = dirName + 'epl-goalkeeping.csv'\n",
    "GKData = pd.read_csv(filePath)\n",
    "\n",
    "#Remove empty nan columns at the end\n",
    "data = data.iloc[:, 0:22]\n",
    "pd.set_option('display.max_columns', None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b63a_ejYMVK"
   },
   "source": [
    "## 3. Data Transformation & Exploration\n",
    "<a name='section3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "def corr_matrix(X, feature):\n",
    "    corr= X.corr()\n",
    "    corr_y = abs(corr[feature])\n",
    "    highest_corr = corr_y[corr_y >0.2]\n",
    "    highest_corr.sort_values(ascending=True)\n",
    "    return highest_corr\n",
    "\n",
    "def rf_model(X_train, X_test, y_train, y_test):\n",
    "    rf=RandomForestClassifier(random_state = 42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    accuracy = calc_accuracy(preds, y_test)\n",
    "    return rf, preds, accuracy\n",
    "\n",
    "def feat_importances(X_train, rf):\n",
    "    feature_importances = list(zip(X_train, rf.feature_importances_))\n",
    "    feature_importances_ranked = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    return feature_importances_ranked\n",
    "\n",
    "def select_feat(X_train, y_train):\n",
    "    feature_selector = SelectFromModel(RandomForestClassifier(random_state = 42)).fit(X_train, y_train)\n",
    "    selected_feat= X_train.columns[(feature_selector.get_support())]\n",
    "    return selected_feat\n",
    "\n",
    "def calc_accuracy(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds) * 100\n",
    "    return accuracy\n",
    "\n",
    "def rf_tree_visualiser(rf, featuresetName, feature_names):\n",
    "    tree = rf.estimators_[10]  #Take 10th random tree\n",
    "    export_graphviz(tree, out_file = featuresetName + '.dot', feature_names = list(feature_names),\n",
    "                    rounded = True, proportion = False, \n",
    "                    precision = 2, filled = True, max_depth = 3)\n",
    "    call(['dot', '-Tpng', featuresetName + '.dot', '-o', featuresetName + '.png'],shell=True)\n",
    "    return featuresetName + '.png'\n",
    "\n",
    "def scatter(data, title, xlabel, ylabel):\n",
    "    # Assume data is an array of tuples\n",
    "    x, y = zip(*data)\n",
    "    # s is the area of the circles in the plot\n",
    "    plt.scatter(x, y, s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "# https://towardsdatascience.com/stop-one-hot-encoding-your-time-based-features-24c699face2f\n",
    "def transformation(column):\n",
    "    max_value = column.max()\n",
    "    sin_values = [math.sin((2*math.pi*x)/max_value) for x in list(column)]\n",
    "    cos_values = [math.cos((2*math.pi*x)/max_value) for x in list(column)]\n",
    "    return sin_values, cos_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Intial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# Feature Visualisation\n",
    "# Visualise correlations between different statistics\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Sort data by teams\n",
    "teams = {}\n",
    "referees = {}\n",
    "for i in data.groupby('HomeTeam').mean().T.columns:\n",
    "    teams[i] = []\n",
    "for i in data.groupby('Referee').mean().T.columns:\n",
    "    referees[i] = []\n",
    "\n",
    "# Team Summary Statistics\n",
    "home_team_stats = pd.DataFrame()\n",
    "away_team_stats = pd.DataFrame()\n",
    "\n",
    "teams = pd.unique(data[[\"HomeTeam\"]].values.ravel())\n",
    "\n",
    "for team in teams:\n",
    "    # Compute summary stats as home team\n",
    "    team_stats = data[(data[\"HomeTeam\"] == team)]\n",
    "    team_stats = team_stats.iloc[:, [3, 6, 10, 12, 14, 16, 18, 20]]\n",
    "    team_stats = team_stats.sum()\n",
    "\n",
    "    performance = data[(data[\"HomeTeam\"] == team)].iloc[:, 5]\n",
    "    num_vals = len(performance)\n",
    "    \n",
    "    performance = performance.value_counts()\n",
    "    performance_keys = performance.keys()\n",
    "    performance_values = performance.values\n",
    "    performance = zip(performance.keys(), performance.values)\n",
    "    \n",
    "    for key, value in performance:\n",
    "        metric = value/num_vals\n",
    "        \n",
    "        if key == \"H\":\n",
    "            team_stats[\"Win Rate\"] = metric\n",
    "            \n",
    "        elif key == \"A\":\n",
    "            team_stats[\"Lose Rate\"] = metric\n",
    "        \n",
    "        else:\n",
    "            team_stats[\"Draw Rate\"] = metric\n",
    "\n",
    "    home_team_stats[team] = pd.DataFrame(team_stats) ##causing problems\n",
    "\n",
    "    # Compute summary stats as away team\n",
    "    team_stats = data[(data[\"AwayTeam\"] == team)]\n",
    "    team_stats = team_stats.iloc[:, [4, 7, 11, 13, 15, 17, 19, 21]]\n",
    "    team_stats = team_stats.sum()\n",
    "\n",
    "    performance = data[(data[\"AwayTeam\"] == team)].iloc[:, 5]\n",
    "    num_vals = len(performance)\n",
    "\n",
    "    performance = performance.value_counts()\n",
    "    performance_keys = performance.keys()\n",
    "    performance_values = performance.values\n",
    "    performance = zip(performance.keys(), performance.values)\n",
    "    \n",
    "    for key, value in performance:\n",
    "        metric = value/num_vals\n",
    "        \n",
    "        if key == \"A\":\n",
    "            team_stats[\"Win Rate\"] = metric\n",
    "            \n",
    "        elif key == \"H\":\n",
    "            team_stats[\"Lose Rate\"] = metric\n",
    "        \n",
    "        else:\n",
    "            team_stats[\"Draw Rate\"] = metric\n",
    "\n",
    "\n",
    "    away_team_stats[team] = pd.DataFrame(team_stats)\n",
    "\n",
    "# Sort by strongest to weakest team, by win rate\n",
    "home_team_stats = home_team_stats.sort_values(by='Win Rate', axis=1, ascending=False)\n",
    "away_team_stats = away_team_stats.sort_values(by='Win Rate', axis=1, ascending=False)\n",
    "home_team_stats\n",
    "#Interesting to note, Man U ranked lower on every metric except fouls and yellow cards compared to Chelsea but had higher win rate -> could suggest the more aggressive the team, the higher the win rate\n",
    "# print(home_team_stats.iloc[:, 0])\n",
    "# print(away_team_stats.iloc[:, 0])\n",
    "# print(np.array(home_team_stats.iloc[:, 0]) - np.array(away_team_stats.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Relationship Between Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix between full time goals and other features\n",
    "highest_corr = corr_matrix(data, \"FTHG\")\n",
    "print(\"FTHG: \\n\" + str(highest_corr))\n",
    "\n",
    "highest_corr = corr_matrix(data, \"FTAG\")\n",
    "print(\"FTAG: \\n\" + str(highest_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into input and output data\n",
    "\n",
    "#Output variable\n",
    "y = data.iloc[:, 5:6]\n",
    "#Reformat y to make it suitable for LabelEncoder\n",
    "\n",
    "y = np.array(y).reshape(len(y))\n",
    "# #Encode y\n",
    "# y = LabelEncoder().fit_transform(y) #################this needs to be done separately for train/test\n",
    "\n",
    "#Input variables\n",
    "#Remove give away columns such as goals scored\n",
    "data_filtered = data.drop(labels = data.columns[[3, 4, 5, 6, 7, 8]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "\n",
    "#Dates\n",
    "data_filtered['Date'] = pd.to_datetime(data_filtered['Date'])\n",
    "#year has been removed as we need to predict future results -> https://towardsdatascience.com/machine-learning-with-datetime-feature-engineering-predicting-healthcare-appointment-no-shows-5e4ca3a85f96\n",
    "year = data_filtered['Date'].dt.year\n",
    "data_filtered['Month'] = data_filtered['Date'].dt.month\n",
    "data_filtered['Week'] = data_filtered['Date'].dt.isocalendar().week\n",
    "data_filtered['Day'] = data_filtered['Date'].dt.day\n",
    "#Extract encoded dates\n",
    "dates_split = data_filtered.iloc[:, 16:19]\n",
    "#Remove encoded dates and original date column\n",
    "data_filtered = data_filtered.drop(labels = data_filtered.columns[[0, 16, 17, 18]], axis = 1)\n",
    "\n",
    "#Encode categorical data\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#Teams\n",
    "home_t = data_filtered.iloc[:, 0:1]\n",
    "home_t = encoder.fit_transform(home_t) #################does this need to be done separately?\n",
    "\n",
    "away_t = data_filtered.iloc[:, 1:2]\n",
    "away_t = encoder.fit_transform(away_t) #################does this need to be done separately?\n",
    "data_filtered = data_filtered.drop(labels = data_filtered.columns[[0,1]], axis = 1)\n",
    "\n",
    "#Referees \n",
    "ref = data_filtered.iloc[:, 0:1]\n",
    "ref = encoder.fit_transform(ref)       #################does this need to be done separately?\n",
    "data_filtered = data_filtered.drop(labels = data_filtered.columns[[0]], axis = 1)\n",
    "\n",
    "#Re-stack columns\n",
    "data_filtered = data_filtered.join(pd.DataFrame(ref.toarray()), rsuffix = '_ref')\n",
    "data_filtered = data_filtered.join(pd.DataFrame(home_t.toarray()), rsuffix = '_home')\n",
    "data_filtered = data_filtered.join(pd.DataFrame(away_t.toarray()), rsuffix = '_away')\n",
    "data_filtered = dates_split.join(data_filtered)\n",
    "data_filtered.columns = data_filtered.columns.astype(str)\n",
    "data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train model on entire featureset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "rf, preds, base_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on entire featureset: \" + str(base_accuracy) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print rf tree N.B. may not work without importing graphviz, random forest images will be on GitHub\n",
    "Image(filename = rf_tree_visualiser(rf, 'featureSetTree', data_filtered.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model without Referee feature\n",
    "data_filtered_no_ref = data_filtered.iloc[:, 0:15].join(data_filtered.iloc[:, 58:])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_no_ref, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy without Referee: \" + str(accuracy) + \"%\")\n",
    "print(\"Difference from before: \" + str(accuracy - base_accuracy) + \"%\")\n",
    "#Ref is having negative impact so remove\n",
    "data_filtered = data_filtered_no_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print rf tree (no ref)\n",
    "Image(filename = rf_tree_visualiser(rf, 'featureSetTreeNoRef', data_filtered_no_ref.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model without Date feature\n",
    "data_filtered_no_date = data_filtered.iloc[:, 3:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_no_date, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy without Dates: \" + str(accuracy) + \"%\")\n",
    "print(\"Difference from before: \" + str(accuracy - base_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print rf tree (no dates)\n",
    "Image(filename = rf_tree_visualiser(rf, 'featureSetTreeNoDate', data_filtered_no_date.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model on only in-game stats to identify most important ones\n",
    "data_filtered_only_game_stats = data_filtered.iloc[:, 3:15]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_only_game_stats, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, all_stats_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on all in-game stats: \" + str(all_stats_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise and analyse initial results\n",
    "\n",
    "#Display feature importances in descending order\n",
    "feature_importances = feat_importances(X_train, rf)\n",
    "print(\"Feature Importances: \")\n",
    "[print('Feature: {:35} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(\"\\nClassification Report: \")\n",
    "print(classification_report(y_test, preds))\n",
    "#Important note: AF/HF rank higher than HC/AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise feature importance\n",
    "scatter(feature_importances, \"Feature importances\", \"Feature\", \"Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Pearson Correlation Heatmap to see the top 10 features related to the match result FTR\n",
    "\n",
    "def plotGraph(X_all, Y_all):\n",
    "\n",
    "    train_data=pd.concat([X_all,Y_all],axis=1)\n",
    "\n",
    "    #FTR correlation matrix\n",
    "    plt.figure(figsize=(12,12))\n",
    "    k = 11 # number of variables for heatmap\n",
    "    cols = abs(train_data.astype(float).corr()).nlargest(k, 'FTR')['FTR'].index\n",
    "    cm = np.corrcoef(train_data[cols].values.T)\n",
    "    sns.set(font_scale=1.25)\n",
    "    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "    plt.show()\n",
    "\n",
    "attributes = data.drop(['Date','HomeTeam', 'AwayTeam', 'Referee','FTR'],1)\n",
    "attributes['HTR'] = attributes['HTR'].map({'H':1,'A':0,'D':2})\n",
    "label = data['FTR']\n",
    "label = label.map({'H':1,'A':0,'D':2})\n",
    "plotGraph(attributes,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection\n",
    "#change names and display selected features more nicely, ideally with their importance, gini impurity...\n",
    "selected_feat = select_feat(X_train, y_train)\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model on selected in-game stats only\n",
    "indexes = []\n",
    "for feat in selected_feat:\n",
    "    indexes.append(data_filtered_only_game_stats.columns.get_loc(feat))\n",
    "    \n",
    "data_filtered_filtered_game_stats = data_filtered_only_game_stats.iloc[:, indexes]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_filtered_game_stats, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, reduced_stats_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on reduced in-game stats: \" + str(reduced_stats_accuracy) + \"%\")\n",
    "print(\"Difference compared to all in-game stats: \" + str(reduced_stats_accuracy - all_stats_accuracy) + \"%\")\n",
    "\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(\"\\nClassification Report: \")\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation of new featureset/tree\n",
    "data_filtered_filtered_game_stats.plot(kind='hist', subplots=True, sharex=False, sharey=False, bins=50, layout=(2,4), figsize=(12, 6))\n",
    "data_filtered_filtered_game_stats.plot(kind='box', subplots=True, layout=(2,4), sharex=False, sharey=False, figsize=(12, 6))\n",
    "data_filtered_filtered_game_stats.plot(kind='density', subplots=True, layout=(2,4), sharex=False, sharey=False, figsize=(12, 6))\n",
    "Image(filename = rf_tree_visualiser(rf, 'selectedFeatureSetTree', data_filtered_filtered_game_stats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produce new dataset\n",
    "#Fix column names\n",
    "#Restack teams and dates\n",
    "\n",
    "#Original teams are needed to be able to compute priors\n",
    "data_new = data.iloc[:, [1, 2]].join(data_filtered_filtered_game_stats)\n",
    "data_new = dates_split.join(data_new)\n",
    "\n",
    "#Stack previously removed giveaway columns\n",
    "data_new = data_new.join(data.iloc[:, [3, 4, 5, 6, 7, 8]])\n",
    "\n",
    "#Feature engineer second half goals\n",
    "#Second half home goals\n",
    "SHHG = np.array(data.iloc[:, [3]]) - np.array(data.iloc[:, [6]])\n",
    "#Second half away goals\n",
    "SHAG = np.array(data.iloc[:, [4]]) - np.array(data.iloc[:, [7]])\n",
    "data_new['SHHG'] = pd.DataFrame(SHHG)\n",
    "data_new['SHAG'] = pd.DataFrame(SHAG)\n",
    "data_new.columns = data_new.columns.astype(str)\n",
    "data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See if second half goals have significant correlation to total goals\n",
    "highest_corr = corr_matrix(data_new, \"FTHG\")\n",
    "print(\"FTHG: \\n\" + str(highest_corr))\n",
    "\n",
    "highest_corr = corr_matrix(data_new, \"FTAG\")\n",
    "print(\"FTAG: \\n\" + str(highest_corr))\n",
    "#Second half goals do have very strong correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Priors Feature Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Pearson Correlation Heatmap to extract the top 10 features \n",
    "# there are two pairs of data highly correlated (see details in report), \n",
    "# so we just pick [FTHG, FTAG, HS, AS, HR, AR] from the top 10 features,\n",
    "# additionally [Date, HomeTeam, AwayTeam, FTR], to derive our features.\n",
    "selectedAttributes = [\"Date\",\"HomeTeam\", \"AwayTeam\",\"FTR\",\"FTHG\",\"FTAG\",\"HS\",\"AS\",\"HR\",\"AR\"]\n",
    "training_data = data[selectedAttributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive features and remove unwanted data\n",
    "def removeInvalidData(data):\n",
    "\n",
    "    # remove data which contains None\n",
    "    data.dropna(axis=0, how='any',inplace=True)\n",
    "\n",
    "    # remove data which contains NaN, infinite or overflowed number \n",
    "    indices_to_keep = ~data.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    data = data[indices_to_keep]\n",
    "\n",
    "    return data\n",
    "\n",
    "#check if there are rows containing None, NaN, infinite or overflowed values\n",
    "assert data.shape[0] == removeInvalidData(data).shape[0]\n",
    "data = removeInvalidData(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Cumulative Full-time W/L Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate cumulative Full-Time win-loss ratio for Home/Away teams prior to every match\n",
    "# TODO: Points-based results based on previous wins & losses \n",
    "# PHWL = Previous Home Team Win Loss Ratio\n",
    "# PAWL = Previous Away Team Win Loss Ratio\n",
    "\n",
    "def get_previousFTResults(playing_stat):\n",
    "    \n",
    "    # Create a dictionary with team names as keys\n",
    "    teams = {}\n",
    "    PHWL = []\n",
    "    PAWL = []\n",
    "    \n",
    "    for i in playing_stat.groupby('HomeTeam').mean().T.columns:\n",
    "        teams[i] = [] #Each team gets their own list\n",
    "\n",
    "    # the value corresponding to keys is a list containing the match result\n",
    "    for i in range(len(playing_stat)):\n",
    "        \n",
    "        #list of respective Home/Away team in match\n",
    "        match_ht = teams[playing_stat.iloc[i].HomeTeam]\n",
    "        match_at = teams[playing_stat.iloc[i].AwayTeam]\n",
    "        \n",
    "        #count no. of wins\n",
    "        \n",
    "        h_wins = Counter(match_ht)\n",
    "        a_wins = Counter(match_at)\n",
    "        \n",
    "        #h_wins = no. of home wins\n",
    "        #a_wins = no. of away wins\n",
    "        h_wins = h_wins['W']\n",
    "        a_wins = a_wins['W']\n",
    "        \n",
    "        #append W/L/D to respective teams\n",
    "        \n",
    "        if playing_stat.iloc[i].FTR == 'H':\n",
    "            match_ht.append('W')\n",
    "            match_at.append('L')\n",
    "        elif playing_stat.iloc[i].FTR == 'A':\n",
    "            match_at.append('W')\n",
    "            match_ht.append('L')\n",
    "        else:\n",
    "            match_at.append('D')\n",
    "            match_ht.append('D')\n",
    "       \n",
    "        h_wlRatio = h_wins / len(match_ht)\n",
    "        a_wlRatio = a_wins / len(match_at)\n",
    "        \n",
    "        #Home/Away cumulative WL ratios prior to every match\n",
    "        PHWL.append(h_wlRatio)\n",
    "        PAWL.append(a_wlRatio)\n",
    "        \n",
    "    data_new.loc[:,'PHWL'] = pd.Series(PHWL)\n",
    "    data_new.loc[:,'PAWL'] = pd.Series(PAWL)\n",
    "\n",
    "    return data_new\n",
    "#get_previousFTResults(data_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.2.3 Cumulative Half-time W/L Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate cumulative Half-Time win-loss ratio for Home/Away teams prior to every match\n",
    "# HHTR = Previous Home Half Time Results\n",
    "# AHTR = Previous Away Half Time Results\n",
    "\n",
    "def get_PreviousHTResults(playing_stat):\n",
    "    \n",
    "    # Create a dictionary with team names as keys\n",
    "    teams = {}\n",
    "    HHTR = []\n",
    "    AHTR = []\n",
    "    \n",
    "    for i in playing_stat.groupby('HomeTeam').mean().T.columns:\n",
    "        teams[i] = [] #Each team gets their own list\n",
    "\n",
    "    # the value corresponding to keys is a list containing the match result\n",
    "    for i in range(len(playing_stat)):\n",
    "        \n",
    "        #list of respective Home/Away team in match\n",
    "        match_ht = teams[playing_stat.iloc[i].HomeTeam]\n",
    "        match_at = teams[playing_stat.iloc[i].AwayTeam]\n",
    "        \n",
    "        #count no. of wins\n",
    "        \n",
    "        h_wins = Counter(match_ht)\n",
    "        a_wins = Counter(match_at)\n",
    "        \n",
    "        #h_wins = no. of home wins\n",
    "        #a_wins = no. of away wins\n",
    "        h_wins = h_wins['W']\n",
    "        a_wins = a_wins['W']\n",
    "        \n",
    "        #append W/L/D to respective teams\n",
    "        \n",
    "        if playing_stat.iloc[i].HTR == 'H':\n",
    "            match_ht.append('W')\n",
    "            match_at.append('L')\n",
    "        elif playing_stat.iloc[i].HTR == 'A':\n",
    "            match_at.append('W')\n",
    "            match_ht.append('L')\n",
    "        else:\n",
    "            match_at.append('D')\n",
    "            match_ht.append('D')\n",
    "            \n",
    "        h_wlRatio = h_wins / len(match_ht)\n",
    "        a_wlRatio = a_wins / len(match_at)\n",
    "       \n",
    "        #Home/Away cumulative WL ratios prior to every match\n",
    "        HHTR.append(h_wlRatio)\n",
    "        AHTR.append(a_wlRatio)\n",
    "        \n",
    "    data_new.loc[:,'HHTR'] = pd.Series(HHTR)\n",
    "    data_new.loc[:,'AHTR'] = pd.Series(AHTR)\n",
    "\n",
    "    return data_new\n",
    "\n",
    "\n",
    "#get_PreviousHTResults(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Cumulative Full-Time goals scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Full-Time Cumulative Goal \n",
    "# PHGS = Previous Home Goal Scored\n",
    "# PAGS = Previous Away Goal Scored\n",
    "\n",
    "def getPreviousCumulativeGoals(data):\n",
    "    teams = {}\n",
    "    PHGS = [] \n",
    "    PAGS = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        FTHG = data.iloc[i]['FTHG']\n",
    "        FTAG = data.iloc[i]['FTAG']\n",
    "\n",
    "        try:\n",
    "            pcgs_h = teams[data.iloc[i].HomeTeam].pop()\n",
    "            pcgs_a = teams[data.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcgs_h = 0\n",
    "            pcgs_a = 0\n",
    "\n",
    "        PHGS.append(pcgs_h)\n",
    "        PAGS.append(pcgs_a)\n",
    "#         print(PAGS)\n",
    "#         print(PHGS)\n",
    "        pcgs_h = pcgs_h + FTHG #Home team's previous goals scored before this match\n",
    "        teams[data.iloc[i].HomeTeam].append(pcgs_h)\n",
    "        pcgs_a = pcgs_a + FTAG #Away team's previous goals scored before this match\n",
    "        teams[data.iloc[i].AwayTeam].append(pcgs_a)\n",
    "\n",
    "    data_new.loc[:,'PHGS'] = pd.Series(PHGS)\n",
    "    data_new.loc[:,'PAGS'] = pd.Series(PAGS)\n",
    "    return data_new\n",
    "\n",
    "#getPreviousCumulativeGoals(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Cumulative Half-time W/L Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Shots in the match\n",
    "# PHS = Home teams previous match Shots, totaled over season\n",
    "# PAS = Away teams previous match Shots, totaled over season\n",
    "\n",
    "def getPreviousShots(data):\n",
    "    teams = {}\n",
    "    PHS = [] \n",
    "    PAS = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HS = data.iloc[i]['HS']\n",
    "        AS = data.iloc[i]['AS']\n",
    "\n",
    "        try:\n",
    "            pcs_h = teams[data.iloc[i].HomeTeam].pop()\n",
    "            pcs_a = teams[data.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcs_h = 0\n",
    "            pcs_a = 0\n",
    "\n",
    "        PHS.append(pcs_h)\n",
    "        PAS.append(pcs_a)\n",
    "        pcs_h = pcs_h + HS #Home team's previous goals scored before this match\n",
    "        teams[data.iloc[i].HomeTeam].append(pcs_h)\n",
    "        pcs_a = pcs_a + AS #Away team's previous goals scored before this match\n",
    "        teams[data.iloc[i].AwayTeam].append(pcs_a)\n",
    "\n",
    "    data_new.loc[:,'PHS'] = pd.Series(PHS)\n",
    "    data_new.loc[:,'PAS'] = pd.Series(PAS)\n",
    "    return data_new\n",
    "\n",
    "#getPreviousShots(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 Previous shots on target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Shots on Target\n",
    "# PHSOT = Home teams Previous Shots on Target, totaled over season\n",
    "# PASOT = Away teams Previous Shots on Target, totaled over season\n",
    "\n",
    "def getPreviousShotsOnTarget(data):\n",
    "    teams = {}\n",
    "    PHSOT = [] \n",
    "    PASOT = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HST = data.iloc[i]['HST']\n",
    "        AST = data.iloc[i]['AST']\n",
    "\n",
    "        try:\n",
    "            pcsot_h = teams[data.iloc[i].HomeTeam].pop()\n",
    "            pcsot_a = teams[data.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcsot_h = 0\n",
    "            pcsot_a = 0\n",
    "\n",
    "        PHSOT.append(pcsot_h)\n",
    "        PASOT.append(pcsot_a)\n",
    "        pcsot_h = pcsot_h + HST #Home team's previous goals scored before this match\n",
    "        teams[data.iloc[i].HomeTeam].append(pcsot_h)\n",
    "        pcsot_a = pcsot_a + AST #Away team's previous goals scored before this match\n",
    "        teams[data.iloc[i].AwayTeam].append(pcsot_a)\n",
    "\n",
    "    data_new.loc[:,'PHSOT'] = pd.Series(PHSOT)\n",
    "    data_new.loc[:,'PASOT'] = pd.Series(PASOT)\n",
    "    return data_new\n",
    "\n",
    "#getPreviousShotsOnTarget(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7 Computing previous fouls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Fouls\n",
    "# PHTF = Home teams Previous Fouls, Totaled over season\n",
    "# PATF = Away teams Previous Fouls, Totaled over season\n",
    "\n",
    "def getPreviousTeamFouls(data):\n",
    "    teams = {}\n",
    "    PHTF = [] \n",
    "    PATF = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HF = data.iloc[i]['HF']\n",
    "        AF = data.iloc[i]['AF']\n",
    "\n",
    "        try:\n",
    "            pcf_h = teams[data.iloc[i].HomeTeam].pop()\n",
    "            pcf_a = teams[data.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcf_h = 0\n",
    "            pcf_a = 0\n",
    "\n",
    "        PHTF.append(pcf_h)\n",
    "        PATF.append(pcf_a)\n",
    "        pcf_h = pcf_h + HF #Home team's previous fouls before this match\n",
    "        teams[data.iloc[i].HomeTeam].append(pcf_h)\n",
    "        pcf_a = pcf_a + AF #Away team's previous fouls before this match\n",
    "        teams[data.iloc[i].AwayTeam].append(pcf_a)\n",
    "\n",
    "    data_new.loc[:,'PHTF'] = pd.Series(PHTF)\n",
    "    data_new.loc[:,'PATF'] = pd.Series(PATF)\n",
    "    return data_new\n",
    "\n",
    "#getPreviousTeamFouls(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.8 Computing previous corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Corners\n",
    "# PHTC = Home teams Previous Corners, Totaled over season\n",
    "# PATC = Away teams Previous Corners, Totaled over season\n",
    "\n",
    "def getPreviousTeamCorners(data):\n",
    "    teams = {}\n",
    "    PHTC = [] \n",
    "    PATC = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HC = data.iloc[i]['HC']\n",
    "        AC = data.iloc[i]['AC']\n",
    "\n",
    "        try:\n",
    "            pcc_h = teams[data.iloc[i].HomeTeam].pop()\n",
    "            pcc_a = teams[data.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcc_h = 0\n",
    "            pcc_a = 0\n",
    "\n",
    "        PHTC.append(pcc_h)\n",
    "        PATC.append(pcc_a)\n",
    "        pcc_h = pcc_h + HC #Home team's previous corners before this match\n",
    "        teams[data.iloc[i].HomeTeam].append(pcc_h)\n",
    "        pcc_a = pcc_a + AC #Away team's previous corners before this match\n",
    "        teams[data.iloc[i].AwayTeam].append(pcc_a)\n",
    "\n",
    "    data_new.loc[:,'PHTC'] = pd.Series(PHTC)\n",
    "    data_new.loc[:,'PATC'] = pd.Series(PATC)\n",
    "    return data_new\n",
    "\n",
    "#getPreviousTeamCorners(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.9 Computing previous goals before half-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Goals before half time\n",
    "# PHTHG = Home teams Previous Goals Before Half Time, Totaled over season\n",
    "# PHTAG = Away teams Previous Goals Before Half Time, Totaled over season\n",
    "\n",
    "def getPreviousHalfTimeGoalsScored(data):\n",
    "    teams = {}\n",
    "    PHTHG = [] \n",
    "    PHTAG = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HTHG = data.iloc[i]['HTHG']\n",
    "        HTAG = data.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            pchtg_h = teams[data.iloc[i].HomeTeam].pop()\n",
    "            pchtg_a = teams[data.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pchtg_h = 0\n",
    "            pchtg_a = 0\n",
    "\n",
    "        PHTHG.append(pchtg_h)\n",
    "        PHTAG.append(pchtg_a)\n",
    "        pchtg_h = pchtg_h + HTHG #Home team's previous first half goals scored before this match\n",
    "        teams[data.iloc[i].HomeTeam].append(pchtg_h)\n",
    "        pchtg_a = pchtg_a + HTAG #Away team's previous first half goals scored before this match\n",
    "        teams[data.iloc[i].AwayTeam].append(pchtg_a)\n",
    "\n",
    "    data_new.loc[:,'PHTHG'] = pd.Series(PHTHG)\n",
    "    data_new.loc[:,'PHTAG'] = pd.Series(PHTAG)\n",
    "    return data_new\n",
    "\n",
    "#getPreviousHalfTimeGoalsScored(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.0 Compute previous goals after half-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Second Half Time Goals in the match\n",
    "# PSHHG = Previous Second Half Time Goals scored by Home team, totaled over season\n",
    "# PSHAG = Previous Second Half Time Goals scored by Away team, totaled over season\n",
    "\n",
    "def getPreviousSecondHalfGoals(data):\n",
    "    teams = {}\n",
    "    PSHHG = [] \n",
    "    PSHAG = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "                \n",
    "        FTHG = data.iloc[i]['FTHG']\n",
    "        FTAG = data.iloc[i]['FTAG']\n",
    "        HTHG = data.iloc[i]['HTHG']\n",
    "        HTAG = data.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            shg_h = teams[data.iloc[i].HomeTeam].pop()\n",
    "            shg_a = teams[data.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            shg_h = 0\n",
    "            shg_a = 0\n",
    "\n",
    "        PSHHG.append(shg_h)\n",
    "        PSHAG.append(shg_a)\n",
    "        shg_h = shg_h + (FTHG - HTHG) #Home team's previous second half goals scored before this match\n",
    "        teams[data.iloc[i].HomeTeam].append(shg_h)\n",
    "        shg_a = shg_a + (FTAG - HTAG) #Away team's previous second half goals scored before this match\n",
    "        teams[data.iloc[i].AwayTeam].append(shg_a)\n",
    "\n",
    "    data_new.loc[:,'PSHHG'] = pd.Series(PSHHG)\n",
    "    data_new.loc[:,'PSHAG'] = pd.Series(PSHAG)\n",
    "    return data_new\n",
    "\n",
    "#getPreviousSecondHalfGoals(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Computing previous goals conceded before half-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate previous goals conceded before half-time\n",
    "# PHTHGC = Home Team Previous Goals Conceded Before Half Time, totaled over season\n",
    "# PHTAGC = Away Team Previous Goals Conceded Before Half Time, Totaled over season\n",
    "\n",
    "def getPreviousHalfTimeGoalConceded(data):\n",
    "    teams = {}\n",
    "    PHTHGC = [] \n",
    "    PHTAGC = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "                      \n",
    "        HTHG = data.iloc[i]['HTHG']\n",
    "        HTAG = data.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            phtgc_h = teams[data.iloc[i].HomeTeam].pop()\n",
    "            phtgc_a = teams[data.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            phtgc_h = 0\n",
    "            phtgc_a = 0\n",
    "\n",
    "        PHTHGC.append(phtgc_h)\n",
    "        PHTAGC.append(phtgc_a)\n",
    "        phtgc_h = phtgc_h + HTAG #Home team's previous half time goals conceded before this match\n",
    "        teams[data.iloc[i].HomeTeam].append(phtgc_h)\n",
    "        phtgc_a = phtgc_a + HTHG #Away team's previous half time goals conceded before this match\n",
    "        teams[data.iloc[i].AwayTeam].append(phtgc_a)\n",
    "\n",
    "    data_new.loc[:,'PHTHGC'] = pd.Series(PHTHGC)\n",
    "    data_new.loc[:,'PHTAGC'] = pd.Series(PHTAGC)\n",
    "    return data_new\n",
    "\n",
    "#getPreviousHalfTimeGoalConceded(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Computing previous goals conceded after half-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate previous goals conceded after half-time\n",
    "# PSHHGC = Previous second half home team goals conceded, totaled over season\n",
    "# PSHAGC = Previous second half away team goals conceded, totaled over season\n",
    "\n",
    "def getPreviousSecondHalfGoalConceded(data):\n",
    "    teams = {}\n",
    "    PSHHGC = [] \n",
    "    PSHAGC = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "  \n",
    "        FTHG = data.iloc[i]['FTHG']\n",
    "        FTAG = data.iloc[i]['FTAG']   \n",
    "        HTHG = data.iloc[i]['HTHG']\n",
    "        HTAG = data.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            pshhgc_h = teams[data.iloc[i].HomeTeam].pop()\n",
    "            pshhgc_a = teams[data.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pshhgc_h = 0\n",
    "            pshhgc_a = 0\n",
    "\n",
    "        PSHHGC.append(pshhgc_h)\n",
    "        PSHAGC.append(pshhgc_a)\n",
    "        pshhgc_h = pshhgc_h + (FTAG - HTAG) #Home team's previous half time goals conceded before this match\n",
    "        teams[data.iloc[i].HomeTeam].append(pshhgc_h)\n",
    "        pshhgc_a = pshhgc_a + (FTHG - HTHG) #Away team's previous half time goals conceded before this match\n",
    "        teams[data.iloc[i].AwayTeam].append(pshhgc_a)\n",
    "\n",
    "    data_new.loc[:,'PSHHGC'] = pd.Series(PSHHGC)\n",
    "    data_new.loc[:,'PSHAGC'] = pd.Series(PSHAGC)\n",
    "    return data_new\n",
    "\n",
    "#getPreviousSecondHalfGoalConceded(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Matches Played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors ***NOT WORKING***\n",
    "# Calculate previous goals conceded after half-time\n",
    "# PMPH = Previous total matches played for home team\n",
    "# PMPA = Previous total matches played for away team\n",
    "def getPreviousMatchesPlayed(data):\n",
    "    teams = {}\n",
    "    PMPH = [] \n",
    "    PMPA = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        try:\n",
    "            pmp_h = teams[data.iloc[i].HomeTeam].pop()\n",
    "            pmp_a = teams[data.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pmp_h = 0\n",
    "            pmp_a = 0\n",
    "\n",
    "        PMPH.append(pmp_h)\n",
    "        PMPA.append(pmp_a)\n",
    "        pmp_h = pmp_h + 1 #Home team's previous number matches played\n",
    "        teams[data.iloc[i].HomeTeam].append(pmp_h)\n",
    "        pmp_a = pmp_a + 1 #Away team's previous number matches played\n",
    "        teams[data.iloc[i].AwayTeam].append(pmp_a)\n",
    "\n",
    "    data_new.loc[:,'PMPH'] = pd.Series(PMPH)\n",
    "    data_new.loc[:,'PMPA'] = pd.Series(PMPA)\n",
    "    return data_new\n",
    "\n",
    "#print(getPreviousMatchesPlayed(data_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Additional Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Distance Travelled for Away Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance needed for away teams to travel to playing stadiums(in km)\n",
    "# The locationData contains the latitude and longitude of teams\n",
    "def getDistance(data, locationData):\n",
    "  array = []\n",
    "  for x in data.iterrows():\n",
    "   \n",
    "    home_lat = (locationData.loc[locationData['Team'] == x[1].HomeTeam]).Latitude\n",
    "    home_long = (locationData.loc[locationData['Team'] == x[1].HomeTeam]).Longitude\n",
    "    home_location = (np.float32(home_lat), np.float32(home_long))\n",
    "    \n",
    "    away_lat = (locationData.loc[locationData['Team'] == x[1].AwayTeam]).Latitude\n",
    "   \n",
    "    away_long = (locationData.loc[locationData['Team'] == x[1].AwayTeam]).Longitude\n",
    "    away_location = (np.float32(away_lat), np.float32(away_long))\n",
    "    array.append(np.float32(geodesic(home_location, away_location).km))\n",
    "  \n",
    "  \n",
    "  ADIS = pd.Series(array)\n",
    "  data.loc[:,'ADIS'] = ADIS\n",
    "\n",
    "  return data\n",
    "\n",
    "#getDistance(data, locationData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Average shots on goal in the past 3 matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average shots on goal for the past 3 matches\n",
    "# HAS, AAS\n",
    "def getPreviousShotOnGoal_3(data):\n",
    "    teams = {}\n",
    "    HAS = [] \n",
    "    AAS = []   \n",
    "    \n",
    "    for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = deque([None, None, None]) #[3rd, 2nd, latest data]\n",
    "            \n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "\n",
    "            \n",
    "        try:\n",
    "            as_h = np.mean(teams[data.iloc[i].HomeTeam])\n",
    "            as_a = np.mean(teams[data.iloc[i].AwayTeam])\n",
    "        except:\n",
    "            as_h = None\n",
    "            as_a = None\n",
    "\n",
    "        HAS.append(as_h)\n",
    "        AAS.append(as_a)\n",
    "\n",
    "        teams[data.iloc[i].HomeTeam].popleft()\n",
    "        teams[data.iloc[i].HomeTeam].append(data.iloc[i].HS)\n",
    "\n",
    "        teams[data.iloc[i].AwayTeam].popleft()\n",
    "        teams[data.iloc[i].AwayTeam].append(data.iloc[i].AS)\n",
    "\n",
    "    data.loc[:,'HAS'] = pd.Series(HAS)\n",
    "    data.loc[:,'AAS'] = pd.Series(AAS)\n",
    "\n",
    "    return data\n",
    "\n",
    "#getPreviousShotOnGoal_3(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 WL Performance of past 3 matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Home-Away teams in past 3 matches\n",
    "# HM1, AM1, HM2, AM2, HM3, AM3\n",
    "def getPerformanceOfLast3Matches(data):\n",
    "    HM1 = []    # performance of the last match of home team\n",
    "    AM1 = []    # performance of the last match of away team\n",
    "\n",
    "    HM2 = []    # performance of the 2nd last match of home team\n",
    "    AM2 = []\n",
    "\n",
    "    HM3 = []    # performance of the 3rd last match of home team\n",
    "    AM3 = []\n",
    "\n",
    "    teams = {}\n",
    "    \n",
    "    for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "               teams[name] = deque([None, None, None])  #[3rd, 2nd, latest data]\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "\n",
    "        HM3.append(teams[data.iloc[i].HomeTeam].popleft())\n",
    "        AM3.append(teams[data.iloc[i].AwayTeam].popleft())\n",
    "        HM2.append(teams[data.iloc[i].HomeTeam][0])\n",
    "        AM2.append(teams[data.iloc[i].AwayTeam][0])\n",
    "        HM1.append(teams[data.iloc[i].HomeTeam][1])\n",
    "        AM1.append(teams[data.iloc[i].AwayTeam][1])\n",
    "\n",
    "        if data.iloc[i].FTR == 'H':\n",
    "            teams[data.iloc[i].HomeTeam].append('W')\n",
    "            teams[data.iloc[i].AwayTeam].append('L')\n",
    "        elif data.iloc[i].FTR == 'A':\n",
    "            teams[data.iloc[i].AwayTeam].append('W')\n",
    "            teams[data.iloc[i].HomeTeam].append('L')\n",
    "        else:\n",
    "            teams[data.iloc[i].AwayTeam].append('D')\n",
    "            teams[data.iloc[i].HomeTeam].append('D')\n",
    "\n",
    "    data.loc[:,'HM1'] = HM1\n",
    "    data.loc[:,'AM1'] = AM1\n",
    "    data.loc[:,'HM2'] = HM2\n",
    "    data.loc[:,'AM2'] = AM2\n",
    "    data.loc[:,'HM3'] = HM3\n",
    "    data.loc[:,'AM3'] = AM3\n",
    "\n",
    "    return data\n",
    "\n",
    "#print(getPerformanceOfLast3Matches(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Cumulative Full Time Goal Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate cumulative Full-Time goal different for Home/Away teams prior to every match\n",
    "# HCGD = Home Cumulative Goal Difference\n",
    "# ACGD = Away Cumulative Goal Difference\n",
    "def getCumulativeGoalsDiff(data):\n",
    "    teams = {}\n",
    "    HCGD = [] \n",
    "    ACGD = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(data)):\n",
    "        # as the result in 3.2.1 shows that the number of matchese per season is always the same, so here we simply use i%380==0 to check if it is a new season and to initialize the feature.\n",
    "        if (i % 380 == 0):\n",
    "            for name in data.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = []\n",
    "\n",
    "        FTHG = data.iloc[i]['FTHG']\n",
    "        FTAG = data.iloc[i]['FTAG']\n",
    "\n",
    "        try:\n",
    "            cgd_h = teams[data.iloc[i].HomeTeam].pop()\n",
    "            cgd_a = teams[data.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            cgd_h = 0\n",
    "            cgd_a = 0\n",
    "\n",
    "        HCGD.append(cgd_h)\n",
    "        ACGD.append(cgd_a)\n",
    "        cgd_h = cgd_h + FTHG - FTAG\n",
    "        teams[data.iloc[i].HomeTeam].append(cgd_h)\n",
    "        cgd_a = cgd_a + FTAG - FTHG\n",
    "        teams[data.iloc[i].AwayTeam].append(cgd_a)\n",
    "\n",
    "    data.loc[:,'HCGD'] = pd.Series(HCGD)\n",
    "    data.loc[:,'ACGD'] = pd.Series(ACGD)\n",
    "\n",
    "    return data\n",
    "\n",
    "#getCumulativeGoalsDiff(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.5 Goalkeeper Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Goalkeeping stats for each team for season starting 2008/2009, ending at present (2021/2020)\n",
    "# def getGK(data, GKData):\n",
    "    \n",
    "#     teams = {}\n",
    "#     HSaveP = [] #Save percentage (Home team)\n",
    "#     HCSP   = [] #Clean sheet percentage\n",
    "#     ASaveP = [] \n",
    "#     ACSP   = [] \n",
    "    \n",
    "#     ## Split data into seasons\n",
    "#     data2008 = data.iloc[:379,:] # 2008-2009 season\n",
    "#     data2009 = data.iloc[380:759,:]\n",
    "#     data2010 = data.iloc[760:1139,:]\n",
    "#     data2011 = data.iloc[1140:1519,:]\n",
    "#     data2012 = data.iloc[1520:1899,:]\n",
    "#     data2013 = data.iloc[1900:2279,:]\n",
    "#     data2014 = data.iloc[2280:2659,:]\n",
    "#     data2015 = data.iloc[2660:3039,:]\n",
    "#     data2016 = data.iloc[3040:3419,:]\n",
    "#     data2017 = data.iloc[3420:3799,:]\n",
    "#     data2018 = data.iloc[3800:4179,:]\n",
    "#     data2019 = data.iloc[4180:4559,:]\n",
    "#     data2020 = data.iloc[4560:4939,:] #2020-2021 season\n",
    "    \n",
    "#     GKData2008 = GKData.iloc[:19,:] # 2008-2009 season\n",
    "#     GKData2009 = GKData.iloc[20:39,:]\n",
    "#     GKData2010 = GKData.iloc[40:59,:]\n",
    "#     GKData2011 = GKData.iloc[60:79,:]\n",
    "#     GKData2012 = GKData.iloc[80:99,:]\n",
    "#     GKData2013 = GKData.iloc[100:119,:]\n",
    "#     GKData2014 = GKData.iloc[120:139,:]\n",
    "#     GKData2015 = GKData.iloc[140:159,:]\n",
    "#     GKData2016 = GKData.iloc[160:179,:]\n",
    "#     GKData2017 = GKData.iloc[180:199,:]\n",
    "#     GKData2018 = GKData.iloc[200:219,:]\n",
    "#     GKData2019 = GKData.iloc[220:239,:]\n",
    "#     GKData2020 = GKData.iloc[240:259,:] #2020-2021 season\n",
    "\n",
    "#     for i in data.groupby('HomeTeam').mean().T.columns: #for 2009 season\n",
    "#             teams[i] = [] #Each team gets their own list\n",
    "\n",
    "#     # the value corresponding to keys is a list containing the match result\n",
    "\n",
    "#     for i in range(len(GKData2008)): # i think swap this for data instead and figure how to go through gkdata 1 at a time\n",
    "#         if GKData2008.iloc[i].Squad == data2008.iloc[i].HomeTeam:\n",
    "#             teams[data2008.iloc[i].HomeTeam].append(GKData2008[[\"SavePercentage\",\"CSPercentage\"]].iloc[i])\n",
    "#     print(teams)\n",
    "#     print(len(GKData2008))\n",
    "        \n",
    "# #         SAVEP = teams[data2009.iloc[i].HomeTeam] #use 2008 season gk data\n",
    "# #         CSP = teams[data2009.iloc[i].HomeTeam]\n",
    "\n",
    "# #         HSaveP.append(SAVEP)\n",
    "# #         HCSP.append(CSP)\n",
    "# #         ASaveP.append(SAVEP)\n",
    "# #         ACSP.append(CSP)\n",
    "        \n",
    "# #         teams[data2009.iloc[i].HomeTeam].append(SAVEP)\n",
    "# #         teams[data2009.iloc[i].HomeTeam].append(CSP)\n",
    "# #         teams[data2009.iloc[i].AwayTeam].append(SAVEP)\n",
    "# #         teams[data2009.iloc[i].AwayTeam].append(CSP)\n",
    "\n",
    "# #         data2009.loc[:,'HSaveP'] = pd.Series(HSaveP)\n",
    "# #         data2009.loc[:,'HCSP'] = pd.Series(HCSP)\n",
    "# #         data2009.loc[:,'ASaveP'] = pd.Series(ASaveP)\n",
    "# #         data2009.loc[:,'ACSP'] = pd.Series(ACSP)\n",
    "#     return data2009\n",
    "\n",
    "# getGK(data, GKData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors - extra features pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pickled_to_df(df,filename,column):\n",
    "    matrix = pd.read_pickle(filename)\n",
    "    matrix[2008] = np.NaN\n",
    "#     print(matrix)\n",
    "    difference = []\n",
    "    for i in range(0,len(data_new)):\n",
    "    #     print(ratings_matrix[\"mean\"].loc[data_new[\"HomeTeam\"].iloc[i]])\n",
    "        if pd.isnull(matrix[year[i]].loc[df[\"HomeTeam\"].iloc[i]]) or pd.isnull(matrix[year[i]].loc[df[\"AwayTeam\"].iloc[i]]):\n",
    "            difference.append(np.nan)\n",
    "            \n",
    "        else:\n",
    "            difference.append(matrix[year[i]].loc[df[\"HomeTeam\"].iloc[i]]-matrix[year[i]].loc[df[\"AwayTeam\"].iloc[i]])\n",
    "\n",
    "#     for i in range(0,len(difference)):\n",
    "#         if difference[i]<-0.1:\n",
    "#             difference[i]='A'\n",
    "#         elif difference[i]>0.1:\n",
    "#             difference[i]='H'\n",
    "#         else:\n",
    "#             difference[i]='D'\n",
    "\n",
    "    df[column]=difference\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have decided not to include these scraped features due to the many missing values without a reliable way to impute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# files = glob.glob(\"./Pickles/*\")\n",
    "# for file in files:\n",
    "#     name = file.split(\"\\\\\")[-1].split(\".\")[0].replace(\"DF\",\"\")\n",
    "#     data_new = add_pickled_to_df(data_new,file,name)\n",
    "\n",
    "# data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Derive Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def DerivePriors(data_new):\n",
    "    #get_previousFTResults(data_new) # dont want Full time results in the test data\n",
    "    get_PreviousHTResults(data_new)\n",
    "    getPreviousCumulativeGoals(data_new)\n",
    "    getPreviousShots(data_new)\n",
    "    getPreviousShotsOnTarget(data_new)\n",
    "    getPreviousTeamFouls(data_new)\n",
    "    getPreviousTeamCorners(data_new)\n",
    "    getPreviousHalfTimeGoalsScored(data_new)\n",
    "    getPreviousSecondHalfGoals(data_new)\n",
    "    getPreviousHalfTimeGoalConceded(data_new)\n",
    "    getPreviousSecondHalfGoalConceded(data_new)\n",
    "    getPreviousMatchesPlayed(data_new)\n",
    "    getDistance(data_new, locationData)\n",
    "    getPreviousShotOnGoal_3(data_new)\n",
    "    getPerformanceOfLast3Matches(data_new)\n",
    "    getCumulativeGoalsDiff(data_new)\n",
    "    #getGK(data, GKData)\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove First Initial Season\n",
    "data_new = DerivePriors(data_new).iloc[380:] #chop off first season \n",
    "y=np.delete(y,slice(0,380),axis=0)\n",
    "data_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Final data preprocessing\n",
    "\n",
    "# TODO:\n",
    "# Implement dates using trig - done\n",
    "# Add one hot encoded teams - done\n",
    "# Compute custom features using priors (goals/shots on target, shots on target / total shots, home team fouls / away team fouls)\n",
    "# PHGS/PHSOT, PAGS/PASOT & PHSOT/PHS, PASOT/PAS & PHTF/PATF - done\n",
    "# Implement scaling but don't apply just yet - done\n",
    "# Apply PCA - done\n",
    "\n",
    "dates = data_new.iloc[:, 0:3]\n",
    "month_sin = transformation(dates[\"Month\"])[0]\n",
    "month_cos = transformation(dates[\"Month\"])[1]\n",
    "week_sin = transformation(dates[\"Week\"])[0]\n",
    "week_cos = transformation(dates[\"Week\"])[1]\n",
    "day_sin = transformation(dates[\"Day\"])[0]\n",
    "day_cos = transformation(dates[\"Day\"])[1]\n",
    "\n",
    "teams = pd.DataFrame(home_t.toarray()).add_prefix(\"home_\").join(pd.DataFrame(away_t.toarray()).add_prefix(\"away_\"))\n",
    "\n",
    "# Select only columns that contain priors, can't use in-game stats to predict the future\n",
    "priors = data_new.iloc[:, 21:39]\n",
    "\n",
    "# PHGS_PHSOT is ratio of home goals to home shots on target\n",
    "PHGS_PHSOT = np.where(priors[\"PHSOT\"] != 0, priors[\"PHGS\"]/priors[\"PHSOT\"], 0)\n",
    "# PHGS_PHSOT is ratio of away goals to away shots on target\n",
    "PAGS_PASOT = np.where(priors[\"PASOT\"] != 0, priors[\"PAGS\"]/priors[\"PASOT\"], 0)\n",
    "# PHSOT_PHS is ratio of home shots on target to home shots\n",
    "PHSOT_PHS = np.where(priors[\"PHS\"] != 0, priors[\"PHSOT\"]/ (priors[\"PHS\"] + priors[\"PHSOT\"]), 0)\n",
    "# PASOT_PAS is ratio of away shots on target to away shots\n",
    "PASOT_PAS = np.where(priors[\"PAS\"] != 0, priors[\"PASOT\"]/ (priors[\"PAS\"] + priors[\"PASOT\"]), 0)\n",
    "# PHTF_PATF is ratio of home fouls to away fouls\n",
    "PHTF_PATF = np.where(priors[\"PATF\"] != 0, priors[\"PHTF\"]/priors[\"PATF\"], 0)\n",
    "\n",
    "# Building final dataset\n",
    "X = pd.DataFrame()\n",
    "X[\"month_cos\"] = month_cos\n",
    "X[\"month_sin\"] = month_sin\n",
    "X[\"week_cos\"] = week_cos\n",
    "X[\"week_sin\"] = week_sin\n",
    "X[\"day_cos\"] = day_cos\n",
    "X[\"day_sin\"] = day_sin\n",
    "X = X.join(teams).join(priors)\n",
    "X[\"PHGS_PHSOT\"] = PHGS_PHSOT.tolist()\n",
    "X[\"PAGS_PASOT\"] = PAGS_PASOT.tolist()\n",
    "X[\"PHSOT_PHS\"] = PHSOT_PHS.tolist()\n",
    "X[\"PASOT_PAS\"] = PASOT_PAS.tolist()\n",
    "X[\"PHTF_PATF\"] = PHTF_PATF.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_val = np.array(y_val).reshape(len(y_val))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_val = encoder.transform(y_val)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "y_train_categorical = keras.utils.to_categorical(y_train)\n",
    "y_val_categorical = keras.utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try without our custom features\n",
    "rf, preds, all_stats_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on all in-game stats: \" + str(all_stats_accuracy) + \"%\")\n",
    "\n",
    "feature_importances = feat_importances(X_train, rf)\n",
    "print(\"Feature Importances: \")\n",
    "[print('Feature: {:35} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if data needs to be scaled so just gonna leave this here\n",
    "scaler = StandardScaler().fit(X_train.iloc[:, 82:])\n",
    "X_train_scaled = scaler.transform(X_train.iloc[:, 82:])\n",
    "X_test_scaled = scaler.transform(X_test.iloc[:, 82:])\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "X_train = np.array(X_train.iloc[:, 0:82])\n",
    "X_test = np.array(X_test.iloc[:, 0:82])\n",
    "\n",
    "X_train_scaled = np.hstack((X_train, X_train_scaled))\n",
    "X_test_scaled = np.hstack((X_test, X_test_scaled))\n",
    "\n",
    "# PCA\n",
    "# https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "# pca = PCA(0.95)\n",
    "# pca.fit(X_train_scaled)\n",
    "# X_train = pca.transform(X_train_scaled)\n",
    "# X_test = pca.transform(X_test_scaled)\n",
    "\n",
    "# pca = PCA(n_components=50)\n",
    "# X = pca.fit_transform(X_scaled)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# X = TSNE(n_components=3, learning_rate='auto', init='random').fit_transform(X)\n",
    "\n",
    "# from sklearn.manifold import MDS\n",
    "# embedding = MDS(n_components=2)\n",
    "# X = embedding.fit_transform(X) -> took way too long\n",
    "\n",
    "# from sklearn.manifold import Isomap\n",
    "# embedding = Isomap(n_components=2)\n",
    "# X = embedding.fit_transform(X) -> gave terrible results\n",
    "\n",
    "# import umap.umap_ as umap\n",
    "# reducer = umap.UMAP(random_state=42,n_components=15)\n",
    "# X = reducer.fit_transform(X_scaled) -> requires outdated numpy\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=15, kernel='rbf')\n",
    "kpca.fit(X_train_scaled)\n",
    "X_train = kpca.transform(X_train_scaled)\n",
    "X_test = kpca.transform(X_test_scaled)\n",
    "#tune hyperparams for this -> gamma\n",
    "\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# clf = LinearDiscriminantAnalysis()\n",
    "# clf.fit(X_train_scaled, y_train)\n",
    "# X_train = clf.transform(X_train_scaled)\n",
    "# X_test = clf.transform(X_test_scaled)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# X = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X)\n",
    "\n",
    "#consider combos of these eg pca then lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90xVEDPEp5Js"
   },
   "source": [
    "## 4. Methodology Overview\n",
    "<a name='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOxVov2mqGZ-"
   },
   "source": [
    "## 4.1 Model Training & Validation\n",
    "<a name='section5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to remove warning to see clearer result\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Implement into pipeline ##\n",
    "# Some general comments:\n",
    "# Gaussian NB is most suitable for non-categorical classification\n",
    "# Based on diagram above (gaussian distributed density plots) the features we use are gaussian distributed however \n",
    "# the teams are not actually gaussian distributed \n",
    "# And the features we use are not conditionally independent as the statistics arent independent (e.g. shots affect\n",
    "# shots on target etc.)\n",
    "# Therefore we expect that the prediction will not be accurate and naives bayes is not suitable\n",
    "\n",
    "#prove calculations and results later\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    " \n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_gnb = gnb.fit(X_train, y_train).predict(X_test)\n",
    "accuracy_score(y_test, y_gnb)\n",
    " \n",
    "\n",
    "#Smoothing parameter scaling\n",
    "# param_grid_nb = {\n",
    "#     'var_smoothing': np.logspace(0,-9, num=100)\n",
    "# }\n",
    "# gnb = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)\n",
    "# y_gnb = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    " \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, y_gnb), \": is the accuracy score gnb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using generic SVM to estimate\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "# gammas = np.power(2, np.linspace(-15, 3, 10))\n",
    "# accuracy_validation = np.empty((5, len(gammas)))\n",
    "\n",
    "# for l, gamma in enumerate(gammas):\n",
    "#     svm = SVC(kernel='rbf', gamma=gamma, C=100)\n",
    "#     svm.fit(X_train, y_train)\n",
    "        \n",
    "#     predict_test = svm.predict(X_test)  # test\n",
    "#     print(accuracy_score(y_test, predict_test))\n",
    "\n",
    "# SVM = svm.SVC(kernel=\"linear\")   #(kernel=\"poly\", degree=3, coef0=1, C=5) (kernel=\"linear\")\n",
    "# SVM.fit(training_data,y_train)# predict the labels on validation dataset\n",
    "# predictions_SVM = SVM.predict(testing_data)# Use accuracy_score function to get the accuracy\n",
    "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)\n",
    "\n",
    "# scores = cross_val_score(SVM, X_whole, y_enc, cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=100))\n",
    "# scores.mean()\n",
    "\n",
    "def fineTuneSVM(X_train, y_train):\n",
    "    # define model and parameters\n",
    "    svm = SVC()   \n",
    "    # SVM solves an optimization problem of quadratic order \n",
    "    # link on SVC: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    # The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.\n",
    "    # Therefore, we will stick with basic kernels like linear and rbf which do the job well without sacrificing processing time.\n",
    "    kernel = ['linear', 'rbf'] \n",
    "    # kernel = ['poly', 'rbf', 'sigmoid'] #Advanced kernels \n",
    "    C = [50, 10, 1.0, 0.1, 0.01]\n",
    "    gamma = ['scale']\n",
    "    \n",
    "    # define grid search\n",
    "    grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    grid_search = GridSearchCV(estimator=svm, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "fineTuneSVM(X_train, y_train)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver = 'liblinear',penalty = 'l1', C = 0.01)\n",
    "y_lr = lr.fit(X_train, y_train).predict(X_test)\n",
    "accuracy_score(y_test,y_lr)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, y_lr), \": is the accuracy score Logistic Regression\")\n",
    "\n",
    "\n",
    "# Finding best hyperparameters\n",
    "\n",
    "# define models and parameters\n",
    "lr = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear','saga']\n",
    "penalty = ['l1','l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=lr, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "### print all the tested results\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Sophisticated Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_pipeline = [\n",
    "    ('xgboost' , (Pipeline([('xgboost' ,xgb.XGBClassifier())]))), \n",
    "#     ('nn' , (Pipeline([('nn' , kears_estimator )]))), \n",
    "#     ('...' , (Pipeline([('...' , ... )]))),\n",
    "#     ('...' , (Pipeline([('...' , ... )]))),\n",
    "#     ('...' , (Pipeline([('...' , ... )])))\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for pipe ,model in model_pipeline:\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds) * 100\n",
    "    results.append(accuracy)\n",
    "    output = \"%s: %f\" % (pipe, accuracy)\n",
    "    print(output)\n",
    "\n",
    "\n",
    "param_pipeline = Pipeline([(\"classifier\", xgb.XGBClassifier())])\n",
    "model_param_grid = [\n",
    "                {\"classifier\": [xgb.XGBClassifier()],\n",
    "#                  \"classifier__penalty\": ['l2','l1'],\n",
    "#                  \"classifier__C\": np.logspace(0, 4, 10)\n",
    "                 },\n",
    "#                 {\"classifier\": [kears_estimator],\n",
    "#                  \"tfidf__ngram_range\": [(1,1), (1,2), (2,2), (1,3)],\n",
    "#                 \"tfidf__use_idf\": [True, False],\n",
    "#                 \"kc__epochs\": [10, 100, ],\n",
    "#                 \"kc__dense_nparams\": [32, 256, 512],\n",
    "#                 \"kc__init\": [ 'uniform', 'zeros', 'normal', ], \n",
    "#                 \"kc__batch_size\":[2, 16, 32],\n",
    "#                 \"kc__optimizer\":['RMSprop', 'Adam', 'Adamax', 'sgd'],\n",
    "#                 \"kc__dropout\": [0.5, 0.4, 0.3, 0.2, 0.1, 0]\n",
    "#                  },\n",
    "#                 {\"classifier\": [...],\n",
    "#                  \"classifier__...\": [...],\n",
    "#                  },\n",
    "#                 {\"classifier\": [...],\n",
    "#                  \"classifier__...\": [...],\n",
    "#                  },\n",
    "#                 {\"classifier\": [...],\n",
    "#                  \"classifier__...\": [...],\n",
    "#                  }\n",
    "                ]\n",
    "# gridsearch = GridSearchCV(param_pipeline, model_param_grid, cv=5, verbose=1,n_jobs=-1, return_train_score=True)\n",
    "# best_model = gridsearch.fit(X_train,y_train)\n",
    "# print(best_model.best_estimator_)\n",
    "# print(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimising hyperparameters for XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "#xgb_cl = xgb.XGBClassifier(use_label_encoder=False)\n",
    "#xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Round 1 values inspired by https://towardsdatascience.com/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390\n",
    "#param_grid = {\n",
    "#    \"max_depth\": [3, 4, 5, 7],\n",
    "#    \"learning_rate\": [0.1, 0.01, 0.05],\n",
    "#    \"gamma\": [0, 0.25, 1],\n",
    "#    \"reg_lambda\": [0, 1, 10],\n",
    "#    \"scale_pos_weight\": [1, 3, 5],\n",
    "#    \"subsample\": [0.8],\n",
    "#    \"colsample_bytree\": [0.5],\n",
    "#}\n",
    "\n",
    "# Init Grid Search\n",
    "#grid_cv = GridSearchCV(xgb_cl, param_grid, n_jobs=-1, cv=3, scoring=\"roc_auc\")\n",
    "\n",
    "# Fit\n",
    "#_ = grid_cv.fit(X_train, y_train)\n",
    "\n",
    "# Round 1 output:\n",
    "#{'colsample_bytree': 0.5,\n",
    "# 'gamma': 0,\n",
    "# 'learning_rate': 0.1,\n",
    "# 'max_depth': 3,\n",
    "# 'reg_lambda': 0,\n",
    "# 'scale_pos_weight': 1,\n",
    "# 'subsample': 0.8}\n",
    "\n",
    "# Round 2:\n",
    "#param_grid = {\n",
    "#    \"max_depth\": [1, 2, 3],\n",
    "#    \"learning_rate\": [0.1, 0.15, 0.2],\n",
    "#    \"gamma\": [0, 0.01, 0.02],\n",
    "#    \"reg_lambda\": [0, 1, 10],\n",
    "#    \"scale_pos_weight\": [1, 3, 5],\n",
    "#    \"subsample\": [0.8],\n",
    "#    \"colsample_bytree\": [0.5],\n",
    "#}\n",
    "\n",
    "#grid_cv_2 = GridSearchCV(xgb_cl, param_grid, n_jobs=-1, cv=3, scoring=\"roc_auc\")\n",
    "\n",
    "#_ = grid_cv_2.fit(X_train, y_train)\n",
    "\n",
    "#grid_cv_2.best_params_\n",
    "\n",
    "# Round 2 output:\n",
    "#{'colsample_bytree': 0.5,\n",
    "# 'gamma': 0,\n",
    "# 'learning_rate': 0.1,\n",
    "# 'max_depth': 1,\n",
    "# 'reg_lambda': 0,\n",
    "# 'scale_pos_weight': 1,\n",
    "# 'subsample': 0.8}\n",
    "\n",
    "# Don't think another round is necessary, now to compute accuracy using these hyperparameters\n",
    "final_cl = xgb.XGBClassifier(\n",
    "    colsample_bytree=0.5,\n",
    "    subsample=0.8,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=1,\n",
    "    reg_lambda=0,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "_ = final_cl.fit(X_train, y_train)\n",
    "\n",
    "preds = final_cl.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, preds)\n",
    "\n",
    "# Accuracy with new hyperparameters is: 0.4824561403508772, accuracy with default hyperparameters is 0.42172740"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimising hyperparameters for AdaBoost\n",
    "# First classified boosting algorithm\n",
    "\n",
    "# Sources used:\n",
    "# https://towardsdatascience.com/adaboost-from-scratch-37a936da3d50\n",
    "# https://analyticsindiamag.com/introduction-to-boosting-implementing-adaboost-in-python/\n",
    "# https://machinelearningmastery.com/adaboost-ensemble-in-python/\n",
    "\n",
    "# Hyperparameter types (Modified Y/N):\n",
    "# Num. of trees (Y)\n",
    "# Weak learner (N)\n",
    "# Learning rate (Y)\n",
    "# Alternate algorithm (Decision Tree/Logistic Regression)\n",
    "\n",
    "#First classified boosting algorithm\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define the model\n",
    "abc = AdaBoostClassifier()\n",
    "abc.fit(X_train, y_train)\n",
    "\n",
    "# grid = {\n",
    "#     \"n_estimators\": [10, 50, 100, 500],\n",
    "#     \"learning_rate\": [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "# }\n",
    "# abc_grid_cv = GridSearchCV(estimator=abc, param_grid = grid, n_jobs=-1, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "# abc_grid_cv.fit(X_train, y_train)\n",
    "\n",
    "# adc_pred = abc_grid_cv.predict(X_test)\n",
    "\n",
    "# print(\"Train: %f , Params: %s, Test: %f\" % (abc_grid_cv.best_score_, abc_grid_cv.best_params_, accuracy_score(y_test, adc_pred)))\n",
    "\n",
    "#Best score and parameters for decision tree round 1\n",
    "# Train: 0.525740 , Params: {'learning_rate': 0.001, 'n_estimators': 500}, Test: 0.521592\n",
    "\n",
    "#Round 2\n",
    "grid2 = {\n",
    "    \"n_estimators\": [400, 500, 600, 800, 1000],\n",
    "    \"learning_rate\": [0.0008, 0.001, 0.0012, 0.0014],\n",
    "}\n",
    "abc_grid_cv2 = GridSearchCV(estimator=abc, param_grid = grid2, n_jobs=-1, cv=3, scoring=\"accuracy\")\n",
    "abc_grid_cv2.fit(X_train, y_train)\n",
    "abc_pred2 = abc_grid_cv2.predict(X_test)\n",
    "print(\"Train: %f , Params: %s, Test: %f\" % (abc_grid_cv2.best_score_, abc_grid_cv2.best_params_, accuracy_score(y_test, abc_pred2)))\n",
    "\n",
    "#Best score and parameters for decision tree round 2\n",
    "#Train: 0.525740 , Params: {'learning_rate': 0.0008, 'n_estimators': 400}, Test: 0.521592\n",
    "#No improvement use as final values\n",
    "\n",
    "# Try with logistic regression algorithm\n",
    "# abc_lr = AdaBoostClassifier(base_estimator=LogisticRegression(solver = 'liblinear',penalty = 'l1', C = 0.01))\n",
    "# abc_lr.fit(X_train, y_train)\n",
    "\n",
    "# grid = {\n",
    "#     \"n_estimators\": [10, 50, 100, 500],\n",
    "#     \"learning_rate\": [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "# }\n",
    "# abc_lr_grid_cv = GridSearchCV(estimator=abc_lr, param_grid = grid, n_jobs=-1, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "# abc_lr_grid_cv.fit(X_train, y_train)\n",
    "\n",
    "# adc_lr_pred = abc_lr_grid_cv.predict(X_test)\n",
    "\n",
    "# print(\"Train: %f , Params: %s, Test: %f\" % (abc_lr_grid_cv.best_score_, abc_lr_grid_cv.best_params_, accuracy_score(y_test, adc_lr_pred)))\n",
    "\n",
    "#Output\n",
    "# Train: 0.489301 , Params: {'learning_rate': 0.1, 'n_estimators': 500}, Test: 0.486505\n",
    "\n",
    "#Use round 2 decision tree output as predictor\n",
    "abc_final = AdaBoostClassifier(n_estimators=400, learning_rate=0.0008)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimising hyperparameters for GradientBoost\n",
    "\n",
    "# Sources used:\n",
    "# https://www.datasciencelearner.com/gradient-boosting-hyperparameters-tuning/\n",
    "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "#define the model\n",
    "# gbc = GradientBoostingClassifier(subsample = 0.8)\n",
    "# gbc.fit(X_train, y_train)\n",
    "# gbc_pre_pred = gbc.predict(X_test)\n",
    "# print(\"Pretuning Test: %f\" % accuracy_score(y_test, gbc_pre_pred))\n",
    "\n",
    "# grid = {\n",
    "#     \"n_estimators\":[5,50,250,500],\n",
    "#     \"max_depth\":[5,6],\n",
    "#     \"learning_rate\":[0.1],\n",
    "#     \"min_samples_split\":[40]\n",
    "# }\n",
    "\n",
    "# gbc_grid_cv = GridSearchCV(estimator=gbc, param_grid = grid, n_jobs=-1, cv=3, scoring=\"accuracy\")\n",
    "# gbc_grid_cv.fit(X_train, y_train)\n",
    "# gbc_pred = gbc_grid_cv.predict(X_test)\n",
    "# print(\"Train: %f , Params: %s, Test: %f\" % (gbc_grid_cv.best_score_, gbc_grid_cv.best_params_, accuracy_score(y_test, gbc_pred)))\n",
    "\n",
    "# Outputs\n",
    "# Train: 0.515040 , Params: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 40, 'n_estimators': 5}, Test: 0.522267\n",
    "\n",
    "#Round 2\n",
    "grid2 = {\n",
    "    \"n_estimators\":[3,5,7,9],\n",
    "    \"max_depth\":[5],\n",
    "    \"learning_rate\":[0.05,0.1,0.2],\n",
    "    \"min_samples_split\":[30,40,50]\n",
    "}\n",
    "\n",
    "gbc_grid_cv2 = GridSearchCV(estimator=gbc, param_grid = grid2, n_jobs=-1, cv=3, scoring=\"accuracy\")\n",
    "gbc_grid_cv2.fit(X_train, y_train)\n",
    "gbc_pred2 = gbc_grid_cv2.predict(X_test)\n",
    "print(\"Train: %f , Params: %s, Test: %f\" % (gbc_grid_cv2.best_score_, gbc_grid_cv2.best_params_, accuracy_score(y_test, gbc_pred2)))\n",
    "\n",
    "# Outputs\n",
    "# Train: 0.525448 , Params: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 50, 'n_estimators': 7}, Test: 0.528340\n",
    "\n",
    "gbc_final = GradientBoostingClassifier(\n",
    "    subsample = 0.8, \n",
    "    learning_rate = 0.1, \n",
    "    max_depth = 5, \n",
    "    min_samples_split = 50, \n",
    "    n_estimators = 7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "# https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install optuna\n",
    "#!{sys.executable} -m pip install lightgbm\n",
    "import lightgbm\n",
    "import optuna\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 0.2, 0.9, step=0.1\n",
    "        ),\n",
    "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 0.2, 0.9, step=0.1\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = lightgbm.LGBMClassifier(objective=\"multiclass\", **param_grid)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric=\"multi_logloss\",\n",
    "            early_stopping_rounds=100,\n",
    "        )\n",
    "        preds = model.predict_proba(X_test)\n",
    "        cv_scores[idx] = log_loss(y_test, preds)\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\n",
    "func = lambda trial: objective(trial, X, y)\n",
    "study.optimize(func, n_trials=20)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = lightgbm.LGBMClassifier(objective=\"multiclass\", **study.best_params)\n",
    "model.fit(X_train, y_train, eval_set=[(X_test,y_test), (X_train, y_train)], eval_metric='multi_logloss')\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Standard, baseline NN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#need to stop randomization\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "#Rename to vanilla\n",
    "model = Sequential()\n",
    "#need to replace input shape by X shape\n",
    "model.add(layers.Dense(128, input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation('linear'))\n",
    "model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val_categorical))\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparam tuning for vanilla nn\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(trial.suggest_categorical('n_nodes', [32, 64, 128, 256]), input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation', ['relu', 'linear', 'tanh'])))\n",
    "    model.add(Dropout(0.1), )\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train, y_train_categorical, batch_size=50, epochs=5, validation_split=0.1, shuffle=False)\n",
    "    return model.evaluate(X_val, y_val_categorical)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Deep NN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#need to stop randomization\n",
    "batch_size = 50\n",
    "epochs = 2\n",
    "#Rename to vanilla\n",
    "model = Sequential()\n",
    "#need to replace input shape by X shape\n",
    "model.add(layers.Dense(128, input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation('relu'))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(64, input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation('relu'))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(32, input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation('relu'))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating RNN #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, SimpleRNN\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(128, input_shape = (13, 15), return_sequences = True))\n",
    "model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 20)\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "preds = preds.reshape(1482, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating GRU #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GRU\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(GRU(128, input_shape = (13, 15), return_sequences = True))\n",
    "model.add(Dropout(0.4), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 20)\n",
    "\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "preds = preds.reshape(1482, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating LSTM\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "\n",
    "X_train_test = np.reshape(X_train, (266, 13, 15))\n",
    "\n",
    "y_train_test = np.reshape(y_train_categorical, (266, 13, 3))\n",
    "\n",
    "X_test_test = X_test.reshape(114, 13, 15)\n",
    "\n",
    "# X_train_lstm, y_train_categorical_lstm = lstm_data_transform(X_train, y_train_categorical, num_steps=5)\n",
    "# X_train_lstm = np.array(X_train_lstm).reshape(-1, 5, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical_lstm).reshape(-1, 5, y_train_categorical.shape[1])\n",
    "\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape = (13, 15), return_sequences = True))\n",
    "model.add(Dropout(0.4), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 50, batch_size = 100)\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "preds = preds.reshape(1482, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating CNN #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "# input_shape = (1, X_train.shape[1])\n",
    "model.add(Conv1D(128, kernel_size=X_train.shape[1], input_shape=(None, 1)))\n",
    "# model.add(Conv1D(64, kernel_size=X_train.shape[1], input_shape=(None, 1)))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train_categorical_lstm, epochs = 20)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=2)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Time Series Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet\n",
    "# https://machinelearningmastery.com/time-series-forecasting-with-prophet-in-python/\n",
    "\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
    "\n",
    "def objective(trial, df):\n",
    "    seasonality = ['additive', 'multiplicative']\n",
    "    param_grid = {\n",
    "        \"changepoint_prior_scale\": trial.suggest_uniform(\"changepoint_prior_scale\", 0.001, 0.5),\n",
    "        \"seasonality_prior_scale\": trial.suggest_uniform(\"seasonality_prior_scale\", 0.01, 10),\n",
    "        \"seasonality_mode\": seasonality[trial.suggest_int(\"seasonality_mode\", 0, 1)]\n",
    "    }\n",
    "\n",
    "    m = Prophet(**param_grid)\n",
    "    m.fit(df)\n",
    "    df_cv = cross_validation(m, \n",
    "                             initial='3458 days', \n",
    "                             period='13 days', \n",
    "                             horizon = '13 days',\n",
    "                             parallel=\"processes\")\n",
    "    \n",
    "    df_p = performance_metrics(df_cv, rolling_window=1)\n",
    "    \n",
    "    return df_p['rmse'].values[0]\n",
    "\n",
    "\n",
    "ds = pd.to_datetime(data[\"Date\"])\n",
    "\n",
    "encoder = LabelEncoder().fit(y)\n",
    "\n",
    "prophet_df = pd.DataFrame()\n",
    "prophet_df[\"ds\"] = ds\n",
    "prophet_df[\"y\"] = encoder.transform(y)\n",
    "\n",
    "#study = optuna.create_study()\n",
    "#func = lambda trial: objective(trial, prophet_df)\n",
    "#study.optimize(func, n_trials=20)\n",
    "\n",
    "#print(f\"\\tBest value (rmse): {study.best_value}\")\n",
    "\n",
    "#print(f\"\\tBest params:\")\n",
    "#for key, value in study.best_params.items():\n",
    "#    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "#Best value (rmse): 0.8722181615529774\n",
    "#Best params:\n",
    "#changepoint_prior_scale: 0.3742218538948863\n",
    "#seasonality_prior_scale: 0.05051527961102975\n",
    "#seasonality_mode: 1 (multiplicative)\n",
    "\n",
    "best_params = {\n",
    "    \"changepoint_prior_scale\": 0.3742218538948863,\n",
    "    \"seasonality_prior_scale\": 0.05051527961102975,\n",
    "    \"seasonality_mode\": 'multiplicative'\n",
    "}\n",
    "\n",
    "model = Prophet(**best_params)\n",
    "\n",
    "# 3458 is the equivalent of 266 seasons if we define a season as 13 matches and reserve the remaining 114 seasons for testing\n",
    "ds_train = prophet_df.head(3458)\n",
    "ds_test = prophet_df.tail(len(prophet_df) - 3458)\n",
    "\n",
    "model.fit(ds_train)\n",
    "\n",
    "future = ds_test.drop(columns=[\"y\"])\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "#print(np.rint(forecast[\"yhat\"]))\n",
    "accuracy = accuracy_score(ds_test[\"y\"], np.rint(forecast[\"yhat\"]))\n",
    "print(accuracy)\n",
    "\n",
    "# Accuracy as of 2021-12-07 is 0.22739541160593793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arima\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "import optuna\n",
    "import pmdarima as pm\n",
    "\n",
    "def objective(trial, X):\n",
    "    param_grid = (\n",
    "        trial.suggest_int(\"p\", 1, 3),\n",
    "        0,\n",
    "        trial.suggest_int(\"q\", 1, 3)\n",
    "    )\n",
    "    \n",
    "    param_grid_seasonal = (\n",
    "        trial.suggest_int(\"P\", 0, 3),\n",
    "        1,\n",
    "        trial.suggest_int(\"Q\", 0, 3),\n",
    "        12\n",
    "    )\n",
    "\n",
    "    train_size = int(len(X) * 0.70)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = SARIMAX(history, order=param_grid, seasonal_order=param_grid_seasonal)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    # calculate out of sample error\n",
    "    rmse = sqrt(mean_squared_error(test, predictions))\n",
    "    return rmse\n",
    "\n",
    "def evaluate_arima_model(X, arima_order):\n",
    "    # prepare training dataset\n",
    "    train_size = int(len(X) * 0.66)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    # calculate out of sample error\n",
    "    rmse = sqrt(mean_squared_error(test, predictions))\n",
    "    return rmse\n",
    "\n",
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    dataset = dataset.astype('float32')\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    rmse = evaluate_arima_model(dataset, order)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, order\n",
    "                    print('ARIMA%s RMSE=%.3f' % (order,rmse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "def get_accuracy(X):\n",
    "    train_size = int(len(X) * 0.70)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = SARIMAX(history)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(round(yhat))\n",
    "        history.append(test[t])\n",
    "    return accuracy_score(test, predictions)\n",
    "\n",
    "\n",
    "idx = data['Date']\n",
    "encoder = LabelEncoder().fit(y)\n",
    "y = encoder.transform(y)\n",
    "ts = pd.Series(y, index=idx).astype('float32')\n",
    "\n",
    "#p_values = [0, 1, 2, 4, 6, 8, 10]\n",
    "#d_values = range(0, 3)\n",
    "#q_values = range(0, 3)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#evaluate_models(ts.values, p_values, d_values, q_values)\n",
    "\n",
    "#study = optuna.create_study()\n",
    "#func = lambda trial: objective(trial, ts.values)\n",
    "#study.optimize(func, n_trials=10)\n",
    "\n",
    "#print(f\"\\tBest value (rmse): {study.best_value}\")\n",
    "\n",
    "#print(f\"\\tBest params:\")\n",
    "#for key, value in study.best_params.items():\n",
    "#    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "#smodel = pm.auto_arima(ts.values, start_p=1, start_q=1,\n",
    "#                         test='adf',\n",
    "#                         max_p=3, max_q=3, m=12,\n",
    "#                         start_P=0, seasonal=True,\n",
    "#                         d=None, D=1, trace=True,\n",
    "#                         error_action='ignore',  \n",
    "#                         suppress_warnings=True, \n",
    "#                         stepwise=True)\n",
    "\n",
    "#print(smodel.summary())\n",
    "\n",
    "# Summary:\n",
    "# Optuna - took 2 hours and didn't return any hyperparameters (never finished)\n",
    "# pmd.auto_arima - ran out of memory\n",
    "# Standard grid search - same as Optuna\n",
    "\n",
    "print(get_accuracy(ts))\n",
    "\n",
    "# Accuracy as of 2021-12-12 is 0.2813765182186235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_aPyBUwqMAD"
   },
   "source": [
    "## 5. Results\n",
    "<a name='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2XDY1oOAyEZ"
   },
   "source": [
    "## 6. Final Predictions on Test Set\n",
    "<a name='section7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CW1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
