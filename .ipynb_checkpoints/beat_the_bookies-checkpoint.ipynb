{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRxeI7EOx9k_"
   },
   "source": [
    "# Beat The Bookies: Predicting EPL Matches\n",
    "_Team C_\n",
    "\n",
    "__Mohammad Ali Syed, Abdul Al-Fahim, Dylan Hoi, Henry Chen, Chris Wong & Yolanne Lee__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOcQFZzsgHWK"
   },
   "source": [
    "**Contents:**\n",
    "\n",
    "- [Section 1](#section1): Introduction\n",
    "\n",
    "- [Section 2](#section2): Data Import\n",
    "\n",
    "- [Section 3](#section3): Data Transformation & Exploration\n",
    ">- [Section 3.1](#section31): Initial Data Exploration\n",
    ">>- [Section 3.1.2](#section312): Relationship Between Attributes\n",
    ">>- [Section 3.1.3](#section313): Initial Data Preprocessing\n",
    ">>- [Section 3.1.4](#section314): Training model on entire featureset\n",
    ">>- [Section 3.1.5](#section315): Random Forest Tree for entire featureset\n",
    ">>- [Section 3.1.6](#section316): Training model without Referee\n",
    ">>- [Section 3.1.7](#section317): Random Forest Tree without Referee\n",
    ">>- [Section 3.1.8](#section318): Training model without Date\n",
    ">>- [Section 3.1.9](#section319): Random Forest Tree without Date\n",
    ">>- [Section 3.1.10](#section3110): Training model on only in-game stats\n",
    ">>- [Section 3.1.11](#section3111): Visualising selected features\n",
    ">- [Section 3.2](#section32): Priors Feature Construction\n",
    ">>- [Section 3.2.1](#section321): Data Cleaning\n",
    ">>- [Section 3.2.2](#section322): Cumulative Full-time W/L Ratio\n",
    ">>- [Section 3.2.3](#section323): Cumulative Half-time W/L Ratio\n",
    ">>- [Section 3.2.4](#section324): Cumulative Full-Time goals scored\n",
    ">>- [Section 3.2.5](#section325): Cumulative Half-time W/L Ratio\n",
    ">>- [Section 3.2.6](#section326): Previous shots on target\n",
    ">>- [Section 3.2.7](#section327): Computing previous fouls\n",
    ">>- [Section 3.2.8](#section328): Computing previous corners\n",
    ">>- [Section 3.2.9](#section329): Computing previous goals before half-time\n",
    ">>- [Section 3.2.10](#section3210): Compute previous goals after half-time\n",
    ">>- [Section 3.2.11](#section3211): Computing previous goals conceded before half-time\n",
    ">>- [Section 3.2.12](#section3212): Computing previous goals conceded after half-time\n",
    ">>- [Section 3.2.13](#section3213): Matches Played\n",
    ">- [Section 3.3](#section33): Additional Features\n",
    ">>- [Section 3.3.1](#section331): Distance Travelled for Away Teams\n",
    ">>- [Section 3.3.2](#section332): Average shots on goal in the past 3 matches\n",
    ">>- [Section 3.3.3](#section333): WL Performance of past 3 matches\n",
    ">>- [Section 3.3.4](#section334): Cumulative Full Time Goal Difference\n",
    ">>- [Section 3.3.5](#section335): Goalkeeper Stats\n",
    ">- [Section 3.4](#section34): Derive Priors\n",
    ">- [Section 3.5](#section35): Final Data Preprocessing\n",
    ">>- [Section 3.5.1](#section351): Split Data\n",
    ">- [Section 3.6](#section36): Scale Data\n",
    "\n",
    "\n",
    "- [Section 4](#section4): Methodology Overview\n",
    "\n",
    "- [Section 5](#section5): Model Training & Validation\n",
    ">- [Section 5.1](#section51): Base Models\n",
    ">>- [Section 5.1.1](#section511): Gaussian Naive Bayes\n",
    ">>- [Section 5.1.2](#section512): Generic SVM\n",
    ">>- [Section 5.1.3](#section513): Logistic Regression\n",
    ">- [Section 5.2](#section52): Boosting Models\n",
    ">>- [Section 5.2.1](#section521): XGBoost\n",
    ">>- [Section 5.2.2](#section522): AdaBoost\n",
    ">>- [Section 5.2.3](#section523): GradientBoost\n",
    ">>- [Section 5.2.4](#section524): LightGBM\n",
    ">- [Section 5.3](#section53): Neural Network Models\n",
    ">>- [Section 5.3.1](#section531): Standard Neural Network\n",
    ">>- [Section 5.3.2](#section532): Vanilla Neural Network\n",
    ">>- [Section 5.3.3](#section533): Deep Neural Network\n",
    ">>- [Section 5.3.4](#section534): Recurrent Neural Network\n",
    ">>- [Section 5.3.5](#section535): Gated Recurrent Neural Network\n",
    ">>- [Section 5.3.6](#section536): Long Short-Term Memory Neural Network\n",
    ">>- [Section 5.3.7](#section537): Convolutional Neural Network\n",
    ">- [Section 5.4](#section54): Time Series Models\n",
    ">>- [Section 5.4.1](#section541): Prophet\n",
    ">>- [Section 5.4.2](#section542): Arima\n",
    "\n",
    "- [Section 6](#section6): Results\n",
    "\n",
    "- [Section 7](#section7): Final Predictions on Test Set\n",
    ">- [Section 7.1](#section71): Data Cleaning\n",
    ">- [Section 7.2](#section72): Priors Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-EqnUuI8gWm"
   },
   "source": [
    "## 2. Data Import\n",
    "<a name='section2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import seaborn as sns\n",
    "from collections import Counter, deque\n",
    "\n",
    "#!pip install geopy\n",
    "#!pip install sklearn\n",
    "\n",
    "#For Computing Priors\n",
    "from geopy.distance import geodesic \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report,confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "#For Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "\n",
    "#For Model Selection\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>Referee</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>HST</th>\n",
       "      <th>AST</th>\n",
       "      <th>HF</th>\n",
       "      <th>AF</th>\n",
       "      <th>HC</th>\n",
       "      <th>AC</th>\n",
       "      <th>HY</th>\n",
       "      <th>AY</th>\n",
       "      <th>HR</th>\n",
       "      <th>AR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19/08/2000</td>\n",
       "      <td>Charlton</td>\n",
       "      <td>Man City</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>Rob Harris</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19/08/2000</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>Graham Barber</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19/08/2000</td>\n",
       "      <td>Coventry</td>\n",
       "      <td>Middlesbrough</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>Barry Knight</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19/08/2000</td>\n",
       "      <td>Derby</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>Andy D'Urso</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19/08/2000</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Everton</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>Dermot Gallagher</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  HomeTeam       AwayTeam  FTHG  FTAG FTR  HTHG  HTAG HTR  \\\n",
       "0  19/08/2000  Charlton       Man City     4     0   H     2     0   H   \n",
       "1  19/08/2000   Chelsea       West Ham     4     2   H     1     0   H   \n",
       "2  19/08/2000  Coventry  Middlesbrough     1     3   A     1     1   D   \n",
       "3  19/08/2000     Derby    Southampton     2     2   D     1     2   A   \n",
       "4  19/08/2000     Leeds        Everton     2     0   H     2     0   H   \n",
       "\n",
       "            Referee  HS  AS  HST  AST  HF  AF  HC  AC  HY  AY  HR  AR  \n",
       "0        Rob Harris  17   8   14    4  13  12   6   6   1   2   0   0  \n",
       "1     Graham Barber  17  12   10    5  19  14   7   7   1   2   0   0  \n",
       "2      Barry Knight   6  16    3    9  15  21   8   4   5   3   1   0  \n",
       "3       Andy D'Urso   6  13    4    6  11  13   5   8   1   1   0   0  \n",
       "4  Dermot Gallagher  17  12    8    6  21  20   6   4   1   3   0   0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data\n",
    "\n",
    "# EPL Training Data\n",
    "dirName = 'Data_Files/'\n",
    "filePath = dirName + 'epl-training.csv'\n",
    "data = pd.read_csv(filePath)\n",
    "\n",
    "#EPL Test Dataset\n",
    "dirName = 'Data_Files/'\n",
    "filePath = dirName + 'epl-test.csv'\n",
    "testData = pd.read_csv(filePath)\n",
    "\n",
    "# Additional EPL Training Data\n",
    "# downloaded from www.football-stats.co.uk and concatenated from seasons 2000-2008.\n",
    "# Reformatted to suit our current data architecture, additional 3,047 rows x 22 columns\n",
    "filePath = dirName + 'epl-training-extra.csv'\n",
    "extraData = pd.read_csv(filePath)\n",
    "data = extraData.append(data, ignore_index = True) #append additional data of seasons 2000-2008\n",
    "\n",
    "# Additional EPL Training Data\n",
    "# downloaded from www.football-stats.co.uk and concatenated from seasons 2021.\n",
    "# Reformatted to suit our current data architecture, additional 158 rows x 22 columns\n",
    "filePath = dirName + 'epl-training-updated.csv'\n",
    "updatedData = pd.read_csv(filePath)\n",
    "\n",
    "# Additional EPL Stadium Location Data\n",
    "filePath = dirName + 'epl-stadium.csv'\n",
    "positionalData = pd.read_csv(filePath)\n",
    "\n",
    "# Additional EPL Goalkeeper Data\n",
    "filePath = dirName + 'epl-goalkeeping.csv'\n",
    "GKData = pd.read_csv(filePath)\n",
    "\n",
    "#Remove empty nan columns at the end\n",
    "data = data.iloc[:, 0:22]\n",
    "pd.set_option('display.max_columns', None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b63a_ejYMVK"
   },
   "source": [
    "## 3. Data Transformation & Exploration\n",
    "<a name='section3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "def corr_matrix(X, feature):\n",
    "    corr= X.corr()\n",
    "    corr_y = abs(corr[feature])\n",
    "    highest_corr = corr_y[corr_y >0.2]\n",
    "    highest_corr.sort_values(ascending=True)\n",
    "    return highest_corr\n",
    "\n",
    "def rf_model(X_train, X_test, y_train, y_test):\n",
    "    rf=RandomForestClassifier(random_state = 42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    accuracy = calc_accuracy(preds, y_test)\n",
    "    return rf, preds, accuracy\n",
    "\n",
    "def feat_importances(X_train, rf):\n",
    "    feature_importances = list(zip(X_train, rf.feature_importances_))\n",
    "    feature_importances_ranked = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    return feature_importances_ranked\n",
    "\n",
    "def select_feat(X_train, y_train):\n",
    "    feature_selector = SelectFromModel(RandomForestClassifier(random_state = 42)).fit(X_train, y_train)\n",
    "    selected_feat= X_train.columns[(feature_selector.get_support())]\n",
    "    return selected_feat\n",
    "\n",
    "def calc_accuracy(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds) * 100\n",
    "    return accuracy\n",
    "\n",
    "def rf_tree_visualiser(rf, featuresetName, feature_names):\n",
    "    tree = rf.estimators_[10]  #Take 10th random tree\n",
    "    export_graphviz(tree, out_file = featuresetName + '.dot', feature_names = list(feature_names),\n",
    "                    rounded = True, proportion = False, \n",
    "                    precision = 2, filled = True, max_depth = 3)\n",
    "    call(['dot', '-Tpng', featuresetName + '.dot', '-o', featuresetName + '.png'],shell=True)\n",
    "    return featuresetName + '.png'\n",
    "\n",
    "def scatter(data, title, xlabel, ylabel):\n",
    "    # Assume data is an array of tuples\n",
    "    x, y = zip(*data)\n",
    "    # s is the area of the circles in the plot\n",
    "    plt.scatter(x, y, s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "# https://towardsdatascience.com/stop-one-hot-encoding-your-time-based-features-24c699face2f\n",
    "def transformation(column):\n",
    "    max_value = column.max()\n",
    "    sin_values = [math.sin((2*math.pi*x)/max_value) for x in list(column)]\n",
    "    cos_values = [math.cos((2*math.pi*x)/max_value) for x in list(column)]\n",
    "    return sin_values, cos_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Intial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Man United</th>\n",
       "      <th>Arsenal</th>\n",
       "      <th>Chelsea</th>\n",
       "      <th>Liverpool</th>\n",
       "      <th>Man City</th>\n",
       "      <th>Tottenham</th>\n",
       "      <th>Everton</th>\n",
       "      <th>Newcastle</th>\n",
       "      <th>Ipswich</th>\n",
       "      <th>Stoke</th>\n",
       "      <th>Leeds</th>\n",
       "      <th>Leicester</th>\n",
       "      <th>Fulham</th>\n",
       "      <th>West Ham</th>\n",
       "      <th>Blackburn</th>\n",
       "      <th>Portsmouth</th>\n",
       "      <th>Charlton</th>\n",
       "      <th>Reading</th>\n",
       "      <th>Southampton</th>\n",
       "      <th>Bolton</th>\n",
       "      <th>Sheffield United</th>\n",
       "      <th>Swansea</th>\n",
       "      <th>Birmingham</th>\n",
       "      <th>Middlesbrough</th>\n",
       "      <th>Aston Villa</th>\n",
       "      <th>Wolves</th>\n",
       "      <th>Bournemouth</th>\n",
       "      <th>Burnley</th>\n",
       "      <th>Norwich</th>\n",
       "      <th>Watford</th>\n",
       "      <th>Crystal Palace</th>\n",
       "      <th>Wigan</th>\n",
       "      <th>West Brom</th>\n",
       "      <th>Hull</th>\n",
       "      <th>Sunderland</th>\n",
       "      <th>Brighton</th>\n",
       "      <th>Cardiff</th>\n",
       "      <th>Blackpool</th>\n",
       "      <th>QPR</th>\n",
       "      <th>Derby</th>\n",
       "      <th>Coventry</th>\n",
       "      <th>Bradford</th>\n",
       "      <th>Huddersfield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FTHG</th>\n",
       "      <td>842.000000</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>835.000000</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>629.000000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>487.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HTHG</th>\n",
       "      <td>392.000000</td>\n",
       "      <td>368.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>364.000000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>273.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HS</th>\n",
       "      <td>6518.000000</td>\n",
       "      <td>6203.000000</td>\n",
       "      <td>6530.000000</td>\n",
       "      <td>6793.000000</td>\n",
       "      <td>5949.000000</td>\n",
       "      <td>6170.000000</td>\n",
       "      <td>5331.000000</td>\n",
       "      <td>4862.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>2181.000000</td>\n",
       "      <td>1284.000000</td>\n",
       "      <td>2412.000000</td>\n",
       "      <td>3433.000000</td>\n",
       "      <td>4406.000000</td>\n",
       "      <td>2645.000000</td>\n",
       "      <td>1826.000000</td>\n",
       "      <td>1436.000000</td>\n",
       "      <td>570.000000</td>\n",
       "      <td>3561.000000</td>\n",
       "      <td>2787.000000</td>\n",
       "      <td>620.000000</td>\n",
       "      <td>1647.000000</td>\n",
       "      <td>1365.000000</td>\n",
       "      <td>2236.000000</td>\n",
       "      <td>4054.000000</td>\n",
       "      <td>1643.000000</td>\n",
       "      <td>1191.000000</td>\n",
       "      <td>1492.000000</td>\n",
       "      <td>1403.000000</td>\n",
       "      <td>1378.000000</td>\n",
       "      <td>2092.000000</td>\n",
       "      <td>1986.000000</td>\n",
       "      <td>2934.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>3206.000000</td>\n",
       "      <td>948.000000</td>\n",
       "      <td>458.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>843.000000</td>\n",
       "      <td>536.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>410.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HST</th>\n",
       "      <td>3195.000000</td>\n",
       "      <td>3130.000000</td>\n",
       "      <td>3052.000000</td>\n",
       "      <td>3094.000000</td>\n",
       "      <td>2749.000000</td>\n",
       "      <td>2958.000000</td>\n",
       "      <td>2475.000000</td>\n",
       "      <td>2253.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>854.000000</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>928.000000</td>\n",
       "      <td>1698.000000</td>\n",
       "      <td>1934.000000</td>\n",
       "      <td>1343.000000</td>\n",
       "      <td>988.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>1433.000000</td>\n",
       "      <td>1546.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>707.000000</td>\n",
       "      <td>1117.000000</td>\n",
       "      <td>1856.000000</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>1279.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>1476.000000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>112.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF</th>\n",
       "      <td>4370.000000</td>\n",
       "      <td>4341.000000</td>\n",
       "      <td>4325.000000</td>\n",
       "      <td>4053.000000</td>\n",
       "      <td>4137.000000</td>\n",
       "      <td>4396.000000</td>\n",
       "      <td>4699.000000</td>\n",
       "      <td>4227.000000</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>2191.000000</td>\n",
       "      <td>1358.000000</td>\n",
       "      <td>2114.000000</td>\n",
       "      <td>3247.000000</td>\n",
       "      <td>3909.000000</td>\n",
       "      <td>2886.000000</td>\n",
       "      <td>1691.000000</td>\n",
       "      <td>1639.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>3079.000000</td>\n",
       "      <td>2617.000000</td>\n",
       "      <td>646.000000</td>\n",
       "      <td>1280.000000</td>\n",
       "      <td>1634.000000</td>\n",
       "      <td>2314.000000</td>\n",
       "      <td>4077.000000</td>\n",
       "      <td>1441.000000</td>\n",
       "      <td>872.000000</td>\n",
       "      <td>1375.000000</td>\n",
       "      <td>1218.000000</td>\n",
       "      <td>1451.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>1804.000000</td>\n",
       "      <td>2788.000000</td>\n",
       "      <td>1180.000000</td>\n",
       "      <td>3211.000000</td>\n",
       "      <td>833.000000</td>\n",
       "      <td>362.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>602.000000</td>\n",
       "      <td>769.000000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HC</th>\n",
       "      <td>2794.000000</td>\n",
       "      <td>2933.000000</td>\n",
       "      <td>2693.000000</td>\n",
       "      <td>2921.000000</td>\n",
       "      <td>2764.000000</td>\n",
       "      <td>2621.000000</td>\n",
       "      <td>2497.000000</td>\n",
       "      <td>2166.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>1002.000000</td>\n",
       "      <td>622.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>1531.000000</td>\n",
       "      <td>1956.000000</td>\n",
       "      <td>1206.000000</td>\n",
       "      <td>839.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>1514.000000</td>\n",
       "      <td>1288.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>724.000000</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>2124.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>540.000000</td>\n",
       "      <td>630.000000</td>\n",
       "      <td>589.000000</td>\n",
       "      <td>559.000000</td>\n",
       "      <td>953.000000</td>\n",
       "      <td>870.000000</td>\n",
       "      <td>1358.000000</td>\n",
       "      <td>459.000000</td>\n",
       "      <td>1437.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>187.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HY</th>\n",
       "      <td>523.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>487.000000</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>501.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>481.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HR</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Win Rate</th>\n",
       "      <td>0.699248</td>\n",
       "      <td>0.671679</td>\n",
       "      <td>0.664160</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.623684</td>\n",
       "      <td>0.581454</td>\n",
       "      <td>0.496241</td>\n",
       "      <td>0.448753</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.426316</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.412281</td>\n",
       "      <td>0.411483</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.387560</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.383459</td>\n",
       "      <td>0.375940</td>\n",
       "      <td>0.373684</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.360902</td>\n",
       "      <td>0.357895</td>\n",
       "      <td>0.353383</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.327485</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.304511</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lose Rate</th>\n",
       "      <td>0.127820</td>\n",
       "      <td>0.117794</td>\n",
       "      <td>0.112782</td>\n",
       "      <td>0.120301</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.203008</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>0.296399</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.326316</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.357895</td>\n",
       "      <td>0.345029</td>\n",
       "      <td>0.306220</td>\n",
       "      <td>0.338346</td>\n",
       "      <td>0.345865</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.311005</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.338346</td>\n",
       "      <td>0.278195</td>\n",
       "      <td>0.326316</td>\n",
       "      <td>0.330409</td>\n",
       "      <td>0.390977</td>\n",
       "      <td>0.378947</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.461988</td>\n",
       "      <td>0.388158</td>\n",
       "      <td>0.425101</td>\n",
       "      <td>0.442105</td>\n",
       "      <td>0.413534</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Draw Rate</th>\n",
       "      <td>0.172932</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.223058</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>0.165789</td>\n",
       "      <td>0.215539</td>\n",
       "      <td>0.255639</td>\n",
       "      <td>0.254848</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.242690</td>\n",
       "      <td>0.282297</td>\n",
       "      <td>0.255639</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.301435</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.278195</td>\n",
       "      <td>0.345865</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.301170</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.240602</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.298246</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.267206</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.281955</td>\n",
       "      <td>0.381579</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Man United      Arsenal      Chelsea    Liverpool     Man City  \\\n",
       "FTHG        842.000000   857.000000   835.000000   797.000000   787.000000   \n",
       "HTHG        392.000000   368.000000   397.000000   364.000000   358.000000   \n",
       "HS         6518.000000  6203.000000  6530.000000  6793.000000  5949.000000   \n",
       "HST        3195.000000  3130.000000  3052.000000  3094.000000  2749.000000   \n",
       "HF         4370.000000  4341.000000  4325.000000  4053.000000  4137.000000   \n",
       "HC         2794.000000  2933.000000  2693.000000  2921.000000  2764.000000   \n",
       "HY          523.000000   549.000000   523.000000   393.000000   487.000000   \n",
       "HR           13.000000    24.000000    24.000000    16.000000    24.000000   \n",
       "Win Rate      0.699248     0.671679     0.664160     0.631579     0.623684   \n",
       "Lose Rate     0.127820     0.117794     0.112782     0.120301     0.210526   \n",
       "Draw Rate     0.172932     0.210526     0.223058     0.248120     0.165789   \n",
       "\n",
       "             Tottenham      Everton    Newcastle     Ipswich        Stoke  \\\n",
       "FTHG        720.000000   629.000000   526.000000   51.000000   248.000000   \n",
       "HTHG        324.000000   273.000000   238.000000   17.000000   115.000000   \n",
       "HS         6170.000000  5331.000000  4862.000000  528.000000  2181.000000   \n",
       "HST        2958.000000  2475.000000  2253.000000  261.000000   854.000000   \n",
       "HF         4396.000000  4699.000000  4227.000000  382.000000  2191.000000   \n",
       "HC         2621.000000  2497.000000  2166.000000  268.000000  1002.000000   \n",
       "HY          521.000000   560.000000   501.000000   25.000000   317.000000   \n",
       "HR           19.000000    28.000000    26.000000    1.000000    14.000000   \n",
       "Win Rate      0.581454     0.496241     0.448753    0.447368     0.426316   \n",
       "Lose Rate     0.203008     0.248120     0.296399    0.315789     0.289474   \n",
       "Draw Rate     0.215539     0.255639     0.254848    0.236842     0.284211   \n",
       "\n",
       "                 Leeds    Leicester       Fulham     West Ham    Blackburn  \\\n",
       "FTHG        145.000000   274.000000   380.000000   487.000000   289.000000   \n",
       "HTHG         65.000000   120.000000   167.000000   219.000000   139.000000   \n",
       "HS         1284.000000  2412.000000  3433.000000  4406.000000  2645.000000   \n",
       "HST         590.000000   928.000000  1698.000000  1934.000000  1343.000000   \n",
       "HF         1358.000000  2114.000000  3247.000000  3909.000000  2886.000000   \n",
       "HC          622.000000  1106.000000  1531.000000  1956.000000  1206.000000   \n",
       "HY          153.000000   244.000000   380.000000   534.000000   342.000000   \n",
       "HR            4.000000    13.000000    23.000000    17.000000    23.000000   \n",
       "Win Rate      0.421053     0.421053     0.421053     0.412281     0.411483   \n",
       "Lose Rate     0.326316     0.315789     0.357895     0.345029     0.306220   \n",
       "Draw Rate     0.252632     0.263158     0.221053     0.242690     0.282297   \n",
       "\n",
       "            Portsmouth     Charlton     Reading  Southampton       Bolton  \\\n",
       "FTHG        184.000000   179.000000   71.000000   376.000000   278.000000   \n",
       "HTHG         75.000000    86.000000   28.000000   178.000000   129.000000   \n",
       "HS         1826.000000  1436.000000  570.000000  3561.000000  2787.000000   \n",
       "HST         988.000000   772.000000  297.000000  1433.000000  1546.000000   \n",
       "HF         1691.000000  1639.000000  583.000000  3079.000000  2617.000000   \n",
       "HC          839.000000   731.000000  382.000000  1514.000000  1288.000000   \n",
       "HY          194.000000   158.000000   65.000000   334.000000   296.000000   \n",
       "HR           12.000000    13.000000    5.000000    24.000000    17.000000   \n",
       "Win Rate      0.406015     0.406015    0.403509     0.394737     0.387560   \n",
       "Lose Rate     0.338346     0.345865    0.385965     0.315789     0.311005   \n",
       "Draw Rate     0.255639     0.248120    0.210526     0.289474     0.301435   \n",
       "\n",
       "           Sheffield United      Swansea   Birmingham  Middlesbrough  \\\n",
       "FTHG              60.000000   179.000000   162.000000     251.000000   \n",
       "HTHG              27.000000    73.000000    66.000000     110.000000   \n",
       "HS               620.000000  1647.000000  1365.000000    2236.000000   \n",
       "HST              251.000000   678.000000   707.000000    1117.000000   \n",
       "HF               646.000000  1280.000000  1634.000000    2314.000000   \n",
       "HC               345.000000   724.000000   748.000000    1071.000000   \n",
       "HY                93.000000   188.000000   191.000000     272.000000   \n",
       "HR                 2.000000     6.000000    13.000000      12.000000   \n",
       "Win Rate           0.385965     0.383459     0.375940       0.373684   \n",
       "Lose Rate          0.438596     0.338346     0.278195       0.326316   \n",
       "Draw Rate          0.175439     0.278195     0.345865       0.300000   \n",
       "\n",
       "           Aston Villa       Wolves  Bournemouth      Burnley      Norwich  \\\n",
       "FTHG        428.000000   161.000000   136.000000   143.000000   144.000000   \n",
       "HTHG        201.000000    60.000000    58.000000    67.000000    58.000000   \n",
       "HS         4054.000000  1643.000000  1191.000000  1492.000000  1403.000000   \n",
       "HST        1856.000000   721.000000   399.000000   527.000000   600.000000   \n",
       "HF         4077.000000  1441.000000   872.000000  1375.000000  1218.000000   \n",
       "HC         2124.000000   772.000000   540.000000   630.000000   589.000000   \n",
       "HY          481.000000   203.000000   152.000000   202.000000   160.000000   \n",
       "HR           23.000000     5.000000     5.000000     0.000000     7.000000   \n",
       "Win Rate      0.368421     0.360902     0.357895     0.353383     0.333333   \n",
       "Lose Rate     0.330409     0.390977     0.378947     0.406015     0.385965   \n",
       "Draw Rate     0.301170     0.248120     0.263158     0.240602     0.280702   \n",
       "\n",
       "               Watford  Crystal Palace        Wigan    West Brom         Hull  \\\n",
       "FTHG        139.000000      186.000000   169.000000   295.000000   107.000000   \n",
       "HTHG         50.000000       78.000000    80.000000   129.000000    52.000000   \n",
       "HS         1378.000000     2092.000000  1986.000000  2934.000000  1106.000000   \n",
       "HST         460.000000      716.000000  1024.000000  1279.000000   438.000000   \n",
       "HF         1451.000000     1967.000000  1804.000000  2788.000000  1180.000000   \n",
       "HC          559.000000      953.000000   870.000000  1358.000000   459.000000   \n",
       "HY          228.000000      267.000000   229.000000   359.000000   143.000000   \n",
       "HR           12.000000        7.000000     9.000000    13.000000    12.000000   \n",
       "Win Rate      0.333333        0.327485     0.315789     0.307692     0.305263   \n",
       "Lose Rate     0.368421        0.461988     0.388158     0.425101     0.442105   \n",
       "Draw Rate     0.298246        0.210526     0.296053     0.267206     0.252632   \n",
       "\n",
       "            Sunderland    Brighton     Cardiff   Blackpool         QPR  \\\n",
       "FTHG        288.000000   85.000000   41.000000   30.000000   60.000000   \n",
       "HTHG        135.000000   34.000000   14.000000   15.000000   24.000000   \n",
       "HS         3206.000000  948.000000  458.000000  253.000000  843.000000   \n",
       "HST        1476.000000  272.000000  142.000000  134.000000  356.000000   \n",
       "HF         3211.000000  833.000000  362.000000  216.000000  602.000000   \n",
       "HC         1437.000000  407.000000  205.000000   94.000000  285.000000   \n",
       "HY          448.000000  126.000000   58.000000   22.000000   87.000000   \n",
       "HR           17.000000    4.000000    0.000000    0.000000    9.000000   \n",
       "Win Rate      0.304511    0.289474    0.289474    0.263158    0.263158   \n",
       "Lose Rate     0.413534    0.328947    0.526316    0.473684    0.421053   \n",
       "Draw Rate     0.281955    0.381579    0.184211    0.263158    0.315789   \n",
       "\n",
       "                Derby    Coventry    Bradford  Huddersfield  \n",
       "FTHG        55.000000   14.000000   20.000000     26.000000  \n",
       "HTHG        27.000000    8.000000    8.000000     14.000000  \n",
       "HS         536.000000  213.000000  221.000000    410.000000  \n",
       "HST        257.000000   95.000000  101.000000    112.000000  \n",
       "HF         769.000000  247.000000  255.000000    394.000000  \n",
       "HC         268.000000  127.000000  108.000000    187.000000  \n",
       "HY          97.000000   25.000000   27.000000     53.000000  \n",
       "HR           2.000000    3.000000    0.000000      5.000000  \n",
       "Win Rate     0.245614    0.210526    0.210526      0.210526  \n",
       "Lose Rate    0.473684    0.421053    0.421053      0.578947  \n",
       "Draw Rate    0.280702    0.368421    0.368421      0.210526  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################# Feature Visualisation\n",
    "# Visualise correlations between different statistics\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Sort data by teams\n",
    "teams = {}\n",
    "referees = {}\n",
    "for i in data.groupby('HomeTeam').mean().T.columns:\n",
    "    teams[i] = []\n",
    "for i in data.groupby('Referee').mean().T.columns:\n",
    "    referees[i] = []\n",
    "\n",
    "# Team Summary Statistics\n",
    "home_team_stats = pd.DataFrame()\n",
    "away_team_stats = pd.DataFrame()\n",
    "\n",
    "teams = pd.unique(data[[\"HomeTeam\"]].values.ravel())\n",
    "\n",
    "for team in teams:\n",
    "    # Compute summary stats as home team\n",
    "    team_stats = data[(data[\"HomeTeam\"] == team)]\n",
    "    team_stats = team_stats.iloc[:, [3, 6, 10, 12, 14, 16, 18, 20]]\n",
    "    team_stats = team_stats.sum()\n",
    "\n",
    "    performance = data[(data[\"HomeTeam\"] == team)].iloc[:, 5]\n",
    "    num_vals = len(performance)\n",
    "    \n",
    "    performance = performance.value_counts()\n",
    "    performance_keys = performance.keys()\n",
    "    performance_values = performance.values\n",
    "    performance = zip(performance.keys(), performance.values)\n",
    "    \n",
    "    for key, value in performance:\n",
    "        metric = value/num_vals\n",
    "        \n",
    "        if key == \"H\":\n",
    "            team_stats[\"Win Rate\"] = metric\n",
    "            \n",
    "        elif key == \"A\":\n",
    "            team_stats[\"Lose Rate\"] = metric\n",
    "        \n",
    "        else:\n",
    "            team_stats[\"Draw Rate\"] = metric\n",
    "\n",
    "    home_team_stats[team] = pd.DataFrame(team_stats) ##causing problems\n",
    "\n",
    "    # Compute summary stats as away team\n",
    "    team_stats = data[(data[\"AwayTeam\"] == team)]\n",
    "    team_stats = team_stats.iloc[:, [4, 7, 11, 13, 15, 17, 19, 21]]\n",
    "    team_stats = team_stats.sum()\n",
    "\n",
    "    performance = data[(data[\"AwayTeam\"] == team)].iloc[:, 5]\n",
    "    num_vals = len(performance)\n",
    "\n",
    "    performance = performance.value_counts()\n",
    "    performance_keys = performance.keys()\n",
    "    performance_values = performance.values\n",
    "    performance = zip(performance.keys(), performance.values)\n",
    "    \n",
    "    for key, value in performance:\n",
    "        metric = value/num_vals\n",
    "        \n",
    "        if key == \"A\":\n",
    "            team_stats[\"Win Rate\"] = metric\n",
    "            \n",
    "        elif key == \"H\":\n",
    "            team_stats[\"Lose Rate\"] = metric\n",
    "        \n",
    "        else:\n",
    "            team_stats[\"Draw Rate\"] = metric\n",
    "\n",
    "\n",
    "    away_team_stats[team] = pd.DataFrame(team_stats)\n",
    "\n",
    "# Sort by strongest to weakest team, by win rate\n",
    "home_team_stats = home_team_stats.sort_values(by='Win Rate', axis=1, ascending=False)\n",
    "away_team_stats = away_team_stats.sort_values(by='Win Rate', axis=1, ascending=False)\n",
    "home_team_stats\n",
    "#Interesting to note, Man U ranked lower on every metric except fouls and yellow cards compared to Chelsea but had higher win rate -> could suggest the more aggressive the team, the higher the win rate\n",
    "# print(home_team_stats.iloc[:, 0])\n",
    "# print(away_team_stats.iloc[:, 0])\n",
    "# print(np.array(home_team_stats.iloc[:, 0]) - np.array(away_team_stats.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Relationship Between Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTHG: \n",
      "FTHG    1.000000\n",
      "HTHG    0.685341\n",
      "HS      0.280689\n",
      "HST     0.424065\n",
      "Name: FTHG, dtype: float64\n",
      "FTAG: \n",
      "FTAG    1.000000\n",
      "HTAG    0.679786\n",
      "AS      0.315637\n",
      "AST     0.440352\n",
      "Name: FTAG, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Correlation matrix between full time goals and other features\n",
    "highest_corr = corr_matrix(data, \"FTHG\")\n",
    "print(\"FTHG: \\n\" + str(highest_corr))\n",
    "\n",
    "highest_corr = corr_matrix(data, \"FTAG\")\n",
    "print(\"FTAG: \\n\" + str(highest_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into input and output data\n",
    "\n",
    "#Output variable\n",
    "y = data.iloc[:, 5:6]\n",
    "#Reformat y to make it suitable for LabelEncoder\n",
    "\n",
    "y = np.array(y).reshape(len(y))\n",
    "# #Encode y\n",
    "# y = LabelEncoder().fit_transform(y) #################this needs to be done separately for train/test\n",
    "\n",
    "#Input variables\n",
    "#Remove give away columns such as goals scored\n",
    "data_filtered = data.drop(labels = data.columns[[3, 4, 5, 6, 7, 8]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Initial Data Preprocessing\n",
    "<a name='section313'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>HST</th>\n",
       "      <th>AST</th>\n",
       "      <th>HF</th>\n",
       "      <th>AF</th>\n",
       "      <th>HC</th>\n",
       "      <th>AC</th>\n",
       "      <th>HY</th>\n",
       "      <th>AY</th>\n",
       "      <th>HR</th>\n",
       "      <th>AR</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>0_home</th>\n",
       "      <th>1_home</th>\n",
       "      <th>2_home</th>\n",
       "      <th>3_home</th>\n",
       "      <th>4_home</th>\n",
       "      <th>5_home</th>\n",
       "      <th>6_home</th>\n",
       "      <th>7_home</th>\n",
       "      <th>8_home</th>\n",
       "      <th>9_home</th>\n",
       "      <th>10_home</th>\n",
       "      <th>11_home</th>\n",
       "      <th>12_home</th>\n",
       "      <th>13_home</th>\n",
       "      <th>14_home</th>\n",
       "      <th>15_home</th>\n",
       "      <th>16_home</th>\n",
       "      <th>17_home</th>\n",
       "      <th>18_home</th>\n",
       "      <th>19_home</th>\n",
       "      <th>20_home</th>\n",
       "      <th>21_home</th>\n",
       "      <th>22_home</th>\n",
       "      <th>23_home</th>\n",
       "      <th>24_home</th>\n",
       "      <th>25_home</th>\n",
       "      <th>26_home</th>\n",
       "      <th>27_home</th>\n",
       "      <th>28_home</th>\n",
       "      <th>29_home</th>\n",
       "      <th>30_home</th>\n",
       "      <th>31_home</th>\n",
       "      <th>32_home</th>\n",
       "      <th>33_home</th>\n",
       "      <th>34_home</th>\n",
       "      <th>35_home</th>\n",
       "      <th>36_home</th>\n",
       "      <th>37_home</th>\n",
       "      <th>38_home</th>\n",
       "      <th>39_home</th>\n",
       "      <th>40_home</th>\n",
       "      <th>41_home</th>\n",
       "      <th>42_home</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Month  Week  Day  HS  AS  HST  AST  HF  AF  HC  AC  HY  AY  HR  AR    0  \\\n",
       "0      8    33   19  17   8   14    4  13  12   6   6   1   2   0   0  0.0   \n",
       "1      8    33   19  17  12   10    5  19  14   7   7   1   2   0   0  0.0   \n",
       "2      8    33   19   6  16    3    9  15  21   8   4   5   3   1   0  0.0   \n",
       "3      8    33   19   6  13    4    6  11  13   5   8   1   1   0   0  0.0   \n",
       "4      8    33   19  17  12    8    6  21  20   6   4   1   3   0   0  0.0   \n",
       "\n",
       "     1    2    3    4    5    6    7    8    9   10   11   12   13   14   15  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    16   17   18   19   20   21   22   23   24   25   26   27   28   29   30  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    31   32   33   34   35   36   37   38   39   40   41   42   43   44   45  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "\n",
       "    46   47   48   49   50   51   52   53   54   55   56   57   58   59   60  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    61   62   63   64   65   66   67   68   69   70   71   72   73   74   75  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    76   77   78   79   80   81   82   83   84   85   86   87   88   89   90  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    91   92   93   94   95   96   97   98   99  100  101  102  103  104  105  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   166  167  168  169  0_home  1_home  2_home  3_home  4_home  5_home  6_home  \\\n",
       "0  0.0  0.0  0.0  0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1  0.0  0.0  0.0  0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2  0.0  0.0  0.0  0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3  0.0  0.0  0.0  0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4  0.0  0.0  0.0  0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   7_home  8_home  9_home  10_home  11_home  12_home  13_home  14_home  \\\n",
       "0     0.0     0.0     0.0      0.0      1.0      0.0      0.0      0.0   \n",
       "1     0.0     0.0     0.0      0.0      0.0      1.0      0.0      0.0   \n",
       "2     0.0     0.0     0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "3     0.0     0.0     0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4     0.0     0.0     0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   15_home  16_home  17_home  18_home  19_home  20_home  21_home  22_home  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      1.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "\n",
       "   23_home  24_home  25_home  26_home  27_home  28_home  29_home  30_home  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   31_home  32_home  33_home  34_home  35_home  36_home  37_home  38_home  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   39_home  40_home  41_home  42_home    0    1    2    3    4    5    6    7  \\\n",
       "0      0.0      0.0      0.0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1      0.0      0.0      0.0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2      0.0      0.0      0.0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3      0.0      0.0      0.0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4      0.0      0.0      0.0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "     8    9   10   11   12   13   14   15   16   17   18   19   20   21   22  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    23   24   25   26   27   28   29   30   31   32   33   34   35   36   37  \\\n",
       "0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    38   39   40   41   42  \n",
       "0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  1.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data preprocessing\n",
    "\n",
    "#Dates\n",
    "data_filtered['Date'] = pd.to_datetime(data_filtered['Date'])\n",
    "#year has been removed as we need to predict future results -> https://towardsdatascience.com/machine-learning-with-datetime-feature-engineering-predicting-healthcare-appointment-no-shows-5e4ca3a85f96\n",
    "year = data_filtered['Date'].dt.year\n",
    "data_filtered['Month'] = data_filtered['Date'].dt.month\n",
    "data_filtered['Week'] = data_filtered['Date'].dt.isocalendar().week\n",
    "data_filtered['Day'] = data_filtered['Date'].dt.day\n",
    "#Extract encoded dates\n",
    "dates_split = data_filtered.iloc[:, 16:19]\n",
    "#Remove encoded dates and original date column\n",
    "data_filtered = data_filtered.drop(labels = data_filtered.columns[[0, 16, 17, 18]], axis = 1)\n",
    "\n",
    "#Encode categorical data\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#Teams\n",
    "home_t = data_filtered.iloc[:, 0:1]\n",
    "home_t = encoder.fit_transform(home_t) #################does this need to be done separately?\n",
    "\n",
    "away_t = data_filtered.iloc[:, 1:2]\n",
    "away_t = encoder.fit_transform(away_t) #################does this need to be done separately?\n",
    "data_filtered = data_filtered.drop(labels = data_filtered.columns[[0,1]], axis = 1)\n",
    "\n",
    "#Referees \n",
    "ref = data_filtered.iloc[:, 0:1]\n",
    "ref = encoder.fit_transform(ref)       #################does this need to be done separately?\n",
    "data_filtered = data_filtered.drop(labels = data_filtered.columns[[0]], axis = 1)\n",
    "\n",
    "#Re-stack columns\n",
    "data_filtered = data_filtered.join(pd.DataFrame(ref.toarray()), rsuffix = '_ref')\n",
    "data_filtered = data_filtered.join(pd.DataFrame(home_t.toarray()), rsuffix = '_home')\n",
    "data_filtered = data_filtered.join(pd.DataFrame(away_t.toarray()), rsuffix = '_away')\n",
    "data_filtered = dates_split.join(data_filtered)\n",
    "data_filtered.columns = data_filtered.columns.astype(str)\n",
    "data_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Training model on entire featureset\n",
    "<a name='section314'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on entire featureset: 56.64160401002506%\n"
     ]
    }
   ],
   "source": [
    "#Train model on entire featureset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "rf, preds, base_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on entire featureset: \" + str(base_accuracy) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Random Forest Tree for entire featureset\n",
    "<a name='section315'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print rf tree N.B. may not work without importing graphviz, random forest images will be on GitHub\n",
    "Image(filename = rf_tree_visualiser(rf, 'featureSetTree', data_filtered.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 Training model without Referee\n",
    "<a name='section316'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without Referee: 56.97577276524645%\n",
      "Difference from before: 0.3341687552213841%\n"
     ]
    }
   ],
   "source": [
    "#Train model without Referee feature\n",
    "data_filtered_no_ref = data_filtered.iloc[:, 0:15].join(data_filtered.iloc[:, 58:])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_no_ref, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy without Referee: \" + str(accuracy) + \"%\")\n",
    "print(\"Difference from before: \" + str(accuracy - base_accuracy) + \"%\")\n",
    "#Ref is having negative impact so remove\n",
    "data_filtered = data_filtered_no_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7 Random Forest Tree without Referee\n",
    "<a name='section317'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print rf tree (no ref)\n",
    "Image(filename = rf_tree_visualiser(rf, 'featureSetTreeNoRef', data_filtered_no_ref.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.8 Training model without Date\n",
    "<a name='section318'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without Dates: 56.68337510442774%\n",
      "Difference from before: 0.04177109440267657%\n"
     ]
    }
   ],
   "source": [
    "#Train model without Date feature\n",
    "data_filtered_no_date = data_filtered.iloc[:, 3:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_no_date, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy without Dates: \" + str(accuracy) + \"%\")\n",
    "print(\"Difference from before: \" + str(accuracy - base_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.9 Random Forest Tree without Date\n",
    "<a name='section319'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print rf tree (no dates)\n",
    "Image(filename = rf_tree_visualiser(rf, 'featureSetTreeNoDate', data_filtered_no_date.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.10 Training model on only in-game stats\n",
    "<a name='section3110'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on all in-game stats: 55.51378446115288%\n"
     ]
    }
   ],
   "source": [
    "#Train model on only in-game stats to identify most important ones\n",
    "data_filtered_only_game_stats = data_filtered.iloc[:, 3:15]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_only_game_stats, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, all_stats_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on all in-game stats: \" + str(all_stats_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.11 Visualising selected features\n",
    "<a name='section3111'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances: \n",
      "Feature: HST                                 Importance: 0.12062860482667323\n",
      "Feature: HS                                  Importance: 0.1095698299548388\n",
      "Feature: AF                                  Importance: 0.10773279032401849\n",
      "Feature: AS                                  Importance: 0.10644024578251926\n",
      "Feature: HF                                  Importance: 0.10552340949204712\n",
      "Feature: AST                                 Importance: 0.10340580600855823\n",
      "Feature: HC                                  Importance: 0.09977784314864165\n",
      "Feature: AC                                  Importance: 0.09278372110367616\n",
      "Feature: AY                                  Importance: 0.06835535366590526\n",
      "Feature: HY                                  Importance: 0.061179795298330265\n",
      "Feature: AR                                  Importance: 0.01327811150903203\n",
      "Feature: HR                                  Importance: 0.011324488885759381\n",
      "\n",
      "Confusion Matrix: \n",
      "[[393  63 238]\n",
      " [175  73 355]\n",
      " [149  85 863]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56       694\n",
      "           1       0.33      0.12      0.18       603\n",
      "           2       0.59      0.79      0.68      1097\n",
      "\n",
      "    accuracy                           0.56      2394\n",
      "   macro avg       0.49      0.49      0.47      2394\n",
      "weighted avg       0.51      0.56      0.52      2394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Visualise and analyse initial results\n",
    "\n",
    "#Display feature importances in descending order\n",
    "feature_importances = feat_importances(X_train, rf)\n",
    "print(\"Feature Importances: \")\n",
    "[print('Feature: {:35} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(\"\\nClassification Report: \")\n",
    "print(classification_report(y_test, preds))\n",
    "#Important note: AF/HF rank higher than HC/AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise feature importance\n",
    "scatter(feature_importances, \"Feature importances\", \"Feature\", \"Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Pearson Correlation Heatmap to see the top 10 features related to the match result FTR\n",
    "\n",
    "def plotGraph(X_all, Y_all):\n",
    "\n",
    "    train_data=pd.concat([X_all,Y_all],axis=1)\n",
    "\n",
    "    #FTR correlation matrix\n",
    "    plt.figure(figsize=(12,12))\n",
    "    k = 11 # number of variables for heatmap\n",
    "    cols = abs(train_data.astype(float).corr()).nlargest(k, 'FTR')['FTR'].index\n",
    "    cm = np.corrcoef(train_data[cols].values.T)\n",
    "    sns.set(font_scale=1.25)\n",
    "    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, cmap=\"Blues\", yticklabels=cols.values, xticklabels=cols.values)\n",
    "    plt.show()\n",
    "\n",
    "attributes = data.drop(['Date','HomeTeam', 'AwayTeam', 'Referee','FTR'],1)\n",
    "attributes['HTR'] = attributes['HTR'].map({'H':1,'A':0,'D':2})\n",
    "label = data['FTR']\n",
    "label = label.map({'H':1,'A':0,'D':2})\n",
    "plotGraph(attributes,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Feature Selection\n",
    "#change names and display selected features more nicely, ideally with their importance, gini impurity...\n",
    "selected_feat = select_feat(X_train, y_train)\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on reduced in-game stats: 54.92898913951546%\n",
      "Difference compared to all in-game stats: -0.5847953216374222%\n",
      "\n",
      "Confusion Matrix: \n",
      "[[379  72 243]\n",
      " [163  79 361]\n",
      " [149  91 857]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55       694\n",
      "           1       0.33      0.13      0.19       603\n",
      "           2       0.59      0.78      0.67      1097\n",
      "\n",
      "    accuracy                           0.55      2394\n",
      "   macro avg       0.49      0.49      0.47      2394\n",
      "weighted avg       0.51      0.55      0.51      2394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train model on selected in-game stats only\n",
    "indexes = []\n",
    "for feat in selected_feat:\n",
    "    indexes.append(data_filtered_only_game_stats.columns.get_loc(feat))\n",
    "    \n",
    "data_filtered_filtered_game_stats = data_filtered_only_game_stats.iloc[:, indexes]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_filtered_filtered_game_stats, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "rf, preds, reduced_stats_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on reduced in-game stats: \" + str(reduced_stats_accuracy) + \"%\")\n",
    "print(\"Difference compared to all in-game stats: \" + str(reduced_stats_accuracy - all_stats_accuracy) + \"%\")\n",
    "\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(\"\\nClassification Report: \")\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation of new featureset/tree\n",
    "data_filtered_filtered_game_stats.plot(kind='hist', subplots=True, sharex=False, sharey=False, bins=50, layout=(2,4), figsize=(12, 6))\n",
    "data_filtered_filtered_game_stats.plot(kind='box', subplots=True, layout=(2,4), sharex=False, sharey=False, figsize=(12, 6))\n",
    "data_filtered_filtered_game_stats.plot(kind='density', subplots=True, layout=(2,4), sharex=False, sharey=False, figsize=(12, 6))\n",
    "Image(filename = rf_tree_visualiser(rf, 'selectedFeatureSetTree', data_filtered_filtered_game_stats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>HST</th>\n",
       "      <th>AST</th>\n",
       "      <th>HF</th>\n",
       "      <th>AF</th>\n",
       "      <th>HC</th>\n",
       "      <th>AC</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>SHHG</th>\n",
       "      <th>SHAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>Charlton</td>\n",
       "      <td>Man City</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>Coventry</td>\n",
       "      <td>Middlesbrough</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>Derby</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Everton</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Month  Week  Day  HomeTeam       AwayTeam  HS  AS  HST  AST  HF  AF  HC  \\\n",
       "0      8    33   19  Charlton       Man City  17   8   14    4  13  12   6   \n",
       "1      8    33   19   Chelsea       West Ham  17  12   10    5  19  14   7   \n",
       "2      8    33   19  Coventry  Middlesbrough   6  16    3    9  15  21   8   \n",
       "3      8    33   19     Derby    Southampton   6  13    4    6  11  13   5   \n",
       "4      8    33   19     Leeds        Everton  17  12    8    6  21  20   6   \n",
       "\n",
       "   AC  FTHG  FTAG FTR  HTHG  HTAG HTR  SHHG  SHAG  \n",
       "0   6     4     0   H     2     0   H     2     0  \n",
       "1   7     4     2   H     1     0   H     3     2  \n",
       "2   4     1     3   A     1     1   D     0     2  \n",
       "3   8     2     2   D     1     2   A     1     0  \n",
       "4   4     2     0   H     2     0   H     0     0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Produce new dataset\n",
    "#Fix column names\n",
    "#Restack teams and dates\n",
    "\n",
    "#Original teams are needed to be able to compute priors\n",
    "data_new = data.iloc[:, [1, 2]].join(data_filtered_filtered_game_stats)\n",
    "data_new = dates_split.join(data_new)\n",
    "\n",
    "#Stack previously removed giveaway columns\n",
    "data_new = data_new.join(data.iloc[:, [3, 4, 5, 6, 7, 8]])\n",
    "\n",
    "#Feature engineer second half goals\n",
    "#Second half home goals\n",
    "SHHG = np.array(data.iloc[:, [3]]) - np.array(data.iloc[:, [6]])\n",
    "#Second half away goals\n",
    "SHAG = np.array(data.iloc[:, [4]]) - np.array(data.iloc[:, [7]])\n",
    "data_new['SHHG'] = pd.DataFrame(SHHG)\n",
    "data_new['SHAG'] = pd.DataFrame(SHAG)\n",
    "data_new.columns = data_new.columns.astype(str)\n",
    "data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTHG: \n",
      "HS      0.280689\n",
      "HST     0.424065\n",
      "FTHG    1.000000\n",
      "HTHG    0.685341\n",
      "SHHG    0.768719\n",
      "Name: FTHG, dtype: float64\n",
      "FTAG: \n",
      "AS      0.315637\n",
      "AST     0.440352\n",
      "FTAG    1.000000\n",
      "HTAG    0.679786\n",
      "SHAG    0.775978\n",
      "Name: FTAG, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#See if second half goals have significant correlation to total goals\n",
    "highest_corr = corr_matrix(data_new, \"FTHG\")\n",
    "print(\"FTHG: \\n\" + str(highest_corr))\n",
    "\n",
    "highest_corr = corr_matrix(data_new, \"FTAG\")\n",
    "print(\"FTAG: \\n\" + str(highest_corr))\n",
    "#Second half goals do have very strong correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Priors Feature Construction\n",
    "<a name='section32'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Pearson Correlation Heatmap to extract the top 10 features \n",
    "# there are two pairs of data highly correlated (see details in report), \n",
    "# so we just pick [FTHG, FTAG, HS, AS, HR, AR] from the top 10 features,\n",
    "# additionally [Date, HomeTeam, AwayTeam, FTR], to derive our features.\n",
    "selectedAttributes = [\"Date\",\"HomeTeam\", \"AwayTeam\",\"FTR\",\"FTHG\",\"FTAG\",\"HS\",\"AS\",\"HR\",\"AR\"]\n",
    "training_data = data[selectedAttributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Data Cleaning\n",
    "<a name='section321'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive features and remove unwanted data\n",
    "def removeInvalidData(data):\n",
    "\n",
    "    # remove data which contains None\n",
    "    data.dropna(axis=0, how='any',inplace=True)\n",
    "\n",
    "    # remove data which contains NaN, infinite or overflowed number \n",
    "    indices_to_keep = ~data.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    data = data[indices_to_keep]\n",
    "\n",
    "    return data\n",
    "\n",
    "#check if there are rows containing None, NaN, infinite or overflowed values\n",
    "assert data.shape[0] == removeInvalidData(data).shape[0]\n",
    "data = removeInvalidData(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the different date formats and convert the type from str to timestamp  \n",
    "def convertDate(data):\n",
    "    if not isinstance(data.Date[0],str):\n",
    "        return\n",
    "\n",
    "    newDate = []\n",
    "    for _, matchInfo in data.iterrows():\n",
    "        if len(matchInfo.Date) == 8 :\n",
    "            newDate.append(pd.to_datetime(matchInfo.Date, format=\"%d/%m/%y\" ))\n",
    "        elif len(matchInfo.Date) == 9 :\n",
    "            newDate.append(pd.to_datetime(matchInfo.Date, format=\"%d %b %y\" ))  # the date format in test data\n",
    "        elif len(matchInfo.Date) == 10 :\n",
    "            newDate.append(pd.to_datetime(matchInfo.Date, format=\"%d/%m/%Y\" ))\n",
    "    \n",
    "    data['Date'] = pd.Series(newDate).values\n",
    "\n",
    "    return data\n",
    "\n",
    "# converted the date formats for later exploration and transformation\n",
    "# convertDate(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Cumulative Full-time W/L Ratio\n",
    "<a name='section322'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate cumulative Full-Time win-loss ratio for Home/Away teams prior to every match\n",
    "# TODO: Points-based results based on previous wins & losses \n",
    "# PHWL = Previous Home Team Win Loss Ratio\n",
    "# PAWL = Previous Away Team Win Loss Ratio\n",
    "\n",
    "def get_previousFTResults(playing_stat):\n",
    "    \n",
    "    # Create a dictionary with team names as keys\n",
    "    teams = {}\n",
    "    PHWL = []\n",
    "    PAWL = []\n",
    "    \n",
    "    for i in playing_stat.groupby('HomeTeam').mean().T.columns:\n",
    "        teams[i] = [] #Each team gets their own list\n",
    "\n",
    "    # the value corresponding to keys is a list containing the match result\n",
    "    for i in range(len(playing_stat)):\n",
    "        \n",
    "        #list of respective Home/Away team in match\n",
    "        match_ht = teams[playing_stat.iloc[i].HomeTeam]\n",
    "        match_at = teams[playing_stat.iloc[i].AwayTeam]\n",
    "        \n",
    "        #count no. of wins\n",
    "        \n",
    "        h_wins = Counter(match_ht)\n",
    "        a_wins = Counter(match_at)\n",
    "        \n",
    "        #h_wins = no. of home wins\n",
    "        #a_wins = no. of away wins\n",
    "        h_wins = h_wins['W']\n",
    "        a_wins = a_wins['W']\n",
    "        \n",
    "        #append W/L/D to respective teams\n",
    "        \n",
    "        if y[i] == 'H':\n",
    "            match_ht.append('W')\n",
    "            match_at.append('L')\n",
    "        elif y[i] == 'A':\n",
    "            match_at.append('W')\n",
    "            match_ht.append('L')\n",
    "        else:\n",
    "            match_at.append('D')\n",
    "            match_ht.append('D')\n",
    "       \n",
    "        h_wlRatio = h_wins / len(match_ht)\n",
    "        a_wlRatio = a_wins / len(match_at)\n",
    "        \n",
    "        #Home/Away cumulative WL ratios prior to every match\n",
    "        PHWL.append(h_wlRatio)\n",
    "        PAWL.append(a_wlRatio)\n",
    "        \n",
    "    playing_stat.loc[:,'PHWL'] = pd.Series(PHWL)\n",
    "    playing_stat.loc[:,'PAWL'] = pd.Series(PAWL)\n",
    "\n",
    "    return playing_stat\n",
    "\n",
    "#get_previousFTResults(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.2.3 Cumulative Half-time W/L Ratio\n",
    "<a name='section323'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate cumulative Half-Time win-loss ratio for Home/Away teams prior to every match\n",
    "# HHTR = Previous Home Half Time Results\n",
    "# AHTR = Previous Away Half Time Results\n",
    "\n",
    "def get_PreviousHTResults(playing_stat):\n",
    "    \n",
    "    # Create a dictionary with team names as keys\n",
    "    teams = {}\n",
    "    HHTR = []\n",
    "    AHTR = []\n",
    "    \n",
    "    for i in playing_stat.groupby('HomeTeam').mean().T.columns:\n",
    "        teams[i] = [] #Each team gets their own list\n",
    "\n",
    "    # the value corresponding to keys is a list containing the match result\n",
    "    for i in range(len(playing_stat)):\n",
    "        \n",
    "        #list of respective Home/Away team in match\n",
    "        match_ht = teams[playing_stat.iloc[i].HomeTeam]\n",
    "        match_at = teams[playing_stat.iloc[i].AwayTeam]\n",
    "        \n",
    "        #count no. of wins\n",
    "        \n",
    "        h_wins = Counter(match_ht)\n",
    "        a_wins = Counter(match_at)\n",
    "        \n",
    "        #h_wins = no. of home wins\n",
    "        #a_wins = no. of away wins\n",
    "        h_wins = h_wins['W']\n",
    "        a_wins = a_wins['W']\n",
    "        \n",
    "        #append W/L/D to respective teams\n",
    "        \n",
    "        if playing_stat.iloc[i].HTR == 'H':\n",
    "            match_ht.append('W')\n",
    "            match_at.append('L')\n",
    "        elif playing_stat.iloc[i].HTR == 'A':\n",
    "            match_at.append('W')\n",
    "            match_ht.append('L')\n",
    "        else:\n",
    "            match_at.append('D')\n",
    "            match_ht.append('D')\n",
    "            \n",
    "        h_wlRatio = h_wins / len(match_ht)\n",
    "        a_wlRatio = a_wins / len(match_at)\n",
    "       \n",
    "        #Home/Away cumulative WL ratios prior to every match\n",
    "        HHTR.append(h_wlRatio)\n",
    "        AHTR.append(a_wlRatio)\n",
    "        \n",
    "    playing_stat.loc[:,'HHTR'] = pd.Series(HHTR)\n",
    "    playing_stat.loc[:,'AHTR'] = pd.Series(AHTR)\n",
    "\n",
    "    return playing_stat\n",
    "\n",
    "\n",
    "#get_PreviousHTResults(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Cumulative Full-Time goals scored\n",
    "<a name='section324'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Full-Time Cumulative Goal \n",
    "# PHGS = Previous Home Goal Scored\n",
    "# PAGS = Previous Away Goal Scored\n",
    "\n",
    "def getPreviousCumulativeGoals(priorData):\n",
    "    teams = {}\n",
    "    PHGS = [] \n",
    "    PAGS = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        FTHG = priorData.iloc[i]['FTHG']\n",
    "        FTAG = priorData.iloc[i]['FTAG']\n",
    "\n",
    "        try:\n",
    "            pcgs_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pcgs_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcgs_h = 0\n",
    "            pcgs_a = 0\n",
    "\n",
    "        PHGS.append(pcgs_h)\n",
    "        PAGS.append(pcgs_a)\n",
    "#         print(PAGS)\n",
    "#         print(PHGS)\n",
    "        pcgs_h = pcgs_h + FTHG #Home team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pcgs_h)\n",
    "        pcgs_a = pcgs_a + FTAG #Away team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pcgs_a)\n",
    "\n",
    "    priorData.loc[:,'PHGS'] = pd.Series(PHGS)\n",
    "    priorData.loc[:,'PAGS'] = pd.Series(PAGS)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousCumulativeGoals(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Cumulative Half-time W/L Ratio\n",
    "<a name='section325'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Shots in the match\n",
    "# PHS = Home teams previous match Shots, totaled over season\n",
    "# PAS = Away teams previous match Shots, totaled over season\n",
    "\n",
    "def getPreviousShots(priorData):\n",
    "    teams = {}\n",
    "    PHS = [] \n",
    "    PAS = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HS = priorData.iloc[i]['HS']\n",
    "        AS = priorData.iloc[i]['AS']\n",
    "\n",
    "        try:\n",
    "            pcs_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pcs_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcs_h = 0\n",
    "            pcs_a = 0\n",
    "\n",
    "        PHS.append(pcs_h)\n",
    "        PAS.append(pcs_a)\n",
    "        pcs_h = pcs_h + HS #Home team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pcs_h)\n",
    "        pcs_a = pcs_a + AS #Away team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pcs_a)\n",
    "\n",
    "    priorData.loc[:,'PHS'] = pd.Series(PHS)\n",
    "    priorData.loc[:,'PAS'] = pd.Series(PAS)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousShots(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 Previous shots on target\n",
    "<a name='section326'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Shots on Target\n",
    "# PHSOT = Home teams Previous Shots on Target, totaled over season\n",
    "# PASOT = Away teams Previous Shots on Target, totaled over season\n",
    "\n",
    "def getPreviousShotsOnTarget(priorData):\n",
    "    teams = {}\n",
    "    PHSOT = [] \n",
    "    PASOT = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HST = priorData.iloc[i]['HST']\n",
    "        AST = priorData.iloc[i]['AST']\n",
    "\n",
    "        try:\n",
    "            pcsot_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pcsot_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcsot_h = 0\n",
    "            pcsot_a = 0\n",
    "\n",
    "        PHSOT.append(pcsot_h)\n",
    "        PASOT.append(pcsot_a)\n",
    "        pcsot_h = pcsot_h + HST #Home team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pcsot_h)\n",
    "        pcsot_a = pcsot_a + AST #Away team's previous goals scored before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pcsot_a)\n",
    "\n",
    "    priorData.loc[:,'PHSOT'] = pd.Series(PHSOT)\n",
    "    priorData.loc[:,'PASOT'] = pd.Series(PASOT)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousShotsOnTarget(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7 Computing previous fouls\n",
    "<a name='section327'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Fouls\n",
    "# PHTF = Home teams Previous Fouls, Totaled over season\n",
    "# PATF = Away teams Previous Fouls, Totaled over season\n",
    "\n",
    "def getPreviousTeamFouls(priorData):\n",
    "    teams = {}\n",
    "    PHTF = [] \n",
    "    PATF = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HF = priorData.iloc[i]['HF']\n",
    "        AF = priorData.iloc[i]['AF']\n",
    "\n",
    "        try:\n",
    "            pcf_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pcf_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcf_h = 0\n",
    "            pcf_a = 0\n",
    "\n",
    "        PHTF.append(pcf_h)\n",
    "        PATF.append(pcf_a)\n",
    "        pcf_h = pcf_h + HF #Home team's previous fouls before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pcf_h)\n",
    "        pcf_a = pcf_a + AF #Away team's previous fouls before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pcf_a)\n",
    "\n",
    "    priorData.loc[:,'PHTF'] = pd.Series(PHTF)\n",
    "    priorData.loc[:,'PATF'] = pd.Series(PATF)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousTeamFouls(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.8 Computing previous corners\n",
    "<a name='section328'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Corners\n",
    "# PHTC = Home teams Previous Corners, Totaled over season\n",
    "# PATC = Away teams Previous Corners, Totaled over season\n",
    "\n",
    "def getPreviousTeamCorners(priorData):\n",
    "    teams = {}\n",
    "    PHTC = [] \n",
    "    PATC = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HC = priorData.iloc[i]['HC']\n",
    "        AC = priorData.iloc[i]['AC']\n",
    "\n",
    "        try:\n",
    "            pcc_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pcc_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pcc_h = 0\n",
    "            pcc_a = 0\n",
    "\n",
    "        PHTC.append(pcc_h)\n",
    "        PATC.append(pcc_a)\n",
    "        pcc_h = pcc_h + HC #Home team's previous corners before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pcc_h)\n",
    "        pcc_a = pcc_a + AC #Away team's previous corners before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pcc_a)\n",
    "\n",
    "    priorData.loc[:,'PHTC'] = pd.Series(PHTC)\n",
    "    priorData.loc[:,'PATC'] = pd.Series(PATC)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousTeamCorners(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.9 Computing previous goals before half-time\n",
    "<a name='section329'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Goals before half time\n",
    "# PHTHG = Home teams Previous Goals Before Half Time, Totaled over season\n",
    "# PHTAG = Away teams Previous Goals Before Half Time, Totaled over season\n",
    "\n",
    "def getPreviousHalfTimeGoalsScored(priorData):\n",
    "    teams = {}\n",
    "    PHTHG = [] \n",
    "    PHTAG = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        HTHG = priorData.iloc[i]['HTHG']\n",
    "        HTAG = priorData.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            pchtg_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pchtg_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pchtg_h = 0\n",
    "            pchtg_a = 0\n",
    "\n",
    "        PHTHG.append(pchtg_h)\n",
    "        PHTAG.append(pchtg_a)\n",
    "        pchtg_h = pchtg_h + HTHG #Home team's previous first half goals scored before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pchtg_h)\n",
    "        pchtg_a = pchtg_a + HTAG #Away team's previous first half goals scored before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pchtg_a)\n",
    "\n",
    "    priorData.loc[:,'PHTHG'] = pd.Series(PHTHG)\n",
    "    priorData.loc[:,'PHTAG'] = pd.Series(PHTAG)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousHalfTimeGoalsScored(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.10 Compute previous goals after half-time\n",
    "<a name='section3210'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate Previous Second Half Time Goals in the match\n",
    "# PSHHG = Previous Second Half Time Goals scored by Home team, totaled over season\n",
    "# PSHAG = Previous Second Half Time Goals scored by Away team, totaled over season\n",
    "\n",
    "def getPreviousSecondHalfGoals(priorData):\n",
    "    teams = {}\n",
    "    PSHHG = [] \n",
    "    PSHAG = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "                \n",
    "        FTHG = priorData.iloc[i]['FTHG']\n",
    "        FTAG = priorData.iloc[i]['FTAG']\n",
    "        HTHG = priorData.iloc[i]['HTHG']\n",
    "        HTAG = priorData.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            shg_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            shg_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            shg_h = 0\n",
    "            shg_a = 0\n",
    "\n",
    "        PSHHG.append(shg_h)\n",
    "        PSHAG.append(shg_a)\n",
    "        shg_h = shg_h + (FTHG - HTHG) #Home team's previous second half goals scored before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(shg_h)\n",
    "        shg_a = shg_a + (FTAG - HTAG) #Away team's previous second half goals scored before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(shg_a)\n",
    "\n",
    "    priorData.loc[:,'PSHHG'] = pd.Series(PSHHG)\n",
    "    priorData.loc[:,'PSHAG'] = pd.Series(PSHAG)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousSecondHalfGoals(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.11 Computing previous goals conceded before half-time\n",
    "<a name='section3211'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate previous goals conceded before half-time\n",
    "# PHTHGC = Home Team Previous Goals Conceded Before Half Time, totaled over season\n",
    "# PHTAGC = Away Team Previous Goals Conceded Before Half Time, Totaled over season\n",
    "\n",
    "def getPreviousHalfTimeGoalConceded(priorData):\n",
    "    teams = {}\n",
    "    PHTHGC = [] \n",
    "    PHTAGC = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "                      \n",
    "        HTHG = priorData.iloc[i]['HTHG']\n",
    "        HTAG = priorData.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            phtgc_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            phtgc_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            phtgc_h = 0\n",
    "            phtgc_a = 0\n",
    "\n",
    "        PHTHGC.append(phtgc_h)\n",
    "        PHTAGC.append(phtgc_a)\n",
    "        phtgc_h = phtgc_h + HTAG #Home team's previous half time goals conceded before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(phtgc_h)\n",
    "        phtgc_a = phtgc_a + HTHG #Away team's previous half time goals conceded before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(phtgc_a)\n",
    "\n",
    "    priorData.loc[:,'PHTHGC'] = pd.Series(PHTHGC)\n",
    "    priorData.loc[:,'PHTAGC'] = pd.Series(PHTAGC)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousHalfTimeGoalConceded(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.12 Computing previous goals conceded after half-time\n",
    "<a name='section3212'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate previous goals conceded after half-time\n",
    "# PSHHGC = Previous second half home team goals conceded, totaled over season\n",
    "# PSHAGC = Previous second half away team goals conceded, totaled over season\n",
    "\n",
    "def getPreviousSecondHalfGoalConceded(priorData):\n",
    "    teams = {}\n",
    "    PSHHGC = [] \n",
    "    PSHAGC = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "  \n",
    "        FTHG = priorData.iloc[i]['FTHG']\n",
    "        FTAG = priorData.iloc[i]['FTAG']   \n",
    "        HTHG = priorData.iloc[i]['HTHG']\n",
    "        HTAG = priorData.iloc[i]['HTAG']\n",
    "\n",
    "        try:\n",
    "            pshhgc_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pshhgc_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pshhgc_h = 0\n",
    "            pshhgc_a = 0\n",
    "\n",
    "        PSHHGC.append(pshhgc_h)\n",
    "        PSHAGC.append(pshhgc_a)\n",
    "        pshhgc_h = pshhgc_h + (FTAG - HTAG) #Home team's previous half time goals conceded before this match\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pshhgc_h)\n",
    "        pshhgc_a = pshhgc_a + (FTHG - HTHG) #Away team's previous half time goals conceded before this match\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pshhgc_a)\n",
    "\n",
    "    priorData.loc[:,'PSHHGC'] = pd.Series(PSHHGC)\n",
    "    priorData.loc[:,'PSHAGC'] = pd.Series(PSHAGC)\n",
    "    return priorData\n",
    "\n",
    "#getPreviousSecondHalfGoalConceded(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.13 Matches Played\n",
    "<a name='section3213'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Month  Week  Day          HomeTeam        AwayTeam  HS  AS  HST  AST  \\\n",
      "0         8    33   19          Charlton        Man City  17   8   14    4   \n",
      "1         8    33   19           Chelsea        West Ham  17  12   10    5   \n",
      "2         8    33   19          Coventry   Middlesbrough   6  16    3    9   \n",
      "3         8    33   19             Derby     Southampton   6  13    4    6   \n",
      "4         8    33   19             Leeds         Everton  17  12    8    6   \n",
      "...     ...   ...  ...               ...             ...  ..  ..  ...  ...   \n",
      "7975      5    20   23         Liverpool  Crystal Palace  19   5    5    4   \n",
      "7976      5    20   23          Man City         Everton  21   8   11    3   \n",
      "7977      5    20   23  Sheffield United         Burnley  12  10    3    3   \n",
      "7978      5    20   23          West Ham     Southampton  14  17    7    5   \n",
      "7979      5    20   23            Wolves      Man United  14   9    4    4   \n",
      "\n",
      "      HF  AF  HC  AC  FTHG  FTAG FTR  HTHG  HTAG HTR  SHHG  SHAG  PMPH  PMPA  \n",
      "0     13  12   6   6     4     0   H     2     0   H     2     0     0     0  \n",
      "1     19  14   7   7     4     2   H     1     0   H     3     2     0     0  \n",
      "2     15  21   8   4     1     3   A     1     1   D     0     2     0     0  \n",
      "3     11  13   5   8     2     2   D     1     2   A     1     0     0     0  \n",
      "4     21  20   6   4     2     0   H     2     0   H     0     0     0     0  \n",
      "...   ..  ..  ..  ..   ...   ...  ..   ...   ...  ..   ...   ...   ...   ...  \n",
      "7975  10   8  14   1     2     0   H     1     0   H     1     0   797   341  \n",
      "7976   8  10   7   5     5     0   H     2     0   H     3     0   759   797  \n",
      "7977  11   1   8   9     1     0   H     1     0   H     0     0   113   265  \n",
      "7978   5   9   2   3     3     0   H     2     0   H     1     0   683   531  \n",
      "7979  14   3   6   2     1     2   A     1     2   A     0     0   265   797  \n",
      "\n",
      "[7980 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Computing Priors \n",
    "# Calculate previous goals conceded after half-time\n",
    "# PMPH = Previous total matches played for home team\n",
    "# PMPA = Previous total matches played for away team\n",
    "def getPreviousMatchesPlayed(priorData):\n",
    "    teams = {}\n",
    "    PMPH = [] \n",
    "    PMPA = []   \n",
    "    \n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        if (i % 100000 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = [0]\n",
    "\n",
    "        try:\n",
    "            pmp_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            pmp_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            pmp_h = 0\n",
    "            pmp_a = 0\n",
    "\n",
    "        PMPH.append(pmp_h)\n",
    "        PMPA.append(pmp_a)\n",
    "        pmp_h = pmp_h + 1 #Home team's previous number matches played\n",
    "        teams[priorData.iloc[i].HomeTeam].append(pmp_h)\n",
    "        pmp_a = pmp_a + 1 #Away team's previous number matches played\n",
    "        teams[priorData.iloc[i].AwayTeam].append(pmp_a)\n",
    "\n",
    "    priorData.loc[:,'PMPH'] = pd.Series(PMPH)\n",
    "    priorData.loc[:,'PMPA'] = pd.Series(PMPA)\n",
    "    return priorData\n",
    "\n",
    "print(getPreviousMatchesPlayed(data_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Additional Features\n",
    "<a name='section33'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Distance Travelled for Away Teams\n",
    "<a name='section331'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIS\n",
    "# The positionalData contains the latitude and longitude of teams\n",
    "def getDistance(priorData):\n",
    "  array = []\n",
    "  for x in priorData.iterrows():\n",
    "   \n",
    "    home_lat = (positionalData.loc[positionalData['Team'] == x[1].HomeTeam]).Latitude\n",
    "    home_long = (positionalData.loc[positionalData['Team'] == x[1].HomeTeam]).Longitude\n",
    "    home_location = (np.float32(home_lat), np.float32(home_long))\n",
    "    \n",
    "    away_lat = (positionalData.loc[positionalData['Team'] == x[1].AwayTeam]).Latitude\n",
    "   \n",
    "    away_long = (positionalData.loc[positionalData['Team'] == x[1].AwayTeam]).Longitude\n",
    "    away_location = (np.float32(away_lat), np.float32(away_long))\n",
    "    array.append(np.float32(geodesic(home_location, away_location).km))\n",
    "  \n",
    "  \n",
    "  DIS = pd.Series(array)\n",
    "  priorData.loc[:,'DIS'] = DIS\n",
    "\n",
    "  return priorData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Average shots on goal in the past 3 matches\n",
    "<a name='section332'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average shots on goal for the past 3 matches\n",
    "# HAS, AAS\n",
    "def getPreviousShotOnGoal_3(priorData):\n",
    "    teams = {}\n",
    "    HAS = [] \n",
    "    AAS = []   \n",
    "    \n",
    "    for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = deque([None, None, None]) #[3rd, 2nd, latest priorData]\n",
    "            \n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "\n",
    "            \n",
    "        try:\n",
    "            as_h = np.mean(teams[priorData.iloc[i].HomeTeam])\n",
    "            as_a = np.mean(teams[priorData.iloc[i].AwayTeam])\n",
    "        except:\n",
    "            as_h = None\n",
    "            as_a = None\n",
    "\n",
    "        HAS.append(as_h)\n",
    "        AAS.append(as_a)\n",
    "\n",
    "        teams[priorData.iloc[i].HomeTeam].popleft()\n",
    "        teams[priorData.iloc[i].HomeTeam].append(priorData.iloc[i].HS)\n",
    "\n",
    "        teams[priorData.iloc[i].AwayTeam].popleft()\n",
    "        teams[priorData.iloc[i].AwayTeam].append(priorData.iloc[i].AS)\n",
    "\n",
    "    priorData.loc[:,'HAS'] = pd.Series(HAS)\n",
    "    priorData.loc[:,'AAS'] = pd.Series(AAS)\n",
    "\n",
    "    return priorData\n",
    "\n",
    "#getPreviousShotOnGoal_3(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 WL Performance of past 3 matches\n",
    "<a name='section333'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Home-Away teams in past 3 matches\n",
    "# HM1, AM1, HM2, AM2, HM3, AM3\n",
    "def getPerformanceOfLast3Matches(priorData):\n",
    "    HM1 = []    # performance of the last match of home team\n",
    "    AM1 = []    # performance of the last match of away team\n",
    "\n",
    "    HM2 = []    # performance of the 2nd last match of home team\n",
    "    AM2 = []\n",
    "\n",
    "    HM3 = []    # performance of the 3rd last match of home team\n",
    "    AM3 = []\n",
    "\n",
    "    teams = {}\n",
    "    \n",
    "    for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "               teams[name] = deque([None, None, None])  #[3rd, 2nd, latest priorData]\n",
    "\n",
    "    for i in range(len(priorData)):\n",
    "        \n",
    "\n",
    "        HM3.append(teams[priorData.iloc[i].HomeTeam].popleft())\n",
    "        AM3.append(teams[priorData.iloc[i].AwayTeam].popleft())\n",
    "        HM2.append(teams[priorData.iloc[i].HomeTeam][0])\n",
    "        AM2.append(teams[priorData.iloc[i].AwayTeam][0])\n",
    "        HM1.append(teams[priorData.iloc[i].HomeTeam][1])\n",
    "        AM1.append(teams[priorData.iloc[i].AwayTeam][1])\n",
    "\n",
    "        if priorData.iloc[i].FTR == 'H':\n",
    "            teams[priorData.iloc[i].HomeTeam].append('W')\n",
    "            teams[priorData.iloc[i].AwayTeam].append('L')\n",
    "        elif priorData.iloc[i].FTR == 'A':\n",
    "            teams[priorData.iloc[i].AwayTeam].append('W')\n",
    "            teams[priorData.iloc[i].HomeTeam].append('L')\n",
    "        else:\n",
    "            teams[priorData.iloc[i].AwayTeam].append('D')\n",
    "            teams[priorData.iloc[i].HomeTeam].append('D')\n",
    "\n",
    "    priorData.loc[:,'HM1'] = HM1\n",
    "    priorData.loc[:,'AM1'] = AM1\n",
    "    priorData.loc[:,'HM2'] = HM2\n",
    "    priorData.loc[:,'AM2'] = AM2\n",
    "    priorData.loc[:,'HM3'] = HM3\n",
    "    priorData.loc[:,'AM3'] = AM3\n",
    "\n",
    "    return priorData\n",
    "\n",
    "#print(getPerformanceOfLast3Matches(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Cumulative Full Time Goal Difference\n",
    "<a name='section334'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Priors\n",
    "# Calculate cumulative Full-Time goal different for Home/Away teams prior to every match\n",
    "# HCGD = Home Cumulative Goal Difference\n",
    "# ACGD = Away Cumulative Goal Difference\n",
    "def getCumulativeGoalsDiff(priorData):\n",
    "    teams = {}\n",
    "    HCGD = [] \n",
    "    ACGD = []   \n",
    "\n",
    "    # for each match\n",
    "    for i in range(len(priorData)):\n",
    "        # as the result in 3.2.1 shows that the number of matchese per season is always the same, so here we simply use i%380==0 to check if it is a new season and to initialize the feature.\n",
    "        if (i % 380 == 0):\n",
    "            for name in priorData.groupby('HomeTeam').mean().T.columns:\n",
    "                teams[name] = []\n",
    "\n",
    "        FTHG = priorData.iloc[i]['FTHG']\n",
    "        FTAG = priorData.iloc[i]['FTAG']\n",
    "\n",
    "        try:\n",
    "            cgd_h = teams[priorData.iloc[i].HomeTeam].pop()\n",
    "            cgd_a = teams[priorData.iloc[i].AwayTeam].pop()\n",
    "        except:\n",
    "            cgd_h = 0\n",
    "            cgd_a = 0\n",
    "\n",
    "        HCGD.append(cgd_h)\n",
    "        ACGD.append(cgd_a)\n",
    "        cgd_h = cgd_h + FTHG - FTAG\n",
    "        teams[priorData.iloc[i].HomeTeam].append(cgd_h)\n",
    "        cgd_a = cgd_a + FTAG - FTHG\n",
    "        teams[priorData.iloc[i].AwayTeam].append(cgd_a)\n",
    "\n",
    "    priorData.loc[:,'HCGD'] = pd.Series(HCGD)\n",
    "    priorData.loc[:,'ACGD'] = pd.Series(ACGD)\n",
    "\n",
    "    return priorData\n",
    "\n",
    "#getCumulativeGoalsDiff(data_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 Goalkeeper Stats\n",
    "<a name='section335'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Goalkeeping stats for each team for season starting 2008/2009, ending at present (2021/2020)\n",
    "# def getGK(data, GKData):\n",
    "    \n",
    "#     teams = {}\n",
    "#     HSaveP = [] #Save percentage (Home team)\n",
    "#     HCSP   = [] #Clean sheet percentage\n",
    "#     ASaveP = [] \n",
    "#     ACSP   = [] \n",
    "    \n",
    "#     ## Split data into seasons\n",
    "#     data2008 = data.iloc[:379,:] # 2008-2009 season\n",
    "#     data2009 = data.iloc[380:759,:]\n",
    "#     data2010 = data.iloc[760:1139,:]\n",
    "#     data2011 = data.iloc[1140:1519,:]\n",
    "#     data2012 = data.iloc[1520:1899,:]\n",
    "#     data2013 = data.iloc[1900:2279,:]\n",
    "#     data2014 = data.iloc[2280:2659,:]\n",
    "#     data2015 = data.iloc[2660:3039,:]\n",
    "#     data2016 = data.iloc[3040:3419,:]\n",
    "#     data2017 = data.iloc[3420:3799,:]\n",
    "#     data2018 = data.iloc[3800:4179,:]\n",
    "#     data2019 = data.iloc[4180:4559,:]\n",
    "#     data2020 = data.iloc[4560:4939,:] #2020-2021 season\n",
    "    \n",
    "#     GKData2008 = GKData.iloc[:19,:] # 2008-2009 season\n",
    "#     GKData2009 = GKData.iloc[20:39,:]\n",
    "#     GKData2010 = GKData.iloc[40:59,:]\n",
    "#     GKData2011 = GKData.iloc[60:79,:]\n",
    "#     GKData2012 = GKData.iloc[80:99,:]\n",
    "#     GKData2013 = GKData.iloc[100:119,:]\n",
    "#     GKData2014 = GKData.iloc[120:139,:]\n",
    "#     GKData2015 = GKData.iloc[140:159,:]\n",
    "#     GKData2016 = GKData.iloc[160:179,:]\n",
    "#     GKData2017 = GKData.iloc[180:199,:]\n",
    "#     GKData2018 = GKData.iloc[200:219,:]\n",
    "#     GKData2019 = GKData.iloc[220:239,:]\n",
    "#     GKData2020 = GKData.iloc[240:259,:] #2020-2021 season\n",
    "\n",
    "#     for i in data.groupby('HomeTeam').mean().T.columns: #for 2009 season\n",
    "#             teams[i] = [] #Each team gets their own list\n",
    "\n",
    "#     # the value corresponding to keys is a list containing the match result\n",
    "\n",
    "#     for i in range(len(GKData2008)): # i think swap this for data instead and figure how to go through gkdata 1 at a time\n",
    "#         if GKData2008.iloc[i].Squad == data2008.iloc[i].HomeTeam:\n",
    "#             teams[data2008.iloc[i].HomeTeam].append(GKData2008[[\"SavePercentage\",\"CSPercentage\"]].iloc[i])\n",
    "#     print(teams)\n",
    "#     print(len(GKData2008))\n",
    "        \n",
    "# #         SAVEP = teams[data2009.iloc[i].HomeTeam] #use 2008 season gk data\n",
    "# #         CSP = teams[data2009.iloc[i].HomeTeam]\n",
    "\n",
    "# #         HSaveP.append(SAVEP)\n",
    "# #         HCSP.append(CSP)\n",
    "# #         ASaveP.append(SAVEP)\n",
    "# #         ACSP.append(CSP)\n",
    "        \n",
    "# #         teams[data2009.iloc[i].HomeTeam].append(SAVEP)\n",
    "# #         teams[data2009.iloc[i].HomeTeam].append(CSP)\n",
    "# #         teams[data2009.iloc[i].AwayTeam].append(SAVEP)\n",
    "# #         teams[data2009.iloc[i].AwayTeam].append(CSP)\n",
    "\n",
    "# #         data2009.loc[:,'HSaveP'] = pd.Series(HSaveP)\n",
    "# #         data2009.loc[:,'HCSP'] = pd.Series(HCSP)\n",
    "# #         data2009.loc[:,'ASaveP'] = pd.Series(ASaveP)\n",
    "# #         data2009.loc[:,'ACSP'] = pd.Series(ACSP)\n",
    "#     return data2009\n",
    "\n",
    "# getGK(data, GKData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors - extra features pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pickled_to_df(df,filename,column):\n",
    "    matrix = pd.read_pickle(filename)\n",
    "    matrix[2008] = np.NaN\n",
    "#     print(matrix)\n",
    "    difference = []\n",
    "    for i in range(0,len(data_new)):\n",
    "    #     print(ratings_matrix[\"mean\"].loc[data_new[\"HomeTeam\"].iloc[i]])\n",
    "        if pd.isnull(matrix[year[i]].loc[df[\"HomeTeam\"].iloc[i]]) or pd.isnull(matrix[year[i]].loc[df[\"AwayTeam\"].iloc[i]]):\n",
    "            difference.append(np.nan)\n",
    "            \n",
    "        else:\n",
    "            difference.append(matrix[year[i]].loc[df[\"HomeTeam\"].iloc[i]]-matrix[year[i]].loc[df[\"AwayTeam\"].iloc[i]])\n",
    "\n",
    "#     for i in range(0,len(difference)):\n",
    "#         if difference[i]<-0.1:\n",
    "#             difference[i]='A'\n",
    "#         elif difference[i]>0.1:\n",
    "#             difference[i]='H'\n",
    "#         else:\n",
    "#             difference[i]='D'\n",
    "\n",
    "    df[column]=difference\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have decided not to include these scraped features due to the many missing values without a reliable way to impute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# files = glob.glob(\"./Pickles/*\")\n",
    "# for file in files:\n",
    "#     name = file.split(\"\\\\\")[-1].split(\".\")[0].replace(\"DF\",\"\")\n",
    "#     data_new = add_pickled_to_df(data_new,file,name)\n",
    "\n",
    "# data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Derive Priors\n",
    "<a name='section34'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def DerivePriors(priorData):\n",
    "    #get_previousFTResults(priorData) # dont want Full time results in the test data\n",
    "    get_PreviousHTResults(priorData)\n",
    "    getPreviousCumulativeGoals(priorData)\n",
    "    getPreviousShots(priorData)\n",
    "    getPreviousShotsOnTarget(priorData)\n",
    "    getPreviousTeamFouls(priorData)\n",
    "    getPreviousTeamCorners(priorData)\n",
    "    getPreviousHalfTimeGoalsScored(priorData)\n",
    "    getPreviousSecondHalfGoals(priorData)\n",
    "    getPreviousHalfTimeGoalConceded(priorData)\n",
    "    getPreviousSecondHalfGoalConceded(priorData)\n",
    "    getPreviousMatchesPlayed(priorData)\n",
    "    getDistance(priorData)\n",
    "    getPreviousShotOnGoal_3(priorData)\n",
    "    getPerformanceOfLast3Matches(priorData)\n",
    "    getCumulativeGoalsDiff(priorData)\n",
    "    #getGK(data, GKData)\n",
    "    return priorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>HST</th>\n",
       "      <th>AST</th>\n",
       "      <th>HF</th>\n",
       "      <th>AF</th>\n",
       "      <th>HC</th>\n",
       "      <th>AC</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>SHHG</th>\n",
       "      <th>SHAG</th>\n",
       "      <th>PMPH</th>\n",
       "      <th>PMPA</th>\n",
       "      <th>HHTR</th>\n",
       "      <th>AHTR</th>\n",
       "      <th>PHGS</th>\n",
       "      <th>PAGS</th>\n",
       "      <th>PHS</th>\n",
       "      <th>PAS</th>\n",
       "      <th>PHSOT</th>\n",
       "      <th>PASOT</th>\n",
       "      <th>PHTF</th>\n",
       "      <th>PATF</th>\n",
       "      <th>PHTC</th>\n",
       "      <th>PATC</th>\n",
       "      <th>PHTHG</th>\n",
       "      <th>PHTAG</th>\n",
       "      <th>PSHHG</th>\n",
       "      <th>PSHAG</th>\n",
       "      <th>PHTHGC</th>\n",
       "      <th>PHTAGC</th>\n",
       "      <th>PSHHGC</th>\n",
       "      <th>PSHAGC</th>\n",
       "      <th>DIS</th>\n",
       "      <th>HAS</th>\n",
       "      <th>AAS</th>\n",
       "      <th>HM1</th>\n",
       "      <th>AM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>AM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>AM3</th>\n",
       "      <th>HCGD</th>\n",
       "      <th>ACGD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>Charlton</td>\n",
       "      <td>Everton</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>373</td>\n",
       "      <td>394</td>\n",
       "      <td>217</td>\n",
       "      <td>195</td>\n",
       "      <td>467</td>\n",
       "      <td>558</td>\n",
       "      <td>213</td>\n",
       "      <td>203</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>5929.836426</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>L</td>\n",
       "      <td>D</td>\n",
       "      <td>L</td>\n",
       "      <td>L</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>Derby</td>\n",
       "      <td>Blackburn</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>529</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>114.101906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>None</td>\n",
       "      <td>W</td>\n",
       "      <td>None</td>\n",
       "      <td>L</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>64</td>\n",
       "      <td>40</td>\n",
       "      <td>475</td>\n",
       "      <td>414</td>\n",
       "      <td>228</td>\n",
       "      <td>174</td>\n",
       "      <td>589</td>\n",
       "      <td>509</td>\n",
       "      <td>270</td>\n",
       "      <td>211</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>319.810089</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>L</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>Leicester</td>\n",
       "      <td>Bolton</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>326</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>141.877075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L</td>\n",
       "      <td>None</td>\n",
       "      <td>W</td>\n",
       "      <td>None</td>\n",
       "      <td>L</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>71</td>\n",
       "      <td>45</td>\n",
       "      <td>488</td>\n",
       "      <td>475</td>\n",
       "      <td>249</td>\n",
       "      <td>229</td>\n",
       "      <td>473</td>\n",
       "      <td>473</td>\n",
       "      <td>243</td>\n",
       "      <td>213</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>293.542877</td>\n",
       "      <td>14.666667</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>W</td>\n",
       "      <td>L</td>\n",
       "      <td>D</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>797</td>\n",
       "      <td>341</td>\n",
       "      <td>0.408521</td>\n",
       "      <td>0.216374</td>\n",
       "      <td>1420</td>\n",
       "      <td>378</td>\n",
       "      <td>12013</td>\n",
       "      <td>3784</td>\n",
       "      <td>5550</td>\n",
       "      <td>1298</td>\n",
       "      <td>8540</td>\n",
       "      <td>4028</td>\n",
       "      <td>5153</td>\n",
       "      <td>1732</td>\n",
       "      <td>621</td>\n",
       "      <td>146</td>\n",
       "      <td>799</td>\n",
       "      <td>232</td>\n",
       "      <td>344</td>\n",
       "      <td>206</td>\n",
       "      <td>438</td>\n",
       "      <td>291</td>\n",
       "      <td>298.991241</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>W</td>\n",
       "      <td>L</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>L</td>\n",
       "      <td>24</td>\n",
       "      <td>-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>Man City</td>\n",
       "      <td>Everton</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>759</td>\n",
       "      <td>797</td>\n",
       "      <td>0.418421</td>\n",
       "      <td>0.281955</td>\n",
       "      <td>1374</td>\n",
       "      <td>1059</td>\n",
       "      <td>10879</td>\n",
       "      <td>9446</td>\n",
       "      <td>4969</td>\n",
       "      <td>4393</td>\n",
       "      <td>8536</td>\n",
       "      <td>9646</td>\n",
       "      <td>4890</td>\n",
       "      <td>4411</td>\n",
       "      <td>634</td>\n",
       "      <td>470</td>\n",
       "      <td>740</td>\n",
       "      <td>589</td>\n",
       "      <td>353</td>\n",
       "      <td>427</td>\n",
       "      <td>467</td>\n",
       "      <td>561</td>\n",
       "      <td>51.120102</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>L</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>L</td>\n",
       "      <td>L</td>\n",
       "      <td>D</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>Sheffield United</td>\n",
       "      <td>Burnley</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>265</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.203008</td>\n",
       "      <td>90</td>\n",
       "      <td>266</td>\n",
       "      <td>1069</td>\n",
       "      <td>2725</td>\n",
       "      <td>413</td>\n",
       "      <td>976</td>\n",
       "      <td>1313</td>\n",
       "      <td>2770</td>\n",
       "      <td>555</td>\n",
       "      <td>1140</td>\n",
       "      <td>39</td>\n",
       "      <td>121</td>\n",
       "      <td>51</td>\n",
       "      <td>145</td>\n",
       "      <td>69</td>\n",
       "      <td>188</td>\n",
       "      <td>88</td>\n",
       "      <td>213</td>\n",
       "      <td>67.803841</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>L</td>\n",
       "      <td>L</td>\n",
       "      <td>W</td>\n",
       "      <td>L</td>\n",
       "      <td>L</td>\n",
       "      <td>W</td>\n",
       "      <td>-42</td>\n",
       "      <td>-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>683</td>\n",
       "      <td>531</td>\n",
       "      <td>0.273392</td>\n",
       "      <td>0.259398</td>\n",
       "      <td>845</td>\n",
       "      <td>655</td>\n",
       "      <td>7904</td>\n",
       "      <td>6486</td>\n",
       "      <td>3419</td>\n",
       "      <td>2620</td>\n",
       "      <td>7829</td>\n",
       "      <td>6334</td>\n",
       "      <td>3541</td>\n",
       "      <td>2858</td>\n",
       "      <td>380</td>\n",
       "      <td>299</td>\n",
       "      <td>465</td>\n",
       "      <td>356</td>\n",
       "      <td>447</td>\n",
       "      <td>346</td>\n",
       "      <td>562</td>\n",
       "      <td>387</td>\n",
       "      <td>121.822838</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>W</td>\n",
       "      <td>L</td>\n",
       "      <td>D</td>\n",
       "      <td>W</td>\n",
       "      <td>L</td>\n",
       "      <td>W</td>\n",
       "      <td>12</td>\n",
       "      <td>-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>Wolves</td>\n",
       "      <td>Man United</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>265</td>\n",
       "      <td>797</td>\n",
       "      <td>0.191729</td>\n",
       "      <td>0.461153</td>\n",
       "      <td>289</td>\n",
       "      <td>1503</td>\n",
       "      <td>2938</td>\n",
       "      <td>11570</td>\n",
       "      <td>1279</td>\n",
       "      <td>5668</td>\n",
       "      <td>2954</td>\n",
       "      <td>8971</td>\n",
       "      <td>1423</td>\n",
       "      <td>5039</td>\n",
       "      <td>112</td>\n",
       "      <td>684</td>\n",
       "      <td>177</td>\n",
       "      <td>819</td>\n",
       "      <td>211</td>\n",
       "      <td>322</td>\n",
       "      <td>206</td>\n",
       "      <td>402</td>\n",
       "      <td>97.728035</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>L</td>\n",
       "      <td>D</td>\n",
       "      <td>L</td>\n",
       "      <td>L</td>\n",
       "      <td>W</td>\n",
       "      <td>L</td>\n",
       "      <td>-17</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows  54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Month  Week  Day          HomeTeam        AwayTeam  HS  AS  HST  AST  \\\n",
       "0         8    33   18          Charlton         Everton   8  12    4    9   \n",
       "1         8    33   18             Derby       Blackburn   7  14    3    4   \n",
       "2         8    33   18             Leeds     Southampton  16  11    6    6   \n",
       "3         8    33   18         Leicester          Bolton   6  18    1    8   \n",
       "4         8    33   18         Liverpool        West Ham   9   3    6    3   \n",
       "...     ...   ...  ...               ...             ...  ..  ..  ...  ...   \n",
       "7595      5    20   23         Liverpool  Crystal Palace  19   5    5    4   \n",
       "7596      5    20   23          Man City         Everton  21   8   11    3   \n",
       "7597      5    20   23  Sheffield United         Burnley  12  10    3    3   \n",
       "7598      5    20   23          West Ham     Southampton  14  17    7    5   \n",
       "7599      5    20   23            Wolves      Man United  14   9    4    4   \n",
       "\n",
       "      HF  AF  HC  AC  FTHG  FTAG FTR  HTHG  HTAG HTR  SHHG  SHAG  PMPH  PMPA  \\\n",
       "0     15  17   4   4     1     2   A     0     0   D     1     2    38    38   \n",
       "1     14  15   4  10     2     1   H     1     0   H     1     1    38     0   \n",
       "2     16  24  10   3     2     0   H     0     0   D     2     0    38    38   \n",
       "3     21  17   3   5     0     5   A     0     4   A     0     1    38     0   \n",
       "4     13  12   5   3     2     1   H     1     1   D     1     0    38    38   \n",
       "...   ..  ..  ..  ..   ...   ...  ..   ...   ...  ..   ...   ...   ...   ...   \n",
       "7595  10   8  14   1     2     0   H     1     0   H     1     0   797   341   \n",
       "7596   8  10   7   5     5     0   H     2     0   H     3     0   759   797   \n",
       "7597  11   1   8   9     1     0   H     1     0   H     0     0   113   265   \n",
       "7598   5   9   2   3     3     0   H     2     0   H     1     0   683   531   \n",
       "7599  14   3   6   2     1     2   A     1     2   A     0     0   265   797   \n",
       "\n",
       "          HHTR      AHTR  PHGS  PAGS    PHS    PAS  PHSOT  PASOT  PHTF  PATF  \\\n",
       "0     0.358974  0.179487    50    45    373    394    217    195   467   558   \n",
       "1     0.307692  0.000000    37     0    305      0    141      0   529     0   \n",
       "2     0.307692  0.205128    64    40    475    414    228    174   589   509   \n",
       "3     0.179487  0.000000    39     0    326      0    159      0   426     0   \n",
       "4     0.384615  0.205128    71    45    488    475    249    229   473   473   \n",
       "...        ...       ...   ...   ...    ...    ...    ...    ...   ...   ...   \n",
       "7595  0.408521  0.216374  1420   378  12013   3784   5550   1298  8540  4028   \n",
       "7596  0.418421  0.281955  1374  1059  10879   9446   4969   4393  8536  9646   \n",
       "7597  0.192982  0.203008    90   266   1069   2725    413    976  1313  2770   \n",
       "7598  0.273392  0.259398   845   655   7904   6486   3419   2620  7829  6334   \n",
       "7599  0.191729  0.461153   289  1503   2938  11570   1279   5668  2954  8971   \n",
       "\n",
       "      PHTC  PATC  PHTHG  PHTAG  PSHHG  PSHAG  PHTHGC  PHTAGC  PSHHGC  PSHAGC  \\\n",
       "0      213   203     27     19     23     26      21      26      36      33   \n",
       "1      159     0     20      0     17      0      25       0      34       0   \n",
       "2      270   211     28     16     36     24      26      18      17      30   \n",
       "3      174     0     18      0     21      0      20       0      31       0   \n",
       "4      243   213     28     18     43     27      18      28      21      22   \n",
       "...    ...   ...    ...    ...    ...    ...     ...     ...     ...     ...   \n",
       "7595  5153  1732    621    146    799    232     344     206     438     291   \n",
       "7596  4890  4411    634    470    740    589     353     427     467     561   \n",
       "7597   555  1140     39    121     51    145      69     188      88     213   \n",
       "7598  3541  2858    380    299    465    356     447     346     562     387   \n",
       "7599  1423  5039    112    684    177    819     211     322     206     402   \n",
       "\n",
       "              DIS        HAS        AAS HM1   AM1 HM2   AM2 HM3   AM3  HCGD  \\\n",
       "0     5929.836426  13.000000  15.000000   L     D   L     L   W     W     0   \n",
       "1      114.101906        NaN        NaN   D  None   W  None   L  None     0   \n",
       "2      319.810089  15.333333  12.333333   W     W   W     W   L     L     0   \n",
       "3      141.877075        NaN        NaN   L  None   W  None   L  None     0   \n",
       "4      293.542877  14.666667  13.000000   W     L   D     W   W     L     0   \n",
       "...           ...        ...        ...  ..   ...  ..   ...  ..   ...   ...   \n",
       "7595   298.991241  21.000000  15.333333   W     L   W     W   W     L    24   \n",
       "7596    51.120102  13.000000  15.000000   L     W   W     L   L     D    46   \n",
       "7597    67.803841   9.333333  13.333333   L     L   W     L   L     W   -42   \n",
       "7598   121.822838  15.666667  14.000000   W     L   D     W   L     W    12   \n",
       "7599    97.728035  13.333333  11.666667   L     D   L     L   W     L   -17   \n",
       "\n",
       "      ACGD  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "...    ...  \n",
       "7595   -24  \n",
       "7596     4  \n",
       "7597   -21  \n",
       "7598   -18  \n",
       "7599    28  \n",
       "\n",
       "[7600 rows x 54 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove First Initial Season\n",
    "data_new = DerivePriors(data_new).iloc[380:] #chop off first season \n",
    "y=np.delete(y,slice(0,380),axis=0)\n",
    "data_new.reset_index(drop=True, inplace=True)\n",
    "data_new "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Final Data Preprocessing\n",
    "<a name='section35'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>week_cos</th>\n",
       "      <th>week_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>home_0</th>\n",
       "      <th>home_1</th>\n",
       "      <th>home_2</th>\n",
       "      <th>home_3</th>\n",
       "      <th>home_4</th>\n",
       "      <th>home_5</th>\n",
       "      <th>home_6</th>\n",
       "      <th>home_7</th>\n",
       "      <th>home_8</th>\n",
       "      <th>home_9</th>\n",
       "      <th>home_10</th>\n",
       "      <th>home_11</th>\n",
       "      <th>home_12</th>\n",
       "      <th>home_13</th>\n",
       "      <th>home_14</th>\n",
       "      <th>home_15</th>\n",
       "      <th>home_16</th>\n",
       "      <th>home_17</th>\n",
       "      <th>home_18</th>\n",
       "      <th>home_19</th>\n",
       "      <th>home_20</th>\n",
       "      <th>home_21</th>\n",
       "      <th>home_22</th>\n",
       "      <th>home_23</th>\n",
       "      <th>home_24</th>\n",
       "      <th>home_25</th>\n",
       "      <th>home_26</th>\n",
       "      <th>home_27</th>\n",
       "      <th>home_28</th>\n",
       "      <th>home_29</th>\n",
       "      <th>home_30</th>\n",
       "      <th>home_31</th>\n",
       "      <th>home_32</th>\n",
       "      <th>home_33</th>\n",
       "      <th>home_34</th>\n",
       "      <th>home_35</th>\n",
       "      <th>home_36</th>\n",
       "      <th>home_37</th>\n",
       "      <th>home_38</th>\n",
       "      <th>home_39</th>\n",
       "      <th>home_40</th>\n",
       "      <th>home_41</th>\n",
       "      <th>home_42</th>\n",
       "      <th>away_0</th>\n",
       "      <th>away_1</th>\n",
       "      <th>away_2</th>\n",
       "      <th>away_3</th>\n",
       "      <th>away_4</th>\n",
       "      <th>away_5</th>\n",
       "      <th>away_6</th>\n",
       "      <th>away_7</th>\n",
       "      <th>away_8</th>\n",
       "      <th>away_9</th>\n",
       "      <th>away_10</th>\n",
       "      <th>away_11</th>\n",
       "      <th>away_12</th>\n",
       "      <th>away_13</th>\n",
       "      <th>away_14</th>\n",
       "      <th>away_15</th>\n",
       "      <th>away_16</th>\n",
       "      <th>away_17</th>\n",
       "      <th>away_18</th>\n",
       "      <th>away_19</th>\n",
       "      <th>away_20</th>\n",
       "      <th>away_21</th>\n",
       "      <th>away_22</th>\n",
       "      <th>away_23</th>\n",
       "      <th>away_24</th>\n",
       "      <th>away_25</th>\n",
       "      <th>away_26</th>\n",
       "      <th>away_27</th>\n",
       "      <th>away_28</th>\n",
       "      <th>away_29</th>\n",
       "      <th>away_30</th>\n",
       "      <th>away_31</th>\n",
       "      <th>away_32</th>\n",
       "      <th>away_33</th>\n",
       "      <th>away_34</th>\n",
       "      <th>away_35</th>\n",
       "      <th>away_36</th>\n",
       "      <th>away_37</th>\n",
       "      <th>away_38</th>\n",
       "      <th>away_39</th>\n",
       "      <th>away_40</th>\n",
       "      <th>away_41</th>\n",
       "      <th>away_42</th>\n",
       "      <th>PMPH</th>\n",
       "      <th>PMPA</th>\n",
       "      <th>HHTR</th>\n",
       "      <th>AHTR</th>\n",
       "      <th>PHGS</th>\n",
       "      <th>PAGS</th>\n",
       "      <th>PHS</th>\n",
       "      <th>PAS</th>\n",
       "      <th>PHSOT</th>\n",
       "      <th>PASOT</th>\n",
       "      <th>PHTF</th>\n",
       "      <th>PATF</th>\n",
       "      <th>PHTC</th>\n",
       "      <th>PATC</th>\n",
       "      <th>PHTHG</th>\n",
       "      <th>PHTAG</th>\n",
       "      <th>PSHHG</th>\n",
       "      <th>PSHAG</th>\n",
       "      <th>PHTHGC</th>\n",
       "      <th>PHTAGC</th>\n",
       "      <th>PSHHGC</th>\n",
       "      <th>PSHAGC</th>\n",
       "      <th>DIS</th>\n",
       "      <th>HCGD</th>\n",
       "      <th>ACGD</th>\n",
       "      <th>PHGS_PHSOT</th>\n",
       "      <th>PAGS_PASOT</th>\n",
       "      <th>PHSOT_PHS</th>\n",
       "      <th>PASOT_PAS</th>\n",
       "      <th>PHTF_PATF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.717507</td>\n",
       "      <td>-0.696551</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>373</td>\n",
       "      <td>394</td>\n",
       "      <td>217</td>\n",
       "      <td>195</td>\n",
       "      <td>467</td>\n",
       "      <td>558</td>\n",
       "      <td>213</td>\n",
       "      <td>203</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>5929.836426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230415</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.367797</td>\n",
       "      <td>0.331070</td>\n",
       "      <td>0.836918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.717507</td>\n",
       "      <td>-0.696551</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>529</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>114.101906</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.262411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.717507</td>\n",
       "      <td>-0.696551</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>64</td>\n",
       "      <td>40</td>\n",
       "      <td>475</td>\n",
       "      <td>414</td>\n",
       "      <td>228</td>\n",
       "      <td>174</td>\n",
       "      <td>589</td>\n",
       "      <td>509</td>\n",
       "      <td>270</td>\n",
       "      <td>211</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>319.810089</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.295918</td>\n",
       "      <td>1.157171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.717507</td>\n",
       "      <td>-0.696551</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>326</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>141.877075</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.717507</td>\n",
       "      <td>-0.696551</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>71</td>\n",
       "      <td>45</td>\n",
       "      <td>488</td>\n",
       "      <td>475</td>\n",
       "      <td>249</td>\n",
       "      <td>229</td>\n",
       "      <td>473</td>\n",
       "      <td>473</td>\n",
       "      <td>243</td>\n",
       "      <td>213</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>293.542877</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285141</td>\n",
       "      <td>0.196507</td>\n",
       "      <td>0.337856</td>\n",
       "      <td>0.325284</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.717507</td>\n",
       "      <td>0.696551</td>\n",
       "      <td>-0.050649</td>\n",
       "      <td>-0.998717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>797</td>\n",
       "      <td>341</td>\n",
       "      <td>0.408521</td>\n",
       "      <td>0.216374</td>\n",
       "      <td>1420</td>\n",
       "      <td>378</td>\n",
       "      <td>12013</td>\n",
       "      <td>3784</td>\n",
       "      <td>5550</td>\n",
       "      <td>1298</td>\n",
       "      <td>8540</td>\n",
       "      <td>4028</td>\n",
       "      <td>5153</td>\n",
       "      <td>1732</td>\n",
       "      <td>621</td>\n",
       "      <td>146</td>\n",
       "      <td>799</td>\n",
       "      <td>232</td>\n",
       "      <td>344</td>\n",
       "      <td>206</td>\n",
       "      <td>438</td>\n",
       "      <td>291</td>\n",
       "      <td>298.991241</td>\n",
       "      <td>24</td>\n",
       "      <td>-24</td>\n",
       "      <td>0.255856</td>\n",
       "      <td>0.291217</td>\n",
       "      <td>0.316005</td>\n",
       "      <td>0.255411</td>\n",
       "      <td>2.120159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.717507</td>\n",
       "      <td>0.696551</td>\n",
       "      <td>-0.050649</td>\n",
       "      <td>-0.998717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>759</td>\n",
       "      <td>797</td>\n",
       "      <td>0.418421</td>\n",
       "      <td>0.281955</td>\n",
       "      <td>1374</td>\n",
       "      <td>1059</td>\n",
       "      <td>10879</td>\n",
       "      <td>9446</td>\n",
       "      <td>4969</td>\n",
       "      <td>4393</td>\n",
       "      <td>8536</td>\n",
       "      <td>9646</td>\n",
       "      <td>4890</td>\n",
       "      <td>4411</td>\n",
       "      <td>634</td>\n",
       "      <td>470</td>\n",
       "      <td>740</td>\n",
       "      <td>589</td>\n",
       "      <td>353</td>\n",
       "      <td>427</td>\n",
       "      <td>467</td>\n",
       "      <td>561</td>\n",
       "      <td>51.120102</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>0.276514</td>\n",
       "      <td>0.241065</td>\n",
       "      <td>0.313541</td>\n",
       "      <td>0.317436</td>\n",
       "      <td>0.884926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.717507</td>\n",
       "      <td>0.696551</td>\n",
       "      <td>-0.050649</td>\n",
       "      <td>-0.998717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113</td>\n",
       "      <td>265</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.203008</td>\n",
       "      <td>90</td>\n",
       "      <td>266</td>\n",
       "      <td>1069</td>\n",
       "      <td>2725</td>\n",
       "      <td>413</td>\n",
       "      <td>976</td>\n",
       "      <td>1313</td>\n",
       "      <td>2770</td>\n",
       "      <td>555</td>\n",
       "      <td>1140</td>\n",
       "      <td>39</td>\n",
       "      <td>121</td>\n",
       "      <td>51</td>\n",
       "      <td>145</td>\n",
       "      <td>69</td>\n",
       "      <td>188</td>\n",
       "      <td>88</td>\n",
       "      <td>213</td>\n",
       "      <td>67.803841</td>\n",
       "      <td>-42</td>\n",
       "      <td>-21</td>\n",
       "      <td>0.217918</td>\n",
       "      <td>0.272541</td>\n",
       "      <td>0.278677</td>\n",
       "      <td>0.263713</td>\n",
       "      <td>0.474007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.717507</td>\n",
       "      <td>0.696551</td>\n",
       "      <td>-0.050649</td>\n",
       "      <td>-0.998717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>683</td>\n",
       "      <td>531</td>\n",
       "      <td>0.273392</td>\n",
       "      <td>0.259398</td>\n",
       "      <td>845</td>\n",
       "      <td>655</td>\n",
       "      <td>7904</td>\n",
       "      <td>6486</td>\n",
       "      <td>3419</td>\n",
       "      <td>2620</td>\n",
       "      <td>7829</td>\n",
       "      <td>6334</td>\n",
       "      <td>3541</td>\n",
       "      <td>2858</td>\n",
       "      <td>380</td>\n",
       "      <td>299</td>\n",
       "      <td>465</td>\n",
       "      <td>356</td>\n",
       "      <td>447</td>\n",
       "      <td>346</td>\n",
       "      <td>562</td>\n",
       "      <td>387</td>\n",
       "      <td>121.822838</td>\n",
       "      <td>12</td>\n",
       "      <td>-18</td>\n",
       "      <td>0.247148</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.301952</td>\n",
       "      <td>0.287722</td>\n",
       "      <td>1.236028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.717507</td>\n",
       "      <td>0.696551</td>\n",
       "      <td>-0.050649</td>\n",
       "      <td>-0.998717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>265</td>\n",
       "      <td>797</td>\n",
       "      <td>0.191729</td>\n",
       "      <td>0.461153</td>\n",
       "      <td>289</td>\n",
       "      <td>1503</td>\n",
       "      <td>2938</td>\n",
       "      <td>11570</td>\n",
       "      <td>1279</td>\n",
       "      <td>5668</td>\n",
       "      <td>2954</td>\n",
       "      <td>8971</td>\n",
       "      <td>1423</td>\n",
       "      <td>5039</td>\n",
       "      <td>112</td>\n",
       "      <td>684</td>\n",
       "      <td>177</td>\n",
       "      <td>819</td>\n",
       "      <td>211</td>\n",
       "      <td>322</td>\n",
       "      <td>206</td>\n",
       "      <td>402</td>\n",
       "      <td>97.728035</td>\n",
       "      <td>-17</td>\n",
       "      <td>28</td>\n",
       "      <td>0.225958</td>\n",
       "      <td>0.265173</td>\n",
       "      <td>0.303296</td>\n",
       "      <td>0.328808</td>\n",
       "      <td>0.329283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows  122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      month_cos  month_sin  week_cos  week_sin   day_cos   day_sin  home_0  \\\n",
       "0     -0.500000  -0.866025 -0.717507 -0.696551 -0.874347 -0.485302     0.0   \n",
       "1     -0.500000  -0.866025 -0.717507 -0.696551 -0.874347 -0.485302     0.0   \n",
       "2     -0.500000  -0.866025 -0.717507 -0.696551 -0.874347 -0.485302     0.0   \n",
       "3     -0.500000  -0.866025 -0.717507 -0.696551 -0.874347 -0.485302     0.0   \n",
       "4     -0.500000  -0.866025 -0.717507 -0.696551 -0.874347 -0.485302     0.0   \n",
       "...         ...        ...       ...       ...       ...       ...     ...   \n",
       "7595  -0.866025   0.500000 -0.717507  0.696551 -0.050649 -0.998717     0.0   \n",
       "7596  -0.866025   0.500000 -0.717507  0.696551 -0.050649 -0.998717     0.0   \n",
       "7597  -0.866025   0.500000 -0.717507  0.696551 -0.050649 -0.998717     0.0   \n",
       "7598  -0.866025   0.500000 -0.717507  0.696551 -0.050649 -0.998717     0.0   \n",
       "7599  -0.866025   0.500000 -0.717507  0.696551 -0.050649 -0.998717     0.0   \n",
       "\n",
       "      home_1  home_2  home_3  home_4  home_5  home_6  home_7  home_8  home_9  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "7595     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "7596     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "7597     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "7598     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "7599     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      home_10  home_11  home_12  home_13  home_14  home_15  home_16  home_17  \\\n",
       "0         0.0      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1         0.0      0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2         0.0      0.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "3         0.0      0.0      0.0      0.0      0.0      1.0      0.0      0.0   \n",
       "4         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "7595      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7596      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7597      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7598      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7599      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "      home_18  home_19  home_20  home_21  home_22  home_23  home_24  home_25  \\\n",
       "0         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4         0.0      0.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "7595      0.0      0.0      0.0      0.0      1.0      0.0      0.0      0.0   \n",
       "7596      0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "7597      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7598      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7599      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "      home_26  home_27  home_28  home_29  home_30  home_31  home_32  home_33  \\\n",
       "0         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "7595      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7596      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7597      0.0      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7598      0.0      0.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "7599      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "      home_34  home_35  home_36  home_37  home_38  home_39  home_40  home_41  \\\n",
       "0         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "7595      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7596      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7597      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7598      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7599      0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "\n",
       "      home_42  away_0  away_1  away_2  away_3  away_4  away_5  away_6  away_7  \\\n",
       "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "7595      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "7596      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "7597      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "7598      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "7599      0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      away_8  away_9  away_10  away_11  away_12  away_13  away_14  away_15  \\\n",
       "0        0.0     0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1        0.0     0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2        0.0     0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3        0.0     0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4        0.0     0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...      ...     ...      ...      ...      ...      ...      ...      ...   \n",
       "7595     0.0     0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7596     0.0     0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7597     0.0     0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7598     0.0     0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7599     0.0     0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "      away_16  away_17  away_18  away_19  away_20  away_21  away_22  away_23  \\\n",
       "0         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4         1.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "7595      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7596      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7597      0.0      0.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "7598      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7599      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "      away_24  away_25  away_26  away_27  away_28  away_29  away_30  away_31  \\\n",
       "0         1.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2         0.0      0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "7595      0.0      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7596      0.0      0.0      0.0      0.0      1.0      0.0      0.0      0.0   \n",
       "7597      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7598      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7599      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "      away_32  away_33  away_34  away_35  away_36  away_37  away_38  away_39  \\\n",
       "0         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3         0.0      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "7595      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7596      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7597      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7598      1.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "7599      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "      away_40  away_41  away_42  PMPH  PMPA      HHTR      AHTR  PHGS  PAGS  \\\n",
       "0         0.0      0.0      0.0    38    38  0.358974  0.179487    50    45   \n",
       "1         1.0      0.0      0.0    38     0  0.307692  0.000000    37     0   \n",
       "2         0.0      0.0      0.0    38    38  0.307692  0.205128    64    40   \n",
       "3         0.0      0.0      0.0    38     0  0.179487  0.000000    39     0   \n",
       "4         0.0      0.0      0.0    38    38  0.384615  0.205128    71    45   \n",
       "...       ...      ...      ...   ...   ...       ...       ...   ...   ...   \n",
       "7595      0.0      0.0      0.0   797   341  0.408521  0.216374  1420   378   \n",
       "7596      0.0      0.0      0.0   759   797  0.418421  0.281955  1374  1059   \n",
       "7597      0.0      0.0      0.0   113   265  0.192982  0.203008    90   266   \n",
       "7598      0.0      0.0      0.0   683   531  0.273392  0.259398   845   655   \n",
       "7599      0.0      0.0      0.0   265   797  0.191729  0.461153   289  1503   \n",
       "\n",
       "        PHS    PAS  PHSOT  PASOT  PHTF  PATF  PHTC  PATC  PHTHG  PHTAG  PSHHG  \\\n",
       "0       373    394    217    195   467   558   213   203     27     19     23   \n",
       "1       305      0    141      0   529     0   159     0     20      0     17   \n",
       "2       475    414    228    174   589   509   270   211     28     16     36   \n",
       "3       326      0    159      0   426     0   174     0     18      0     21   \n",
       "4       488    475    249    229   473   473   243   213     28     18     43   \n",
       "...     ...    ...    ...    ...   ...   ...   ...   ...    ...    ...    ...   \n",
       "7595  12013   3784   5550   1298  8540  4028  5153  1732    621    146    799   \n",
       "7596  10879   9446   4969   4393  8536  9646  4890  4411    634    470    740   \n",
       "7597   1069   2725    413    976  1313  2770   555  1140     39    121     51   \n",
       "7598   7904   6486   3419   2620  7829  6334  3541  2858    380    299    465   \n",
       "7599   2938  11570   1279   5668  2954  8971  1423  5039    112    684    177   \n",
       "\n",
       "      PSHAG  PHTHGC  PHTAGC  PSHHGC  PSHAGC          DIS  HCGD  ACGD  \\\n",
       "0        26      21      26      36      33  5929.836426     0     0   \n",
       "1         0      25       0      34       0   114.101906     0     0   \n",
       "2        24      26      18      17      30   319.810089     0     0   \n",
       "3         0      20       0      31       0   141.877075     0     0   \n",
       "4        27      18      28      21      22   293.542877     0     0   \n",
       "...     ...     ...     ...     ...     ...          ...   ...   ...   \n",
       "7595    232     344     206     438     291   298.991241    24   -24   \n",
       "7596    589     353     427     467     561    51.120102    46     4   \n",
       "7597    145      69     188      88     213    67.803841   -42   -21   \n",
       "7598    356     447     346     562     387   121.822838    12   -18   \n",
       "7599    819     211     322     206     402    97.728035   -17    28   \n",
       "\n",
       "      PHGS_PHSOT  PAGS_PASOT  PHSOT_PHS  PASOT_PAS  PHTF_PATF  \n",
       "0       0.230415    0.230769   0.367797   0.331070   0.836918  \n",
       "1       0.262411    0.000000   0.316143   0.000000   0.000000  \n",
       "2       0.280702    0.229885   0.324324   0.295918   1.157171  \n",
       "3       0.245283    0.000000   0.327835   0.000000   0.000000  \n",
       "4       0.285141    0.196507   0.337856   0.325284   1.000000  \n",
       "...          ...         ...        ...        ...        ...  \n",
       "7595    0.255856    0.291217   0.316005   0.255411   2.120159  \n",
       "7596    0.276514    0.241065   0.313541   0.317436   0.884926  \n",
       "7597    0.217918    0.272541   0.278677   0.263713   0.474007  \n",
       "7598    0.247148    0.250000   0.301952   0.287722   1.236028  \n",
       "7599    0.225958    0.265173   0.303296   0.328808   0.329283  \n",
       "\n",
       "[7600 rows x 122 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Final data preprocessing\n",
    "\n",
    "# TODO:\n",
    "# Implement dates using trig - done\n",
    "# Add one hot encoded teams - done\n",
    "# Compute custom features using priors (goals/shots on target, shots on target / total shots, home team fouls / away team fouls)\n",
    "# PHGS/PHSOT, PAGS/PASOT & PHSOT/PHS, PASOT/PAS & PHTF/PATF - done\n",
    "# Implement scaling but don't apply just yet - done\n",
    "# Apply PCA - done\n",
    "\n",
    "dates = data_new.iloc[:, 0:3]\n",
    "month_sin = transformation(dates[\"Month\"])[0]\n",
    "month_cos = transformation(dates[\"Month\"])[1]\n",
    "week_sin = transformation(dates[\"Week\"])[0]\n",
    "week_cos = transformation(dates[\"Week\"])[1]\n",
    "day_sin = transformation(dates[\"Day\"])[0]\n",
    "day_cos = transformation(dates[\"Day\"])[1]\n",
    "\n",
    "teams = pd.DataFrame(home_t.toarray()).add_prefix(\"home_\").join(pd.DataFrame(away_t.toarray()).add_prefix(\"away_\"))\n",
    "\n",
    "# Select only columns that contain priors, can't use in-game stats to predict the future\n",
    "priors = data_new.iloc[:, 21:44]\n",
    "\n",
    "# PHGS_PHSOT is ratio of home goals to home shots on target\n",
    "PHGS_PHSOT = np.where(priors[\"PHSOT\"] != 0, priors[\"PHGS\"]/priors[\"PHSOT\"], 0)\n",
    "# PHGS_PHSOT is ratio of away goals to away shots on target\n",
    "PAGS_PASOT = np.where(priors[\"PASOT\"] != 0, priors[\"PAGS\"]/priors[\"PASOT\"], 0)\n",
    "# PHSOT_PHS is ratio of home shots on target to home shots\n",
    "PHSOT_PHS = np.where(priors[\"PHS\"] != 0, priors[\"PHSOT\"]/ (priors[\"PHS\"] + priors[\"PHSOT\"]), 0)\n",
    "# PASOT_PAS is ratio of away shots on target to away shots\n",
    "PASOT_PAS = np.where(priors[\"PAS\"] != 0, priors[\"PASOT\"]/ (priors[\"PAS\"] + priors[\"PASOT\"]), 0)\n",
    "# PHTF_PATF is ratio of home fouls to away fouls\n",
    "PHTF_PATF = np.where(priors[\"PATF\"] != 0, priors[\"PHTF\"]/priors[\"PATF\"], 0)\n",
    "\n",
    "# Building final dataset\n",
    "X = pd.DataFrame()\n",
    "X[\"month_cos\"] = month_cos\n",
    "X[\"month_sin\"] = month_sin\n",
    "X[\"week_cos\"] = week_cos\n",
    "X[\"week_sin\"] = week_sin\n",
    "X[\"day_cos\"] = day_cos\n",
    "X[\"day_sin\"] = day_sin\n",
    "X = X.join(teams).join(priors)\n",
    "X[\"PHGS_PHSOT\"] = PHGS_PHSOT.tolist()\n",
    "X[\"PAGS_PASOT\"] = PAGS_PASOT.tolist()\n",
    "X[\"PHSOT_PHS\"] = PHSOT_PHS.tolist()\n",
    "X[\"PASOT_PAS\"] = PASOT_PAS.tolist()\n",
    "X[\"PHTF_PATF\"] = PHTF_PATF.tolist()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Split Data\n",
    "<a name='section351'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "y_val = np.array(y_val).reshape(len(y_val))\n",
    "y_test = np.array(y_test).reshape(len(y_test))\n",
    "#Encode y\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_val = encoder.transform(y_val)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "y_train_categorical = keras.utils.to_categorical(y_train)\n",
    "y_val_categorical = keras.utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on all in-game stats: 43.859649122807014%\n",
      "Feature Importances: \n",
      "Feature: AHTR                                Importance: 0.03926229000895186\n",
      "Feature: HHTR                                Importance: 0.037081487976915455\n",
      "Feature: HCGD                                Importance: 0.030885332013977407\n",
      "Feature: ACGD                                Importance: 0.030648082032676653\n",
      "Feature: PHGS_PHSOT                          Importance: 0.03050021001816393\n",
      "Feature: PAGS_PASOT                          Importance: 0.030314137587359704\n",
      "Feature: PASOT_PAS                           Importance: 0.02993995428835218\n",
      "Feature: DIS                                 Importance: 0.029790577850709113\n",
      "Feature: PHSOT_PHS                           Importance: 0.029252524838737175\n",
      "Feature: PHTF_PATF                           Importance: 0.02823730482031175\n",
      "Feature: PATC                                Importance: 0.02571298181449705\n",
      "Feature: PAGS                                Importance: 0.025200952463668474\n",
      "Feature: PSHHG                               Importance: 0.02520009216523465\n",
      "Feature: PAS                                 Importance: 0.0249343705659431\n",
      "Feature: PHTAG                               Importance: 0.02488859223934397\n",
      "Feature: PHSOT                               Importance: 0.02484273402055899\n",
      "Feature: PHGS                                Importance: 0.024656764514126215\n",
      "Feature: PHTHG                               Importance: 0.02464073865490297\n",
      "Feature: PASOT                               Importance: 0.024517564324308967\n",
      "Feature: PSHAG                               Importance: 0.024402213239747332\n",
      "Feature: PHTHGC                              Importance: 0.02426400156934377\n",
      "Feature: PHTC                                Importance: 0.023968400558506686\n",
      "Feature: PHTF                                Importance: 0.02396084712934366\n",
      "Feature: PHS                                 Importance: 0.02383838768322086\n",
      "Feature: day_sin                             Importance: 0.0233752125788771\n",
      "Feature: PATF                                Importance: 0.023346909301354767\n",
      "Feature: PSHHGC                              Importance: 0.02328877735861202\n",
      "Feature: PSHAGC                              Importance: 0.02310887048839448\n",
      "Feature: PHTAGC                              Importance: 0.022728788192781187\n",
      "Feature: PMPH                                Importance: 0.021760134386404943\n",
      "Feature: week_sin                            Importance: 0.021369516233857588\n",
      "Feature: PMPA                                Importance: 0.021165719891693316\n",
      "Feature: day_cos                             Importance: 0.020400498940425598\n",
      "Feature: week_cos                            Importance: 0.02031705981194686\n",
      "Feature: month_cos                           Importance: 0.012431354771574286\n",
      "Feature: month_sin                           Importance: 0.012011451161156395\n",
      "Feature: home_25                             Importance: 0.002565838042979015\n",
      "Feature: home_1                              Importance: 0.0024740665013635896\n",
      "Feature: away_24                             Importance: 0.0024550298616974817\n",
      "Feature: away_16                             Importance: 0.002313983330576726\n",
      "Feature: home_37                             Importance: 0.0022682768297380414\n",
      "Feature: away_23                             Importance: 0.002265052533279815\n",
      "Feature: home_0                              Importance: 0.0022016804437502914\n",
      "Feature: home_5                              Importance: 0.002165063122183621\n",
      "Feature: home_24                             Importance: 0.0021514117001842307\n",
      "Feature: away_17                             Importance: 0.0021438123724138425\n",
      "Feature: away_25                             Importance: 0.0021403127562169286\n",
      "Feature: away_37                             Importance: 0.0021116379587697146\n",
      "Feature: away_1                              Importance: 0.002069268429139897\n",
      "Feature: home_23                             Importance: 0.002068332602953313\n",
      "Feature: home_17                             Importance: 0.0020560539289586253\n",
      "Feature: home_16                             Importance: 0.002037387274345102\n",
      "Feature: away_35                             Importance: 0.0019972482226363873\n",
      "Feature: away_12                             Importance: 0.001984316521378796\n",
      "Feature: away_27                             Importance: 0.0019553460192928995\n",
      "Feature: away_0                              Importance: 0.0019460884824236573\n",
      "Feature: home_12                             Importance: 0.001851982541555871\n",
      "Feature: away_26                             Importance: 0.0018090916749713497\n",
      "Feature: home_35                             Importance: 0.0017592226371369586\n",
      "Feature: home_27                             Importance: 0.0017309037431674113\n",
      "Feature: away_3                              Importance: 0.001717166719468876\n",
      "Feature: away_5                              Importance: 0.0016650851960359413\n",
      "Feature: home_3                              Importance: 0.0016432111846184957\n",
      "Feature: home_40                             Importance: 0.0015895270721983479\n",
      "Feature: home_39                             Importance: 0.0015646419939545427\n",
      "Feature: home_26                             Importance: 0.001499661142848204\n",
      "Feature: away_2                              Importance: 0.0014449622157100424\n",
      "Feature: home_29                             Importance: 0.0013898784600084877\n",
      "Feature: away_40                             Importance: 0.0013756693614274927\n",
      "Feature: away_33                             Importance: 0.0013404032171632145\n",
      "Feature: home_2                              Importance: 0.0013060725826171645\n",
      "Feature: away_42                             Importance: 0.0012741261139699316\n",
      "Feature: home_34                             Importance: 0.0012728119508966043\n",
      "Feature: away_41                             Importance: 0.0012228951907390943\n",
      "Feature: home_41                             Importance: 0.0012076231239416795\n",
      "Feature: away_11                             Importance: 0.0012076007136330238\n",
      "Feature: away_29                             Importance: 0.0012028526311717211\n",
      "Feature: away_39                             Importance: 0.0010901417622867558\n",
      "Feature: home_33                             Importance: 0.0010616632264193082\n",
      "Feature: home_42                             Importance: 0.0010527329741036164\n",
      "Feature: home_11                             Importance: 0.0010470011274326056\n",
      "Feature: away_34                             Importance: 0.0009125845281774246\n",
      "Feature: home_36                             Importance: 0.0008094134456452101\n",
      "Feature: home_28                             Importance: 0.0008042853680524866\n",
      "Feature: home_15                             Importance: 0.0007425139627479723\n",
      "Feature: home_14                             Importance: 0.0007197011523473214\n",
      "Feature: away_22                             Importance: 0.000667183071122406\n",
      "Feature: home_21                             Importance: 0.0006493422530876642\n",
      "Feature: home_19                             Importance: 0.0006253267986552257\n",
      "Feature: away_28                             Importance: 0.0005813939687354673\n",
      "Feature: away_36                             Importance: 0.00057577017625207\n",
      "Feature: away_19                             Importance: 0.0005497007226201714\n",
      "Feature: away_31                             Importance: 0.0005442674680495649\n",
      "Feature: away_15                             Importance: 0.0005329688691271049\n",
      "Feature: home_31                             Importance: 0.0005192814553672939\n",
      "Feature: away_21                             Importance: 0.0005035460524281123\n",
      "Feature: away_30                             Importance: 0.0004918683506923174\n",
      "Feature: home_20                             Importance: 0.00046316212533038304\n",
      "Feature: home_30                             Importance: 0.0004521000386373965\n",
      "Feature: home_22                             Importance: 0.0004347661388165723\n",
      "Feature: away_14                             Importance: 0.0004141376081598915\n",
      "Feature: away_9                              Importance: 0.0003553589869399136\n",
      "Feature: away_32                             Importance: 0.00030719634597445425\n",
      "Feature: home_38                             Importance: 0.000286350733010002\n",
      "Feature: away_10                             Importance: 0.00023755234292732342\n",
      "Feature: home_7                              Importance: 0.0002154081301863304\n",
      "Feature: home_13                             Importance: 0.00020633904842365182\n",
      "Feature: home_10                             Importance: 0.0002053112420088456\n",
      "Feature: away_20                             Importance: 0.0002028012018714969\n",
      "Feature: home_4                              Importance: 0.00018695799848383963\n",
      "Feature: away_4                              Importance: 0.00016907220073475636\n",
      "Feature: away_38                             Importance: 0.0001569973960780036\n",
      "Feature: away_7                              Importance: 0.00014474219126742912\n",
      "Feature: home_32                             Importance: 0.00013924542954369366\n",
      "Feature: away_13                             Importance: 0.00013696534504035142\n",
      "Feature: home_9                              Importance: 0.0001144168657188002\n",
      "Feature: home_6                              Importance: 0.0\n",
      "Feature: home_8                              Importance: 0.0\n",
      "Feature: home_18                             Importance: 0.0\n",
      "Feature: away_6                              Importance: 0.0\n",
      "Feature: away_8                              Importance: 0.0\n",
      "Feature: away_18                             Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "#try without our custom features\n",
    "rf, preds, all_stats_accuracy = rf_model(X_train, X_test, y_train, y_test)\n",
    "print(\"Accuracy on all in-game stats: \" + str(all_stats_accuracy) + \"%\")\n",
    "\n",
    "feature_importances = feat_importances(X_train, rf)\n",
    "print(\"Feature Importances: \")\n",
    "[print('Feature: {:35} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Scale data\n",
    "<a name='section36'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if data needs to be scaled so just gonna leave this here\n",
    "scaler = StandardScaler().fit(X_train.iloc[:, 92:])\n",
    "X_train_scaled = scaler.transform(X_train.iloc[:, 92:])\n",
    "X_test_scaled = scaler.transform(X_test.iloc[:, 92:])\n",
    "X_val_scaled = scaler.transform(X_val.iloc[:, 92:])\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_val = pd.DataFrame(X_val)\n",
    "\n",
    "X_train = np.array(X_train.iloc[:, 0:92])\n",
    "X_test = np.array(X_test.iloc[:, 0:92])\n",
    "X_val = np.array(X_val.iloc[:, 0:92])\n",
    "\n",
    "X_train_scaled = np.hstack((X_train, X_train_scaled))\n",
    "X_test_scaled = np.hstack((X_test, X_test_scaled))\n",
    "X_val_scaled = np.hstack((X_val, X_val_scaled))\n",
    "\n",
    "# PCA\n",
    "# https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "# pca = PCA(0.95)\n",
    "# pca.fit(X_train_scaled)\n",
    "# X_train = pca.transform(X_train_scaled)\n",
    "# X_test = pca.transform(X_test_scaled)\n",
    "\n",
    "# pca = PCA(n_components=50)\n",
    "# X = pca.fit_transform(X_scaled)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# X = TSNE(n_components=3, learning_rate='auto', init='random').fit_transform(X)\n",
    "\n",
    "# from sklearn.manifold import MDS\n",
    "# embedding = MDS(n_components=2)\n",
    "# X = embedding.fit_transform(X) -> took way too long\n",
    "\n",
    "# from sklearn.manifold import Isomap\n",
    "# embedding = Isomap(n_components=2)\n",
    "# X = embedding.fit_transform(X) -> gave terrible results\n",
    "\n",
    "# import umap.umap_ as umap\n",
    "# reducer = umap.UMAP(random_state=42,n_components=15)\n",
    "# X = reducer.fit_transform(X_scaled) -> requires outdated numpy\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=15, kernel='rbf')\n",
    "kpca.fit(X_train_scaled)\n",
    "X_train = kpca.transform(X_train_scaled)\n",
    "X_test = kpca.transform(X_test_scaled)\n",
    "X_val = kpca.transform(X_val_scaled)\n",
    "#tune hyperparams for this -> gamma\n",
    "\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# clf = LinearDiscriminantAnalysis()\n",
    "# clf.fit(X_train_scaled, y_train)\n",
    "# X_train = clf.transform(X_train_scaled)\n",
    "# X_test = clf.transform(X_test_scaled)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# X = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X)\n",
    "\n",
    "#consider combos of these eg pca then lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping data for RNN, LSTM, GRU\n",
    "X_train_test = np.reshape(X_train, (532, 10, X_train.shape[1]))\n",
    "\n",
    "y_train_test = np.reshape(y_train_categorical, (532, 10, 3))\n",
    "\n",
    "y_train_test_cnn = np.array(y_train_categorical).reshape(-1, 1, y_train_categorical.shape[1])\n",
    "\n",
    "X_val_test = np.reshape(X_val, (114, 10, X_val.shape[1]))\n",
    "\n",
    "y_val_categorical_test = np.reshape(y_val_categorical, (114, 10, 3))\n",
    "\n",
    "y_val_test_cnn = np.array(y_val_categorical).reshape(-1, 1, y_val_categorical.shape[1])\n",
    "\n",
    "X_test_test = X_test.reshape(114, 10, X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90xVEDPEp5Js"
   },
   "source": [
    "## 4. Methodology Overview\n",
    "<a name='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOxVov2mqGZ-"
   },
   "source": [
    "## 5. Model Training & Validation\n",
    "<a name='section5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to remove warning to see clearer result\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "#Import for optuna and cv\n",
    "import optuna\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Base Models\n",
    "<a name='section51'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Gaussian Naive Bayes\n",
    "<a name='section511'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Implement into pipeline ##\n",
    "# Some general comments:\n",
    "# Gaussian NB is most suitable for non-categorical classification\n",
    "# Based on diagram above (gaussian distributed density plots) the features we use are gaussian distributed however \n",
    "# the teams are not actually gaussian distributed \n",
    "# And the features we use are not conditionally independent as the statistics arent independent (e.g. shots affect\n",
    "# shots on target etc.)\n",
    "# Therefore we expect that the prediction will not be accurate and naives bayes is not suitable\n",
    "\n",
    "#prove calculations and results later\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_gnb = gnb.fit(X_train, y_train).predict(X_test)\n",
    "accuracy_score(y_test, y_gnb)\n",
    " \n",
    "\n",
    "#Smoothing parameter scaling\n",
    "\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "param_grid_nb = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "}\n",
    "gnb = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, cv=cv_method,verbose=1, scoring='accuracy',n_jobs=-1)\n",
    "y_gnb = gnb.fit(X_train, y_train).predict(X_test)\n",
    "accuracy_score(y_test, y_gnb)\n",
    " \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, y_gnb), \": is the accuracy score gnb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Generic SVM\n",
    "<a name='section512'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using generic SVM to estimate\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "# gammas = np.power(2, np.linspace(-15, 3, 10))\n",
    "# accuracy_validation = np.empty((5, len(gammas)))\n",
    "\n",
    "# for l, gamma in enumerate(gammas):\n",
    "#     svm = SVC(kernel='rbf', gamma=gamma, C=100)\n",
    "#     svm.fit(X_train, y_train)\n",
    "        \n",
    "#     predict_test = svm.predict(X_test)  # test\n",
    "#     print(accuracy_score(y_test, predict_test))\n",
    "\n",
    "# SVM = svm.SVC(kernel=\"linear\")   #(kernel=\"poly\", degree=3, coef0=1, C=5) (kernel=\"linear\")\n",
    "# SVM.fit(training_data,y_train)# predict the labels on validation dataset\n",
    "# predictions_SVM = SVM.predict(testing_data)# Use accuracy_score function to get the accuracy\n",
    "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)\n",
    "\n",
    "# scores = cross_val_score(SVM, X_whole, y_enc, cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=100))\n",
    "# scores.mean()\n",
    "\n",
    "def fineTuneSVM(X_train, y_train):\n",
    "    # define model and parameters\n",
    "    svm = SVC()   \n",
    "    # SVM solves an optimization problem of quadratic order \n",
    "    # link on SVC: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    # The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.\n",
    "    # Therefore, we will stick with basic kernels like linear and rbf which do the job well without sacrificing processing time.\n",
    "    kernel = ['linear', 'rbf'] \n",
    "    # kernel = ['poly', 'rbf', 'sigmoid'] #Advanced kernels \n",
    "    C = [50, 10, 1.0, 0.1, 0.01]\n",
    "    gamma = ['scale']\n",
    "    \n",
    "    # define grid search\n",
    "    grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    grid_search = GridSearchCV(estimator=svm, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "fineTuneSVM(X_train, y_train)\n",
    "\n",
    "# #using optuna broken\n",
    "# def objective_svm(trial, X, y):\n",
    "#     param_grid = {\n",
    "#         \"kernel\": trial.suggest_categorical(\"kernel\", ['linear', 'rbf']),\n",
    "#         \"C\": trial.suggest_categorical(\"C\", [50, 10, 1.0, 0.1, 0.01]),\n",
    "#         \"gamma\": trial.suggest_categorical(\"gamma\", ['scale']),\n",
    "#     }\n",
    "\n",
    "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "#     cv_scores = np.empty(5)\n",
    "#     for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "#         X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "#         y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "#         model = SVC(probability = True, **param_grid)\n",
    "#         model.fit(\n",
    "#             X_train,\n",
    "#             y_train,\n",
    "#         )\n",
    "#         preds = model.predict_proba(X_test)\n",
    "#         cv_scores[idx] = log_loss(y_test, preds)\n",
    "\n",
    "#     return np.mean(cv_scores)\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\", study_name=\"SVM\")\n",
    "# func = lambda trial: objective_svm(trial, X, y)\n",
    "# study.optimize(func, n_trials=4)\n",
    "\n",
    "# print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "# print(f\"\\tBest params:\")\n",
    "# for key, value in study.best_params.items():\n",
    "#     print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "# model = SVC(**study.best_params)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "# print('Testing accuracy ' + str(model.score(X_test, y_test)))\n",
    "\n",
    "#model is slow - directly define model\n",
    "# \tBest value (rmse): 1.02205\n",
    "# \tBest params:\n",
    "# \t\tn_estimators: 400\n",
    "# \t\tlearning_rate: 0.0008\n",
    "# Training accuracy 0.5020676691729323\n",
    "# Testing accuracy 0.5184210526315789\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 Logistic Regression\n",
    "<a name='section513'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multinomial logistic regression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs','saga']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "multi_n = ['multinomial']\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values,multi_class=multi_n)\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=lr, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "### print all the tested results\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Boosting Models\n",
    "<a name='section52'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\alisy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\alisy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\alisy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from xgboost) (1.20.3)\n",
      "[04:16:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alisy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost: 44.912281\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_pipeline = [\n",
    "    ('xgboost' , (Pipeline([('xgboost' ,xgb.XGBClassifier())]))), \n",
    "#     ('nn' , (Pipeline([('nn' , kears_estimator )]))), \n",
    "#     ('...' , (Pipeline([('...' , ... )]))),\n",
    "#     ('...' , (Pipeline([('...' , ... )]))),\n",
    "#     ('...' , (Pipeline([('...' , ... )])))\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for pipe ,model in model_pipeline:\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds) * 100\n",
    "    results.append(accuracy)\n",
    "    output = \"%s: %f\" % (pipe, accuracy)\n",
    "    print(output)\n",
    "\n",
    "\n",
    "param_pipeline = Pipeline([(\"classifier\", xgb.XGBClassifier())])\n",
    "model_param_grid = [\n",
    "                {\"classifier\": [xgb.XGBClassifier()],\n",
    "#                  \"classifier__penalty\": ['l2','l1'],\n",
    "#                  \"classifier__C\": np.logspace(0, 4, 10)\n",
    "                 },\n",
    "#                 {\"classifier\": [kears_estimator],\n",
    "#                  \"tfidf__ngram_range\": [(1,1), (1,2), (2,2), (1,3)],\n",
    "#                 \"tfidf__use_idf\": [True, False],\n",
    "#                 \"kc__epochs\": [10, 100, ],\n",
    "#                 \"kc__dense_nparams\": [32, 256, 512],\n",
    "#                 \"kc__init\": [ 'uniform', 'zeros', 'normal', ], \n",
    "#                 \"kc__batch_size\":[2, 16, 32],\n",
    "#                 \"kc__optimizer\":['RMSprop', 'Adam', 'Adamax', 'sgd'],\n",
    "#                 \"kc__dropout\": [0.5, 0.4, 0.3, 0.2, 0.1, 0]\n",
    "#                  },\n",
    "#                 {\"classifier\": [...],\n",
    "#                  \"classifier__...\": [...],\n",
    "#                  },\n",
    "#                 {\"classifier\": [...],\n",
    "#                  \"classifier__...\": [...],\n",
    "#                  },\n",
    "#                 {\"classifier\": [...],\n",
    "#                  \"classifier__...\": [...],\n",
    "#                  }\n",
    "                ]\n",
    "# gridsearch = GridSearchCV(param_pipeline, model_param_grid, cv=5, verbose=1,n_jobs=-1, return_train_score=True)\n",
    "# best_model = gridsearch.fit(X_train,y_train)\n",
    "# print(best_model.best_estimator_)\n",
    "# print(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 XGBoost\n",
    "<a name='section521'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:03,148]\u001b[0m A new study created in memory with name: XGBoost Classifier\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09496\n",
      "[1]\tvalidation_0-mlogloss:1.09120\n",
      "[2]\tvalidation_0-mlogloss:1.08869\n",
      "[3]\tvalidation_0-mlogloss:1.08607\n",
      "[4]\tvalidation_0-mlogloss:1.08249\n",
      "[5]\tvalidation_0-mlogloss:1.07948\n",
      "[6]\tvalidation_0-mlogloss:1.07780\n",
      "[7]\tvalidation_0-mlogloss:1.07505\n",
      "[8]\tvalidation_0-mlogloss:1.07254\n",
      "[9]\tvalidation_0-mlogloss:1.07077\n",
      "[10]\tvalidation_0-mlogloss:1.06899\n",
      "[11]\tvalidation_0-mlogloss:1.06696\n",
      "[12]\tvalidation_0-mlogloss:1.06444\n",
      "[13]\tvalidation_0-mlogloss:1.06259\n",
      "[14]\tvalidation_0-mlogloss:1.06113\n",
      "[15]\tvalidation_0-mlogloss:1.05916\n",
      "[16]\tvalidation_0-mlogloss:1.05654\n",
      "[17]\tvalidation_0-mlogloss:1.05559\n",
      "[18]\tvalidation_0-mlogloss:1.05359\n",
      "[19]\tvalidation_0-mlogloss:1.05173\n",
      "[20]\tvalidation_0-mlogloss:1.04984\n",
      "[21]\tvalidation_0-mlogloss:1.04848\n",
      "[22]\tvalidation_0-mlogloss:1.04721\n",
      "[23]\tvalidation_0-mlogloss:1.04544\n",
      "[24]\tvalidation_0-mlogloss:1.04390\n",
      "[25]\tvalidation_0-mlogloss:1.04371\n",
      "[26]\tvalidation_0-mlogloss:1.04243\n",
      "[27]\tvalidation_0-mlogloss:1.04099\n",
      "[28]\tvalidation_0-mlogloss:1.04034\n",
      "[29]\tvalidation_0-mlogloss:1.03935\n",
      "[30]\tvalidation_0-mlogloss:1.03821\n",
      "[31]\tvalidation_0-mlogloss:1.03682\n",
      "[32]\tvalidation_0-mlogloss:1.03567\n",
      "[33]\tvalidation_0-mlogloss:1.03489\n",
      "[34]\tvalidation_0-mlogloss:1.03451\n",
      "[35]\tvalidation_0-mlogloss:1.03362\n",
      "[36]\tvalidation_0-mlogloss:1.03235\n",
      "[37]\tvalidation_0-mlogloss:1.03106\n",
      "[38]\tvalidation_0-mlogloss:1.02996\n",
      "[39]\tvalidation_0-mlogloss:1.02860\n",
      "[40]\tvalidation_0-mlogloss:1.02735\n",
      "[41]\tvalidation_0-mlogloss:1.02708\n",
      "[42]\tvalidation_0-mlogloss:1.02696\n",
      "[43]\tvalidation_0-mlogloss:1.02648\n",
      "[44]\tvalidation_0-mlogloss:1.02614\n",
      "[45]\tvalidation_0-mlogloss:1.02550\n",
      "[46]\tvalidation_0-mlogloss:1.02445\n",
      "[47]\tvalidation_0-mlogloss:1.02391\n",
      "[48]\tvalidation_0-mlogloss:1.02327\n",
      "[49]\tvalidation_0-mlogloss:1.02184\n",
      "[50]\tvalidation_0-mlogloss:1.02127\n",
      "[51]\tvalidation_0-mlogloss:1.02077\n",
      "[52]\tvalidation_0-mlogloss:1.02062\n",
      "[53]\tvalidation_0-mlogloss:1.02015\n",
      "[54]\tvalidation_0-mlogloss:1.01955\n",
      "[55]\tvalidation_0-mlogloss:1.01951\n",
      "[56]\tvalidation_0-mlogloss:1.01889\n",
      "[57]\tvalidation_0-mlogloss:1.01841\n",
      "[58]\tvalidation_0-mlogloss:1.01815\n",
      "[59]\tvalidation_0-mlogloss:1.01782\n",
      "[60]\tvalidation_0-mlogloss:1.01774\n",
      "[61]\tvalidation_0-mlogloss:1.01733\n",
      "[62]\tvalidation_0-mlogloss:1.01686\n",
      "[63]\tvalidation_0-mlogloss:1.01659\n",
      "[64]\tvalidation_0-mlogloss:1.01637\n",
      "[65]\tvalidation_0-mlogloss:1.01619\n",
      "[66]\tvalidation_0-mlogloss:1.01572\n",
      "[67]\tvalidation_0-mlogloss:1.01534\n",
      "[68]\tvalidation_0-mlogloss:1.01518\n",
      "[69]\tvalidation_0-mlogloss:1.01438\n",
      "[70]\tvalidation_0-mlogloss:1.01432\n",
      "[71]\tvalidation_0-mlogloss:1.01398\n",
      "[72]\tvalidation_0-mlogloss:1.01381\n",
      "[73]\tvalidation_0-mlogloss:1.01352\n",
      "[74]\tvalidation_0-mlogloss:1.01300\n",
      "[75]\tvalidation_0-mlogloss:1.01306\n",
      "[76]\tvalidation_0-mlogloss:1.01283\n",
      "[77]\tvalidation_0-mlogloss:1.01272\n",
      "[78]\tvalidation_0-mlogloss:1.01260\n",
      "[79]\tvalidation_0-mlogloss:1.01233\n",
      "[80]\tvalidation_0-mlogloss:1.01180\n",
      "[81]\tvalidation_0-mlogloss:1.01172\n",
      "[82]\tvalidation_0-mlogloss:1.01124\n",
      "[83]\tvalidation_0-mlogloss:1.01076\n",
      "[84]\tvalidation_0-mlogloss:1.01050\n",
      "[85]\tvalidation_0-mlogloss:1.01037\n",
      "[86]\tvalidation_0-mlogloss:1.01022\n",
      "[87]\tvalidation_0-mlogloss:1.01015\n",
      "[88]\tvalidation_0-mlogloss:1.00990\n",
      "[89]\tvalidation_0-mlogloss:1.00947\n",
      "[90]\tvalidation_0-mlogloss:1.00935\n",
      "[91]\tvalidation_0-mlogloss:1.00906\n",
      "[92]\tvalidation_0-mlogloss:1.00876\n",
      "[93]\tvalidation_0-mlogloss:1.00858\n",
      "[94]\tvalidation_0-mlogloss:1.00873\n",
      "[95]\tvalidation_0-mlogloss:1.00850\n",
      "[96]\tvalidation_0-mlogloss:1.00833\n",
      "[97]\tvalidation_0-mlogloss:1.00822\n",
      "[98]\tvalidation_0-mlogloss:1.00819\n",
      "[99]\tvalidation_0-mlogloss:1.00813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:04,477]\u001b[0m Trial 0 finished with value: 0.512280701754386 and parameters: {'max_depth': 6, 'learning_rate': 0.03, 'gamma': 1.0, 'reg_lambda': 9, 'scale_pos_weight': 3}. Best is trial 0 with value: 0.512280701754386.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09144\n",
      "[1]\tvalidation_0-mlogloss:1.08293\n",
      "[2]\tvalidation_0-mlogloss:1.07841\n",
      "[3]\tvalidation_0-mlogloss:1.07302\n",
      "[4]\tvalidation_0-mlogloss:1.06608\n",
      "[5]\tvalidation_0-mlogloss:1.06129\n",
      "[6]\tvalidation_0-mlogloss:1.05979\n",
      "[7]\tvalidation_0-mlogloss:1.05633\n",
      "[8]\tvalidation_0-mlogloss:1.05178\n",
      "[9]\tvalidation_0-mlogloss:1.04853\n",
      "[10]\tvalidation_0-mlogloss:1.04688\n",
      "[11]\tvalidation_0-mlogloss:1.04310\n",
      "[12]\tvalidation_0-mlogloss:1.03882\n",
      "[13]\tvalidation_0-mlogloss:1.03642\n",
      "[14]\tvalidation_0-mlogloss:1.03507\n",
      "[15]\tvalidation_0-mlogloss:1.03320\n",
      "[16]\tvalidation_0-mlogloss:1.03118\n",
      "[17]\tvalidation_0-mlogloss:1.03053\n",
      "[18]\tvalidation_0-mlogloss:1.02796\n",
      "[19]\tvalidation_0-mlogloss:1.02662\n",
      "[20]\tvalidation_0-mlogloss:1.02538\n",
      "[21]\tvalidation_0-mlogloss:1.02434\n",
      "[22]\tvalidation_0-mlogloss:1.02321\n",
      "[23]\tvalidation_0-mlogloss:1.02054\n",
      "[24]\tvalidation_0-mlogloss:1.02029\n",
      "[25]\tvalidation_0-mlogloss:1.02012\n",
      "[26]\tvalidation_0-mlogloss:1.01956\n",
      "[27]\tvalidation_0-mlogloss:1.01827\n",
      "[28]\tvalidation_0-mlogloss:1.01801\n",
      "[29]\tvalidation_0-mlogloss:1.01727\n",
      "[30]\tvalidation_0-mlogloss:1.01656\n",
      "[31]\tvalidation_0-mlogloss:1.01608\n",
      "[32]\tvalidation_0-mlogloss:1.01580\n",
      "[33]\tvalidation_0-mlogloss:1.01463\n",
      "[34]\tvalidation_0-mlogloss:1.01438\n",
      "[35]\tvalidation_0-mlogloss:1.01354\n",
      "[36]\tvalidation_0-mlogloss:1.01268\n",
      "[37]\tvalidation_0-mlogloss:1.01246\n",
      "[38]\tvalidation_0-mlogloss:1.01206\n",
      "[39]\tvalidation_0-mlogloss:1.01142\n",
      "[40]\tvalidation_0-mlogloss:1.01063\n",
      "[41]\tvalidation_0-mlogloss:1.01030\n",
      "[42]\tvalidation_0-mlogloss:1.01048\n",
      "[43]\tvalidation_0-mlogloss:1.00998\n",
      "[44]\tvalidation_0-mlogloss:1.00970\n",
      "[45]\tvalidation_0-mlogloss:1.00937\n",
      "[46]\tvalidation_0-mlogloss:1.00892\n",
      "[47]\tvalidation_0-mlogloss:1.00871\n",
      "[48]\tvalidation_0-mlogloss:1.00839\n",
      "[49]\tvalidation_0-mlogloss:1.00754\n",
      "[50]\tvalidation_0-mlogloss:1.00746\n",
      "[51]\tvalidation_0-mlogloss:1.00733\n",
      "[52]\tvalidation_0-mlogloss:1.00727\n",
      "[53]\tvalidation_0-mlogloss:1.00663\n",
      "[54]\tvalidation_0-mlogloss:1.00631\n",
      "[55]\tvalidation_0-mlogloss:1.00615\n",
      "[56]\tvalidation_0-mlogloss:1.00577\n",
      "[57]\tvalidation_0-mlogloss:1.00523\n",
      "[58]\tvalidation_0-mlogloss:1.00513\n",
      "[59]\tvalidation_0-mlogloss:1.00503\n",
      "[60]\tvalidation_0-mlogloss:1.00451\n",
      "[61]\tvalidation_0-mlogloss:1.00404\n",
      "[62]\tvalidation_0-mlogloss:1.00351\n",
      "[63]\tvalidation_0-mlogloss:1.00358\n",
      "[64]\tvalidation_0-mlogloss:1.00363\n",
      "[65]\tvalidation_0-mlogloss:1.00353\n",
      "[66]\tvalidation_0-mlogloss:1.00318\n",
      "[67]\tvalidation_0-mlogloss:1.00312\n",
      "[68]\tvalidation_0-mlogloss:1.00300\n",
      "[69]\tvalidation_0-mlogloss:1.00220\n",
      "[70]\tvalidation_0-mlogloss:1.00221\n",
      "[71]\tvalidation_0-mlogloss:1.00243\n",
      "[72]\tvalidation_0-mlogloss:1.00244\n",
      "[73]\tvalidation_0-mlogloss:1.00224\n",
      "[74]\tvalidation_0-mlogloss:1.00305\n",
      "[75]\tvalidation_0-mlogloss:1.00307\n",
      "[76]\tvalidation_0-mlogloss:1.00312\n",
      "[77]\tvalidation_0-mlogloss:1.00305\n",
      "[78]\tvalidation_0-mlogloss:1.00305\n",
      "[79]\tvalidation_0-mlogloss:1.00284\n",
      "[80]\tvalidation_0-mlogloss:1.00271\n",
      "[81]\tvalidation_0-mlogloss:1.00307\n",
      "[82]\tvalidation_0-mlogloss:1.00292\n",
      "[83]\tvalidation_0-mlogloss:1.00290\n",
      "[84]\tvalidation_0-mlogloss:1.00263\n",
      "[85]\tvalidation_0-mlogloss:1.00209\n",
      "[86]\tvalidation_0-mlogloss:1.00193\n",
      "[87]\tvalidation_0-mlogloss:1.00197\n",
      "[88]\tvalidation_0-mlogloss:1.00205\n",
      "[89]\tvalidation_0-mlogloss:1.00194\n",
      "[90]\tvalidation_0-mlogloss:1.00254\n",
      "[91]\tvalidation_0-mlogloss:1.00268\n",
      "[92]\tvalidation_0-mlogloss:1.00300\n",
      "[93]\tvalidation_0-mlogloss:1.00290\n",
      "[94]\tvalidation_0-mlogloss:1.00337\n",
      "[95]\tvalidation_0-mlogloss:1.00357\n",
      "[96]\tvalidation_0-mlogloss:1.00410\n",
      "[97]\tvalidation_0-mlogloss:1.00454\n",
      "[98]\tvalidation_0-mlogloss:1.00461\n",
      "[99]\tvalidation_0-mlogloss:1.00412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:05,275]\u001b[0m Trial 1 finished with value: 0.5157894736842106 and parameters: {'max_depth': 3, 'learning_rate': 0.06999999999999999, 'gamma': 0.5, 'reg_lambda': 2, 'scale_pos_weight': 1}. Best is trial 1 with value: 0.5157894736842106.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09135\n",
      "[1]\tvalidation_0-mlogloss:1.08341\n",
      "[2]\tvalidation_0-mlogloss:1.07806\n",
      "[3]\tvalidation_0-mlogloss:1.07344\n",
      "[4]\tvalidation_0-mlogloss:1.06728\n",
      "[5]\tvalidation_0-mlogloss:1.06292\n",
      "[6]\tvalidation_0-mlogloss:1.05977\n",
      "[7]\tvalidation_0-mlogloss:1.05523\n",
      "[8]\tvalidation_0-mlogloss:1.05182\n",
      "[9]\tvalidation_0-mlogloss:1.04789\n",
      "[10]\tvalidation_0-mlogloss:1.04566\n",
      "[11]\tvalidation_0-mlogloss:1.04157\n",
      "[12]\tvalidation_0-mlogloss:1.03743\n",
      "[13]\tvalidation_0-mlogloss:1.03532\n",
      "[14]\tvalidation_0-mlogloss:1.03292\n",
      "[15]\tvalidation_0-mlogloss:1.03118\n",
      "[16]\tvalidation_0-mlogloss:1.02910\n",
      "[17]\tvalidation_0-mlogloss:1.02803\n",
      "[18]\tvalidation_0-mlogloss:1.02593\n",
      "[19]\tvalidation_0-mlogloss:1.02512\n",
      "[20]\tvalidation_0-mlogloss:1.02330\n",
      "[21]\tvalidation_0-mlogloss:1.02256\n",
      "[22]\tvalidation_0-mlogloss:1.02107\n",
      "[23]\tvalidation_0-mlogloss:1.01928\n",
      "[24]\tvalidation_0-mlogloss:1.01836\n",
      "[25]\tvalidation_0-mlogloss:1.01878\n",
      "[26]\tvalidation_0-mlogloss:1.01812\n",
      "[27]\tvalidation_0-mlogloss:1.01668\n",
      "[28]\tvalidation_0-mlogloss:1.01632\n",
      "[29]\tvalidation_0-mlogloss:1.01631\n",
      "[30]\tvalidation_0-mlogloss:1.01573\n",
      "[31]\tvalidation_0-mlogloss:1.01452\n",
      "[32]\tvalidation_0-mlogloss:1.01426\n",
      "[33]\tvalidation_0-mlogloss:1.01402\n",
      "[34]\tvalidation_0-mlogloss:1.01384\n",
      "[35]\tvalidation_0-mlogloss:1.01355\n",
      "[36]\tvalidation_0-mlogloss:1.01323\n",
      "[37]\tvalidation_0-mlogloss:1.01262\n",
      "[38]\tvalidation_0-mlogloss:1.01181\n",
      "[39]\tvalidation_0-mlogloss:1.01093\n",
      "[40]\tvalidation_0-mlogloss:1.01012\n",
      "[41]\tvalidation_0-mlogloss:1.01001\n",
      "[42]\tvalidation_0-mlogloss:1.01046\n",
      "[43]\tvalidation_0-mlogloss:1.01000\n",
      "[44]\tvalidation_0-mlogloss:1.01006\n",
      "[45]\tvalidation_0-mlogloss:1.00956\n",
      "[46]\tvalidation_0-mlogloss:1.00962\n",
      "[47]\tvalidation_0-mlogloss:1.00930\n",
      "[48]\tvalidation_0-mlogloss:1.00970\n",
      "[49]\tvalidation_0-mlogloss:1.00895\n",
      "[50]\tvalidation_0-mlogloss:1.00862\n",
      "[51]\tvalidation_0-mlogloss:1.00842\n",
      "[52]\tvalidation_0-mlogloss:1.00826\n",
      "[53]\tvalidation_0-mlogloss:1.00812\n",
      "[54]\tvalidation_0-mlogloss:1.00776\n",
      "[55]\tvalidation_0-mlogloss:1.00731\n",
      "[56]\tvalidation_0-mlogloss:1.00700\n",
      "[57]\tvalidation_0-mlogloss:1.00723\n",
      "[58]\tvalidation_0-mlogloss:1.00760\n",
      "[59]\tvalidation_0-mlogloss:1.00771\n",
      "[60]\tvalidation_0-mlogloss:1.00814\n",
      "[61]\tvalidation_0-mlogloss:1.00781\n",
      "[62]\tvalidation_0-mlogloss:1.00787\n",
      "[63]\tvalidation_0-mlogloss:1.00805\n",
      "[64]\tvalidation_0-mlogloss:1.00781\n",
      "[65]\tvalidation_0-mlogloss:1.00800\n",
      "[66]\tvalidation_0-mlogloss:1.00805\n",
      "[67]\tvalidation_0-mlogloss:1.00813\n",
      "[68]\tvalidation_0-mlogloss:1.00777\n",
      "[69]\tvalidation_0-mlogloss:1.00741\n",
      "[70]\tvalidation_0-mlogloss:1.00756\n",
      "[71]\tvalidation_0-mlogloss:1.00775\n",
      "[72]\tvalidation_0-mlogloss:1.00789\n",
      "[73]\tvalidation_0-mlogloss:1.00800\n",
      "[74]\tvalidation_0-mlogloss:1.00776\n",
      "[75]\tvalidation_0-mlogloss:1.00772\n",
      "[76]\tvalidation_0-mlogloss:1.00758\n",
      "[77]\tvalidation_0-mlogloss:1.00790\n",
      "[78]\tvalidation_0-mlogloss:1.00828\n",
      "[79]\tvalidation_0-mlogloss:1.00820\n",
      "[80]\tvalidation_0-mlogloss:1.00834\n",
      "[81]\tvalidation_0-mlogloss:1.00902\n",
      "[82]\tvalidation_0-mlogloss:1.00893\n",
      "[83]\tvalidation_0-mlogloss:1.00883\n",
      "[84]\tvalidation_0-mlogloss:1.00868\n",
      "[85]\tvalidation_0-mlogloss:1.00820\n",
      "[86]\tvalidation_0-mlogloss:1.00802\n",
      "[87]\tvalidation_0-mlogloss:1.00793\n",
      "[88]\tvalidation_0-mlogloss:1.00798\n",
      "[89]\tvalidation_0-mlogloss:1.00781\n",
      "[90]\tvalidation_0-mlogloss:1.00839\n",
      "[91]\tvalidation_0-mlogloss:1.00832\n",
      "[92]\tvalidation_0-mlogloss:1.00857\n",
      "[93]\tvalidation_0-mlogloss:1.00813\n",
      "[94]\tvalidation_0-mlogloss:1.00864\n",
      "[95]\tvalidation_0-mlogloss:1.00895\n",
      "[96]\tvalidation_0-mlogloss:1.00972\n",
      "[97]\tvalidation_0-mlogloss:1.00949\n",
      "[98]\tvalidation_0-mlogloss:1.00941\n",
      "[99]\tvalidation_0-mlogloss:1.00862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:06,390]\u001b[0m Trial 2 finished with value: 0.5201754385964912 and parameters: {'max_depth': 5, 'learning_rate': 0.060000000000000005, 'gamma': 0.0, 'reg_lambda': 1, 'scale_pos_weight': 3}. Best is trial 2 with value: 0.5201754385964912.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09743\n",
      "[1]\tvalidation_0-mlogloss:1.09609\n",
      "[2]\tvalidation_0-mlogloss:1.09521\n",
      "[3]\tvalidation_0-mlogloss:1.09430\n",
      "[4]\tvalidation_0-mlogloss:1.09300\n",
      "[5]\tvalidation_0-mlogloss:1.09192\n",
      "[6]\tvalidation_0-mlogloss:1.09139\n",
      "[7]\tvalidation_0-mlogloss:1.09032\n",
      "[8]\tvalidation_0-mlogloss:1.08927\n",
      "[9]\tvalidation_0-mlogloss:1.08852\n",
      "[10]\tvalidation_0-mlogloss:1.08780\n",
      "[11]\tvalidation_0-mlogloss:1.08673\n",
      "[12]\tvalidation_0-mlogloss:1.08560\n",
      "[13]\tvalidation_0-mlogloss:1.08474\n",
      "[14]\tvalidation_0-mlogloss:1.08400\n",
      "[15]\tvalidation_0-mlogloss:1.08316\n",
      "[16]\tvalidation_0-mlogloss:1.08209\n",
      "[17]\tvalidation_0-mlogloss:1.08130\n",
      "[18]\tvalidation_0-mlogloss:1.08040\n",
      "[19]\tvalidation_0-mlogloss:1.07923\n",
      "[20]\tvalidation_0-mlogloss:1.07842\n",
      "[21]\tvalidation_0-mlogloss:1.07788\n",
      "[22]\tvalidation_0-mlogloss:1.07716\n",
      "[23]\tvalidation_0-mlogloss:1.07602\n",
      "[24]\tvalidation_0-mlogloss:1.07518\n",
      "[25]\tvalidation_0-mlogloss:1.07478\n",
      "[26]\tvalidation_0-mlogloss:1.07397\n",
      "[27]\tvalidation_0-mlogloss:1.07316\n",
      "[28]\tvalidation_0-mlogloss:1.07267\n",
      "[29]\tvalidation_0-mlogloss:1.07205\n",
      "[30]\tvalidation_0-mlogloss:1.07133\n",
      "[31]\tvalidation_0-mlogloss:1.07031\n",
      "[32]\tvalidation_0-mlogloss:1.06964\n",
      "[33]\tvalidation_0-mlogloss:1.06904\n",
      "[34]\tvalidation_0-mlogloss:1.06863\n",
      "[35]\tvalidation_0-mlogloss:1.06778\n",
      "[36]\tvalidation_0-mlogloss:1.06692\n",
      "[37]\tvalidation_0-mlogloss:1.06604\n",
      "[38]\tvalidation_0-mlogloss:1.06503\n",
      "[39]\tvalidation_0-mlogloss:1.06398\n",
      "[40]\tvalidation_0-mlogloss:1.06303\n",
      "[41]\tvalidation_0-mlogloss:1.06249\n",
      "[42]\tvalidation_0-mlogloss:1.06208\n",
      "[43]\tvalidation_0-mlogloss:1.06146\n",
      "[44]\tvalidation_0-mlogloss:1.06103\n",
      "[45]\tvalidation_0-mlogloss:1.06036\n",
      "[46]\tvalidation_0-mlogloss:1.05948\n",
      "[47]\tvalidation_0-mlogloss:1.05925\n",
      "[48]\tvalidation_0-mlogloss:1.05866\n",
      "[49]\tvalidation_0-mlogloss:1.05775\n",
      "[50]\tvalidation_0-mlogloss:1.05714\n",
      "[51]\tvalidation_0-mlogloss:1.05638\n",
      "[52]\tvalidation_0-mlogloss:1.05598\n",
      "[53]\tvalidation_0-mlogloss:1.05541\n",
      "[54]\tvalidation_0-mlogloss:1.05463\n",
      "[55]\tvalidation_0-mlogloss:1.05426\n",
      "[56]\tvalidation_0-mlogloss:1.05363\n",
      "[57]\tvalidation_0-mlogloss:1.05312\n",
      "[58]\tvalidation_0-mlogloss:1.05294\n",
      "[59]\tvalidation_0-mlogloss:1.05257\n",
      "[60]\tvalidation_0-mlogloss:1.05238\n",
      "[61]\tvalidation_0-mlogloss:1.05164\n",
      "[62]\tvalidation_0-mlogloss:1.05116\n",
      "[63]\tvalidation_0-mlogloss:1.05089\n",
      "[64]\tvalidation_0-mlogloss:1.05042\n",
      "[65]\tvalidation_0-mlogloss:1.05010\n",
      "[66]\tvalidation_0-mlogloss:1.04947\n",
      "[67]\tvalidation_0-mlogloss:1.04894\n",
      "[68]\tvalidation_0-mlogloss:1.04869\n",
      "[69]\tvalidation_0-mlogloss:1.04828\n",
      "[70]\tvalidation_0-mlogloss:1.04783\n",
      "[71]\tvalidation_0-mlogloss:1.04726\n",
      "[72]\tvalidation_0-mlogloss:1.04699\n",
      "[73]\tvalidation_0-mlogloss:1.04633\n",
      "[74]\tvalidation_0-mlogloss:1.04575\n",
      "[75]\tvalidation_0-mlogloss:1.04556\n",
      "[76]\tvalidation_0-mlogloss:1.04516\n",
      "[77]\tvalidation_0-mlogloss:1.04459\n",
      "[78]\tvalidation_0-mlogloss:1.04405\n",
      "[79]\tvalidation_0-mlogloss:1.04355\n",
      "[80]\tvalidation_0-mlogloss:1.04314\n",
      "[81]\tvalidation_0-mlogloss:1.04255\n",
      "[82]\tvalidation_0-mlogloss:1.04186\n",
      "[83]\tvalidation_0-mlogloss:1.04125\n",
      "[84]\tvalidation_0-mlogloss:1.04081\n",
      "[85]\tvalidation_0-mlogloss:1.04026\n",
      "[86]\tvalidation_0-mlogloss:1.03977\n",
      "[87]\tvalidation_0-mlogloss:1.03920\n",
      "[88]\tvalidation_0-mlogloss:1.03902\n",
      "[89]\tvalidation_0-mlogloss:1.03863\n",
      "[90]\tvalidation_0-mlogloss:1.03847\n",
      "[91]\tvalidation_0-mlogloss:1.03801\n",
      "[92]\tvalidation_0-mlogloss:1.03758\n",
      "[93]\tvalidation_0-mlogloss:1.03695\n",
      "[94]\tvalidation_0-mlogloss:1.03682\n",
      "[95]\tvalidation_0-mlogloss:1.03646\n",
      "[96]\tvalidation_0-mlogloss:1.03618\n",
      "[97]\tvalidation_0-mlogloss:1.03579\n",
      "[98]\tvalidation_0-mlogloss:1.03566\n",
      "[99]\tvalidation_0-mlogloss:1.03540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:07,562]\u001b[0m Trial 3 finished with value: 0.5298245614035088 and parameters: {'max_depth': 5, 'learning_rate': 0.01, 'gamma': 1.0, 'reg_lambda': 10, 'scale_pos_weight': 1}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08885\n",
      "[1]\tvalidation_0-mlogloss:1.07772\n",
      "[2]\tvalidation_0-mlogloss:1.07281\n",
      "[3]\tvalidation_0-mlogloss:1.06707\n",
      "[4]\tvalidation_0-mlogloss:1.05913\n",
      "[5]\tvalidation_0-mlogloss:1.05319\n",
      "[6]\tvalidation_0-mlogloss:1.05062\n",
      "[7]\tvalidation_0-mlogloss:1.04621\n",
      "[8]\tvalidation_0-mlogloss:1.04187\n",
      "[9]\tvalidation_0-mlogloss:1.03989\n",
      "[10]\tvalidation_0-mlogloss:1.03838\n",
      "[11]\tvalidation_0-mlogloss:1.03527\n",
      "[12]\tvalidation_0-mlogloss:1.03080\n",
      "[13]\tvalidation_0-mlogloss:1.02890\n",
      "[14]\tvalidation_0-mlogloss:1.02637\n",
      "[15]\tvalidation_0-mlogloss:1.02427\n",
      "[16]\tvalidation_0-mlogloss:1.02263\n",
      "[17]\tvalidation_0-mlogloss:1.02236\n",
      "[18]\tvalidation_0-mlogloss:1.01975\n",
      "[19]\tvalidation_0-mlogloss:1.01844\n",
      "[20]\tvalidation_0-mlogloss:1.01737\n",
      "[21]\tvalidation_0-mlogloss:1.01664\n",
      "[22]\tvalidation_0-mlogloss:1.01589\n",
      "[23]\tvalidation_0-mlogloss:1.01393\n",
      "[24]\tvalidation_0-mlogloss:1.01404\n",
      "[25]\tvalidation_0-mlogloss:1.01388\n",
      "[26]\tvalidation_0-mlogloss:1.01374\n",
      "[27]\tvalidation_0-mlogloss:1.01235\n",
      "[28]\tvalidation_0-mlogloss:1.01226\n",
      "[29]\tvalidation_0-mlogloss:1.01232\n",
      "[30]\tvalidation_0-mlogloss:1.01191\n",
      "[31]\tvalidation_0-mlogloss:1.01110\n",
      "[32]\tvalidation_0-mlogloss:1.01060\n",
      "[33]\tvalidation_0-mlogloss:1.01023\n",
      "[34]\tvalidation_0-mlogloss:1.00964\n",
      "[35]\tvalidation_0-mlogloss:1.00854\n",
      "[36]\tvalidation_0-mlogloss:1.00778\n",
      "[37]\tvalidation_0-mlogloss:1.00798\n",
      "[38]\tvalidation_0-mlogloss:1.00740\n",
      "[39]\tvalidation_0-mlogloss:1.00704\n",
      "[40]\tvalidation_0-mlogloss:1.00637\n",
      "[41]\tvalidation_0-mlogloss:1.00585\n",
      "[42]\tvalidation_0-mlogloss:1.00665\n",
      "[43]\tvalidation_0-mlogloss:1.00634\n",
      "[44]\tvalidation_0-mlogloss:1.00559\n",
      "[45]\tvalidation_0-mlogloss:1.00560\n",
      "[46]\tvalidation_0-mlogloss:1.00531\n",
      "[47]\tvalidation_0-mlogloss:1.00457\n",
      "[48]\tvalidation_0-mlogloss:1.00432\n",
      "[49]\tvalidation_0-mlogloss:1.00379\n",
      "[50]\tvalidation_0-mlogloss:1.00374\n",
      "[51]\tvalidation_0-mlogloss:1.00365\n",
      "[52]\tvalidation_0-mlogloss:1.00363\n",
      "[53]\tvalidation_0-mlogloss:1.00308\n",
      "[54]\tvalidation_0-mlogloss:1.00269\n",
      "[55]\tvalidation_0-mlogloss:1.00234\n",
      "[56]\tvalidation_0-mlogloss:1.00163\n",
      "[57]\tvalidation_0-mlogloss:1.00111\n",
      "[58]\tvalidation_0-mlogloss:1.00165\n",
      "[59]\tvalidation_0-mlogloss:1.00196\n",
      "[60]\tvalidation_0-mlogloss:1.00183\n",
      "[61]\tvalidation_0-mlogloss:1.00169\n",
      "[62]\tvalidation_0-mlogloss:1.00203\n",
      "[63]\tvalidation_0-mlogloss:1.00175\n",
      "[64]\tvalidation_0-mlogloss:1.00165\n",
      "[65]\tvalidation_0-mlogloss:1.00153\n",
      "[66]\tvalidation_0-mlogloss:1.00180\n",
      "[67]\tvalidation_0-mlogloss:1.00158\n",
      "[68]\tvalidation_0-mlogloss:1.00109\n",
      "[69]\tvalidation_0-mlogloss:1.00031\n",
      "[70]\tvalidation_0-mlogloss:1.00007\n",
      "[71]\tvalidation_0-mlogloss:1.00041\n",
      "[72]\tvalidation_0-mlogloss:1.00013\n",
      "[73]\tvalidation_0-mlogloss:1.00015\n",
      "[74]\tvalidation_0-mlogloss:1.00067\n",
      "[75]\tvalidation_0-mlogloss:1.00066\n",
      "[76]\tvalidation_0-mlogloss:1.00065\n",
      "[77]\tvalidation_0-mlogloss:1.00031\n",
      "[78]\tvalidation_0-mlogloss:1.00085\n",
      "[79]\tvalidation_0-mlogloss:1.00095\n",
      "[80]\tvalidation_0-mlogloss:1.00080\n",
      "[81]\tvalidation_0-mlogloss:1.00101\n",
      "[82]\tvalidation_0-mlogloss:1.00112\n",
      "[83]\tvalidation_0-mlogloss:1.00090\n",
      "[84]\tvalidation_0-mlogloss:1.00080\n",
      "[85]\tvalidation_0-mlogloss:1.00080\n",
      "[86]\tvalidation_0-mlogloss:1.00049\n",
      "[87]\tvalidation_0-mlogloss:1.00057\n",
      "[88]\tvalidation_0-mlogloss:1.00030\n",
      "[89]\tvalidation_0-mlogloss:1.00090\n",
      "[90]\tvalidation_0-mlogloss:1.00130\n",
      "[91]\tvalidation_0-mlogloss:1.00106\n",
      "[92]\tvalidation_0-mlogloss:1.00111\n",
      "[93]\tvalidation_0-mlogloss:1.00103\n",
      "[94]\tvalidation_0-mlogloss:1.00140\n",
      "[95]\tvalidation_0-mlogloss:1.00173\n",
      "[96]\tvalidation_0-mlogloss:1.00198\n",
      "[97]\tvalidation_0-mlogloss:1.00182\n",
      "[98]\tvalidation_0-mlogloss:1.00197\n",
      "[99]\tvalidation_0-mlogloss:1.00177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:08,495]\u001b[0m Trial 4 finished with value: 0.512280701754386 and parameters: {'max_depth': 4, 'learning_rate': 0.09, 'gamma': 0.75, 'reg_lambda': 4, 'scale_pos_weight': 3}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09681\n",
      "[1]\tvalidation_0-mlogloss:1.09399\n",
      "[2]\tvalidation_0-mlogloss:1.09201\n",
      "[3]\tvalidation_0-mlogloss:1.09003\n",
      "[4]\tvalidation_0-mlogloss:1.08737\n",
      "[5]\tvalidation_0-mlogloss:1.08517\n",
      "[6]\tvalidation_0-mlogloss:1.08386\n",
      "[7]\tvalidation_0-mlogloss:1.08225\n",
      "[8]\tvalidation_0-mlogloss:1.08021\n",
      "[9]\tvalidation_0-mlogloss:1.07870\n",
      "[10]\tvalidation_0-mlogloss:1.07734\n",
      "[11]\tvalidation_0-mlogloss:1.07543\n",
      "[12]\tvalidation_0-mlogloss:1.07314\n",
      "[13]\tvalidation_0-mlogloss:1.07135\n",
      "[14]\tvalidation_0-mlogloss:1.07021\n",
      "[15]\tvalidation_0-mlogloss:1.06920\n",
      "[16]\tvalidation_0-mlogloss:1.06771\n",
      "[17]\tvalidation_0-mlogloss:1.06639\n",
      "[18]\tvalidation_0-mlogloss:1.06433\n",
      "[19]\tvalidation_0-mlogloss:1.06304\n",
      "[20]\tvalidation_0-mlogloss:1.06199\n",
      "[21]\tvalidation_0-mlogloss:1.06116\n",
      "[22]\tvalidation_0-mlogloss:1.05994\n",
      "[23]\tvalidation_0-mlogloss:1.05791\n",
      "[24]\tvalidation_0-mlogloss:1.05656\n",
      "[25]\tvalidation_0-mlogloss:1.05593\n",
      "[26]\tvalidation_0-mlogloss:1.05456\n",
      "[27]\tvalidation_0-mlogloss:1.05303\n",
      "[28]\tvalidation_0-mlogloss:1.05250\n",
      "[29]\tvalidation_0-mlogloss:1.05180\n",
      "[30]\tvalidation_0-mlogloss:1.05081\n",
      "[31]\tvalidation_0-mlogloss:1.04950\n",
      "[32]\tvalidation_0-mlogloss:1.04882\n",
      "[33]\tvalidation_0-mlogloss:1.04788\n",
      "[34]\tvalidation_0-mlogloss:1.04748\n",
      "[35]\tvalidation_0-mlogloss:1.04662\n",
      "[36]\tvalidation_0-mlogloss:1.04537\n",
      "[37]\tvalidation_0-mlogloss:1.04429\n",
      "[38]\tvalidation_0-mlogloss:1.04315\n",
      "[39]\tvalidation_0-mlogloss:1.04164\n",
      "[40]\tvalidation_0-mlogloss:1.04021\n",
      "[41]\tvalidation_0-mlogloss:1.03978\n",
      "[42]\tvalidation_0-mlogloss:1.03959\n",
      "[43]\tvalidation_0-mlogloss:1.03879\n",
      "[44]\tvalidation_0-mlogloss:1.03832\n",
      "[45]\tvalidation_0-mlogloss:1.03747\n",
      "[46]\tvalidation_0-mlogloss:1.03634\n",
      "[47]\tvalidation_0-mlogloss:1.03602\n",
      "[48]\tvalidation_0-mlogloss:1.03545\n",
      "[49]\tvalidation_0-mlogloss:1.03450\n",
      "[50]\tvalidation_0-mlogloss:1.03380\n",
      "[51]\tvalidation_0-mlogloss:1.03310\n",
      "[52]\tvalidation_0-mlogloss:1.03265\n",
      "[53]\tvalidation_0-mlogloss:1.03166\n",
      "[54]\tvalidation_0-mlogloss:1.03076\n",
      "[55]\tvalidation_0-mlogloss:1.03049\n",
      "[56]\tvalidation_0-mlogloss:1.02985\n",
      "[57]\tvalidation_0-mlogloss:1.02931\n",
      "[58]\tvalidation_0-mlogloss:1.02928\n",
      "[59]\tvalidation_0-mlogloss:1.02887\n",
      "[60]\tvalidation_0-mlogloss:1.02860\n",
      "[61]\tvalidation_0-mlogloss:1.02788\n",
      "[62]\tvalidation_0-mlogloss:1.02711\n",
      "[63]\tvalidation_0-mlogloss:1.02668\n",
      "[64]\tvalidation_0-mlogloss:1.02646\n",
      "[65]\tvalidation_0-mlogloss:1.02638\n",
      "[66]\tvalidation_0-mlogloss:1.02575\n",
      "[67]\tvalidation_0-mlogloss:1.02522\n",
      "[68]\tvalidation_0-mlogloss:1.02503\n",
      "[69]\tvalidation_0-mlogloss:1.02447\n",
      "[70]\tvalidation_0-mlogloss:1.02422\n",
      "[71]\tvalidation_0-mlogloss:1.02361\n",
      "[72]\tvalidation_0-mlogloss:1.02335\n",
      "[73]\tvalidation_0-mlogloss:1.02289\n",
      "[74]\tvalidation_0-mlogloss:1.02253\n",
      "[75]\tvalidation_0-mlogloss:1.02249\n",
      "[76]\tvalidation_0-mlogloss:1.02247\n",
      "[77]\tvalidation_0-mlogloss:1.02207\n",
      "[78]\tvalidation_0-mlogloss:1.02155\n",
      "[79]\tvalidation_0-mlogloss:1.02119\n",
      "[80]\tvalidation_0-mlogloss:1.02076\n",
      "[81]\tvalidation_0-mlogloss:1.02034\n",
      "[82]\tvalidation_0-mlogloss:1.01968\n",
      "[83]\tvalidation_0-mlogloss:1.01905\n",
      "[84]\tvalidation_0-mlogloss:1.01875\n",
      "[85]\tvalidation_0-mlogloss:1.01833\n",
      "[86]\tvalidation_0-mlogloss:1.01807\n",
      "[87]\tvalidation_0-mlogloss:1.01785\n",
      "[88]\tvalidation_0-mlogloss:1.01768\n",
      "[89]\tvalidation_0-mlogloss:1.01722\n",
      "[90]\tvalidation_0-mlogloss:1.01711\n",
      "[91]\tvalidation_0-mlogloss:1.01691\n",
      "[92]\tvalidation_0-mlogloss:1.01650\n",
      "[93]\tvalidation_0-mlogloss:1.01594\n",
      "[94]\tvalidation_0-mlogloss:1.01585\n",
      "[95]\tvalidation_0-mlogloss:1.01561\n",
      "[96]\tvalidation_0-mlogloss:1.01532\n",
      "[97]\tvalidation_0-mlogloss:1.01515\n",
      "[98]\tvalidation_0-mlogloss:1.01516\n",
      "[99]\tvalidation_0-mlogloss:1.01504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:09,433]\u001b[0m Trial 5 finished with value: 0.5245614035087719 and parameters: {'max_depth': 4, 'learning_rate': 0.02, 'gamma': 0.0, 'reg_lambda': 0, 'scale_pos_weight': 3}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08778\n",
      "[1]\tvalidation_0-mlogloss:1.07637\n",
      "[2]\tvalidation_0-mlogloss:1.06918\n",
      "[3]\tvalidation_0-mlogloss:1.06296\n",
      "[4]\tvalidation_0-mlogloss:1.05421\n",
      "[5]\tvalidation_0-mlogloss:1.04876\n",
      "[6]\tvalidation_0-mlogloss:1.04683\n",
      "[7]\tvalidation_0-mlogloss:1.04190\n",
      "[8]\tvalidation_0-mlogloss:1.03787\n",
      "[9]\tvalidation_0-mlogloss:1.03543\n",
      "[10]\tvalidation_0-mlogloss:1.03368\n",
      "[11]\tvalidation_0-mlogloss:1.03085\n",
      "[12]\tvalidation_0-mlogloss:1.02722\n",
      "[13]\tvalidation_0-mlogloss:1.02527\n",
      "[14]\tvalidation_0-mlogloss:1.02381\n",
      "[15]\tvalidation_0-mlogloss:1.02226\n",
      "[16]\tvalidation_0-mlogloss:1.02084\n",
      "[17]\tvalidation_0-mlogloss:1.02039\n",
      "[18]\tvalidation_0-mlogloss:1.01797\n",
      "[19]\tvalidation_0-mlogloss:1.01646\n",
      "[20]\tvalidation_0-mlogloss:1.01504\n",
      "[21]\tvalidation_0-mlogloss:1.01416\n",
      "[22]\tvalidation_0-mlogloss:1.01350\n",
      "[23]\tvalidation_0-mlogloss:1.01171\n",
      "[24]\tvalidation_0-mlogloss:1.01202\n",
      "[25]\tvalidation_0-mlogloss:1.01224\n",
      "[26]\tvalidation_0-mlogloss:1.01209\n",
      "[27]\tvalidation_0-mlogloss:1.01119\n",
      "[28]\tvalidation_0-mlogloss:1.01106\n",
      "[29]\tvalidation_0-mlogloss:1.01075\n",
      "[30]\tvalidation_0-mlogloss:1.01055\n",
      "[31]\tvalidation_0-mlogloss:1.01040\n",
      "[32]\tvalidation_0-mlogloss:1.00986\n",
      "[33]\tvalidation_0-mlogloss:1.00935\n",
      "[34]\tvalidation_0-mlogloss:1.00915\n",
      "[35]\tvalidation_0-mlogloss:1.00817\n",
      "[36]\tvalidation_0-mlogloss:1.00724\n",
      "[37]\tvalidation_0-mlogloss:1.00669\n",
      "[38]\tvalidation_0-mlogloss:1.00713\n",
      "[39]\tvalidation_0-mlogloss:1.00700\n",
      "[40]\tvalidation_0-mlogloss:1.00646\n",
      "[41]\tvalidation_0-mlogloss:1.00611\n",
      "[42]\tvalidation_0-mlogloss:1.00673\n",
      "[43]\tvalidation_0-mlogloss:1.00650\n",
      "[44]\tvalidation_0-mlogloss:1.00628\n",
      "[45]\tvalidation_0-mlogloss:1.00653\n",
      "[46]\tvalidation_0-mlogloss:1.00685\n",
      "[47]\tvalidation_0-mlogloss:1.00615\n",
      "[48]\tvalidation_0-mlogloss:1.00580\n",
      "[49]\tvalidation_0-mlogloss:1.00588\n",
      "[50]\tvalidation_0-mlogloss:1.00586\n",
      "[51]\tvalidation_0-mlogloss:1.00630\n",
      "[52]\tvalidation_0-mlogloss:1.00609\n",
      "[53]\tvalidation_0-mlogloss:1.00505\n",
      "[54]\tvalidation_0-mlogloss:1.00432\n",
      "[55]\tvalidation_0-mlogloss:1.00426\n",
      "[56]\tvalidation_0-mlogloss:1.00428\n",
      "[57]\tvalidation_0-mlogloss:1.00396\n",
      "[58]\tvalidation_0-mlogloss:1.00414\n",
      "[59]\tvalidation_0-mlogloss:1.00368\n",
      "[60]\tvalidation_0-mlogloss:1.00371\n",
      "[61]\tvalidation_0-mlogloss:1.00363\n",
      "[62]\tvalidation_0-mlogloss:1.00392\n",
      "[63]\tvalidation_0-mlogloss:1.00372\n",
      "[64]\tvalidation_0-mlogloss:1.00381\n",
      "[65]\tvalidation_0-mlogloss:1.00395\n",
      "[66]\tvalidation_0-mlogloss:1.00397\n",
      "[67]\tvalidation_0-mlogloss:1.00394\n",
      "[68]\tvalidation_0-mlogloss:1.00341\n",
      "[69]\tvalidation_0-mlogloss:1.00285\n",
      "[70]\tvalidation_0-mlogloss:1.00283\n",
      "[71]\tvalidation_0-mlogloss:1.00303\n",
      "[72]\tvalidation_0-mlogloss:1.00314\n",
      "[73]\tvalidation_0-mlogloss:1.00296\n",
      "[74]\tvalidation_0-mlogloss:1.00322\n",
      "[75]\tvalidation_0-mlogloss:1.00310\n",
      "[76]\tvalidation_0-mlogloss:1.00324\n",
      "[77]\tvalidation_0-mlogloss:1.00283\n",
      "[78]\tvalidation_0-mlogloss:1.00300\n",
      "[79]\tvalidation_0-mlogloss:1.00305\n",
      "[80]\tvalidation_0-mlogloss:1.00302\n",
      "[81]\tvalidation_0-mlogloss:1.00352\n",
      "[82]\tvalidation_0-mlogloss:1.00356\n",
      "[83]\tvalidation_0-mlogloss:1.00345\n",
      "[84]\tvalidation_0-mlogloss:1.00308\n",
      "[85]\tvalidation_0-mlogloss:1.00333\n",
      "[86]\tvalidation_0-mlogloss:1.00314\n",
      "[87]\tvalidation_0-mlogloss:1.00327\n",
      "[88]\tvalidation_0-mlogloss:1.00305\n",
      "[89]\tvalidation_0-mlogloss:1.00320\n",
      "[90]\tvalidation_0-mlogloss:1.00350\n",
      "[91]\tvalidation_0-mlogloss:1.00361\n",
      "[92]\tvalidation_0-mlogloss:1.00352\n",
      "[93]\tvalidation_0-mlogloss:1.00296\n",
      "[94]\tvalidation_0-mlogloss:1.00336\n",
      "[95]\tvalidation_0-mlogloss:1.00367\n",
      "[96]\tvalidation_0-mlogloss:1.00403\n",
      "[97]\tvalidation_0-mlogloss:1.00448\n",
      "[98]\tvalidation_0-mlogloss:1.00501\n",
      "[99]\tvalidation_0-mlogloss:1.00528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:10,413]\u001b[0m Trial 6 finished with value: 0.5175438596491229 and parameters: {'max_depth': 4, 'learning_rate': 0.09999999999999999, 'gamma': 0.75, 'reg_lambda': 10, 'scale_pos_weight': 5}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09358\n",
      "[1]\tvalidation_0-mlogloss:1.08768\n",
      "[2]\tvalidation_0-mlogloss:1.08415\n",
      "[3]\tvalidation_0-mlogloss:1.08019\n",
      "[4]\tvalidation_0-mlogloss:1.07482\n",
      "[5]\tvalidation_0-mlogloss:1.07104\n",
      "[6]\tvalidation_0-mlogloss:1.06960\n",
      "[7]\tvalidation_0-mlogloss:1.06696\n",
      "[8]\tvalidation_0-mlogloss:1.06299\n",
      "[9]\tvalidation_0-mlogloss:1.06034\n",
      "[10]\tvalidation_0-mlogloss:1.05829\n",
      "[11]\tvalidation_0-mlogloss:1.05497\n",
      "[12]\tvalidation_0-mlogloss:1.05161\n",
      "[13]\tvalidation_0-mlogloss:1.04952\n",
      "[14]\tvalidation_0-mlogloss:1.04766\n",
      "[15]\tvalidation_0-mlogloss:1.04569\n",
      "[16]\tvalidation_0-mlogloss:1.04344\n",
      "[17]\tvalidation_0-mlogloss:1.04190\n",
      "[18]\tvalidation_0-mlogloss:1.03965\n",
      "[19]\tvalidation_0-mlogloss:1.03785\n",
      "[20]\tvalidation_0-mlogloss:1.03654\n",
      "[21]\tvalidation_0-mlogloss:1.03549\n",
      "[22]\tvalidation_0-mlogloss:1.03431\n",
      "[23]\tvalidation_0-mlogloss:1.03163\n",
      "[24]\tvalidation_0-mlogloss:1.03109\n",
      "[25]\tvalidation_0-mlogloss:1.03078\n",
      "[26]\tvalidation_0-mlogloss:1.02979\n",
      "[27]\tvalidation_0-mlogloss:1.02829\n",
      "[28]\tvalidation_0-mlogloss:1.02795\n",
      "[29]\tvalidation_0-mlogloss:1.02697\n",
      "[30]\tvalidation_0-mlogloss:1.02591\n",
      "[31]\tvalidation_0-mlogloss:1.02500\n",
      "[32]\tvalidation_0-mlogloss:1.02431\n",
      "[33]\tvalidation_0-mlogloss:1.02339\n",
      "[34]\tvalidation_0-mlogloss:1.02310\n",
      "[35]\tvalidation_0-mlogloss:1.02255\n",
      "[36]\tvalidation_0-mlogloss:1.02136\n",
      "[37]\tvalidation_0-mlogloss:1.02030\n",
      "[38]\tvalidation_0-mlogloss:1.01937\n",
      "[39]\tvalidation_0-mlogloss:1.01831\n",
      "[40]\tvalidation_0-mlogloss:1.01714\n",
      "[41]\tvalidation_0-mlogloss:1.01689\n",
      "[42]\tvalidation_0-mlogloss:1.01701\n",
      "[43]\tvalidation_0-mlogloss:1.01652\n",
      "[44]\tvalidation_0-mlogloss:1.01630\n",
      "[45]\tvalidation_0-mlogloss:1.01568\n",
      "[46]\tvalidation_0-mlogloss:1.01499\n",
      "[47]\tvalidation_0-mlogloss:1.01452\n",
      "[48]\tvalidation_0-mlogloss:1.01403\n",
      "[49]\tvalidation_0-mlogloss:1.01304\n",
      "[50]\tvalidation_0-mlogloss:1.01266\n",
      "[51]\tvalidation_0-mlogloss:1.01216\n",
      "[52]\tvalidation_0-mlogloss:1.01186\n",
      "[53]\tvalidation_0-mlogloss:1.01115\n",
      "[54]\tvalidation_0-mlogloss:1.01017\n",
      "[55]\tvalidation_0-mlogloss:1.00985\n",
      "[56]\tvalidation_0-mlogloss:1.00940\n",
      "[57]\tvalidation_0-mlogloss:1.00906\n",
      "[58]\tvalidation_0-mlogloss:1.00924\n",
      "[59]\tvalidation_0-mlogloss:1.00941\n",
      "[60]\tvalidation_0-mlogloss:1.00905\n",
      "[61]\tvalidation_0-mlogloss:1.00865\n",
      "[62]\tvalidation_0-mlogloss:1.00833\n",
      "[63]\tvalidation_0-mlogloss:1.00805\n",
      "[64]\tvalidation_0-mlogloss:1.00816\n",
      "[65]\tvalidation_0-mlogloss:1.00792\n",
      "[66]\tvalidation_0-mlogloss:1.00763\n",
      "[67]\tvalidation_0-mlogloss:1.00757\n",
      "[68]\tvalidation_0-mlogloss:1.00774\n",
      "[69]\tvalidation_0-mlogloss:1.00698\n",
      "[70]\tvalidation_0-mlogloss:1.00692\n",
      "[71]\tvalidation_0-mlogloss:1.00673\n",
      "[72]\tvalidation_0-mlogloss:1.00664\n",
      "[73]\tvalidation_0-mlogloss:1.00613\n",
      "[74]\tvalidation_0-mlogloss:1.00624\n",
      "[75]\tvalidation_0-mlogloss:1.00624\n",
      "[76]\tvalidation_0-mlogloss:1.00621\n",
      "[77]\tvalidation_0-mlogloss:1.00594\n",
      "[78]\tvalidation_0-mlogloss:1.00581\n",
      "[79]\tvalidation_0-mlogloss:1.00570\n",
      "[80]\tvalidation_0-mlogloss:1.00571\n",
      "[81]\tvalidation_0-mlogloss:1.00560\n",
      "[82]\tvalidation_0-mlogloss:1.00534\n",
      "[83]\tvalidation_0-mlogloss:1.00502\n",
      "[84]\tvalidation_0-mlogloss:1.00473\n",
      "[85]\tvalidation_0-mlogloss:1.00444\n",
      "[86]\tvalidation_0-mlogloss:1.00411\n",
      "[87]\tvalidation_0-mlogloss:1.00381\n",
      "[88]\tvalidation_0-mlogloss:1.00368\n",
      "[89]\tvalidation_0-mlogloss:1.00346\n",
      "[90]\tvalidation_0-mlogloss:1.00340\n",
      "[91]\tvalidation_0-mlogloss:1.00328\n",
      "[92]\tvalidation_0-mlogloss:1.00309\n",
      "[93]\tvalidation_0-mlogloss:1.00290\n",
      "[94]\tvalidation_0-mlogloss:1.00291\n",
      "[95]\tvalidation_0-mlogloss:1.00276\n",
      "[96]\tvalidation_0-mlogloss:1.00272\n",
      "[97]\tvalidation_0-mlogloss:1.00258\n",
      "[98]\tvalidation_0-mlogloss:1.00272\n",
      "[99]\tvalidation_0-mlogloss:1.00256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:11,203]\u001b[0m Trial 7 finished with value: 0.519298245614035 and parameters: {'max_depth': 3, 'learning_rate': 0.05, 'gamma': 0.0, 'reg_lambda': 9, 'scale_pos_weight': 5}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08856\n",
      "[1]\tvalidation_0-mlogloss:1.07717\n",
      "[2]\tvalidation_0-mlogloss:1.07137\n",
      "[3]\tvalidation_0-mlogloss:1.06475\n",
      "[4]\tvalidation_0-mlogloss:1.05644\n",
      "[5]\tvalidation_0-mlogloss:1.05107\n",
      "[6]\tvalidation_0-mlogloss:1.04980\n",
      "[7]\tvalidation_0-mlogloss:1.04625\n",
      "[8]\tvalidation_0-mlogloss:1.04100\n",
      "[9]\tvalidation_0-mlogloss:1.03847\n",
      "[10]\tvalidation_0-mlogloss:1.03676\n",
      "[11]\tvalidation_0-mlogloss:1.03303\n",
      "[12]\tvalidation_0-mlogloss:1.02958\n",
      "[13]\tvalidation_0-mlogloss:1.02742\n",
      "[14]\tvalidation_0-mlogloss:1.02619\n",
      "[15]\tvalidation_0-mlogloss:1.02485\n",
      "[16]\tvalidation_0-mlogloss:1.02374\n",
      "[17]\tvalidation_0-mlogloss:1.02353\n",
      "[18]\tvalidation_0-mlogloss:1.02097\n",
      "[19]\tvalidation_0-mlogloss:1.01917\n",
      "[20]\tvalidation_0-mlogloss:1.01798\n",
      "[21]\tvalidation_0-mlogloss:1.01755\n",
      "[22]\tvalidation_0-mlogloss:1.01649\n",
      "[23]\tvalidation_0-mlogloss:1.01452\n",
      "[24]\tvalidation_0-mlogloss:1.01476\n",
      "[25]\tvalidation_0-mlogloss:1.01404\n",
      "[26]\tvalidation_0-mlogloss:1.01361\n",
      "[27]\tvalidation_0-mlogloss:1.01268\n",
      "[28]\tvalidation_0-mlogloss:1.01244\n",
      "[29]\tvalidation_0-mlogloss:1.01244\n",
      "[30]\tvalidation_0-mlogloss:1.01208\n",
      "[31]\tvalidation_0-mlogloss:1.01186\n",
      "[32]\tvalidation_0-mlogloss:1.01131\n",
      "[33]\tvalidation_0-mlogloss:1.01048\n",
      "[34]\tvalidation_0-mlogloss:1.01038\n",
      "[35]\tvalidation_0-mlogloss:1.01000\n",
      "[36]\tvalidation_0-mlogloss:1.00926\n",
      "[37]\tvalidation_0-mlogloss:1.00945\n",
      "[38]\tvalidation_0-mlogloss:1.00931\n",
      "[39]\tvalidation_0-mlogloss:1.00918\n",
      "[40]\tvalidation_0-mlogloss:1.00861\n",
      "[41]\tvalidation_0-mlogloss:1.00846\n",
      "[42]\tvalidation_0-mlogloss:1.00916\n",
      "[43]\tvalidation_0-mlogloss:1.00906\n",
      "[44]\tvalidation_0-mlogloss:1.00918\n",
      "[45]\tvalidation_0-mlogloss:1.00913\n",
      "[46]\tvalidation_0-mlogloss:1.00922\n",
      "[47]\tvalidation_0-mlogloss:1.00884\n",
      "[48]\tvalidation_0-mlogloss:1.00915\n",
      "[49]\tvalidation_0-mlogloss:1.00876\n",
      "[50]\tvalidation_0-mlogloss:1.00872\n",
      "[51]\tvalidation_0-mlogloss:1.00875\n",
      "[52]\tvalidation_0-mlogloss:1.00866\n",
      "[53]\tvalidation_0-mlogloss:1.00835\n",
      "[54]\tvalidation_0-mlogloss:1.00842\n",
      "[55]\tvalidation_0-mlogloss:1.00829\n",
      "[56]\tvalidation_0-mlogloss:1.00816\n",
      "[57]\tvalidation_0-mlogloss:1.00793\n",
      "[58]\tvalidation_0-mlogloss:1.00825\n",
      "[59]\tvalidation_0-mlogloss:1.00842\n",
      "[60]\tvalidation_0-mlogloss:1.00865\n",
      "[61]\tvalidation_0-mlogloss:1.00881\n",
      "[62]\tvalidation_0-mlogloss:1.00895\n",
      "[63]\tvalidation_0-mlogloss:1.00900\n",
      "[64]\tvalidation_0-mlogloss:1.00859\n",
      "[65]\tvalidation_0-mlogloss:1.00853\n",
      "[66]\tvalidation_0-mlogloss:1.00859\n",
      "[67]\tvalidation_0-mlogloss:1.00845\n",
      "[68]\tvalidation_0-mlogloss:1.00838\n",
      "[69]\tvalidation_0-mlogloss:1.00757\n",
      "[70]\tvalidation_0-mlogloss:1.00796\n",
      "[71]\tvalidation_0-mlogloss:1.00843\n",
      "[72]\tvalidation_0-mlogloss:1.00842\n",
      "[73]\tvalidation_0-mlogloss:1.00806\n",
      "[74]\tvalidation_0-mlogloss:1.00896\n",
      "[75]\tvalidation_0-mlogloss:1.00898\n",
      "[76]\tvalidation_0-mlogloss:1.00920\n",
      "[77]\tvalidation_0-mlogloss:1.00895\n",
      "[78]\tvalidation_0-mlogloss:1.00923\n",
      "[79]\tvalidation_0-mlogloss:1.00952\n",
      "[80]\tvalidation_0-mlogloss:1.00959\n",
      "[81]\tvalidation_0-mlogloss:1.01007\n",
      "[82]\tvalidation_0-mlogloss:1.00988\n",
      "[83]\tvalidation_0-mlogloss:1.01007\n",
      "[84]\tvalidation_0-mlogloss:1.00997\n",
      "[85]\tvalidation_0-mlogloss:1.00996\n",
      "[86]\tvalidation_0-mlogloss:1.01004\n",
      "[87]\tvalidation_0-mlogloss:1.01044\n",
      "[88]\tvalidation_0-mlogloss:1.01013\n",
      "[89]\tvalidation_0-mlogloss:1.01057\n",
      "[90]\tvalidation_0-mlogloss:1.01094\n",
      "[91]\tvalidation_0-mlogloss:1.01121\n",
      "[92]\tvalidation_0-mlogloss:1.01225\n",
      "[93]\tvalidation_0-mlogloss:1.01189\n",
      "[94]\tvalidation_0-mlogloss:1.01256\n",
      "[95]\tvalidation_0-mlogloss:1.01319\n",
      "[96]\tvalidation_0-mlogloss:1.01373\n",
      "[97]\tvalidation_0-mlogloss:1.01338\n",
      "[98]\tvalidation_0-mlogloss:1.01342\n",
      "[99]\tvalidation_0-mlogloss:1.01305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:12,023]\u001b[0m Trial 8 finished with value: 0.5096491228070176 and parameters: {'max_depth': 3, 'learning_rate': 0.09999999999999999, 'gamma': 0.75, 'reg_lambda': 3, 'scale_pos_weight': 5}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09527\n",
      "[1]\tvalidation_0-mlogloss:1.09149\n",
      "[2]\tvalidation_0-mlogloss:1.08881\n",
      "[3]\tvalidation_0-mlogloss:1.08625\n",
      "[4]\tvalidation_0-mlogloss:1.08260\n",
      "[5]\tvalidation_0-mlogloss:1.07999\n",
      "[6]\tvalidation_0-mlogloss:1.07854\n",
      "[7]\tvalidation_0-mlogloss:1.07639\n",
      "[8]\tvalidation_0-mlogloss:1.07393\n",
      "[9]\tvalidation_0-mlogloss:1.07209\n",
      "[10]\tvalidation_0-mlogloss:1.07030\n",
      "[11]\tvalidation_0-mlogloss:1.06820\n",
      "[12]\tvalidation_0-mlogloss:1.06585\n",
      "[13]\tvalidation_0-mlogloss:1.06416\n",
      "[14]\tvalidation_0-mlogloss:1.06271\n",
      "[15]\tvalidation_0-mlogloss:1.06059\n",
      "[16]\tvalidation_0-mlogloss:1.05834\n",
      "[17]\tvalidation_0-mlogloss:1.05694\n",
      "[18]\tvalidation_0-mlogloss:1.05469\n",
      "[19]\tvalidation_0-mlogloss:1.05251\n",
      "[20]\tvalidation_0-mlogloss:1.05105\n",
      "[21]\tvalidation_0-mlogloss:1.05018\n",
      "[22]\tvalidation_0-mlogloss:1.04898\n",
      "[23]\tvalidation_0-mlogloss:1.04719\n",
      "[24]\tvalidation_0-mlogloss:1.04597\n",
      "[25]\tvalidation_0-mlogloss:1.04530\n",
      "[26]\tvalidation_0-mlogloss:1.04406\n",
      "[27]\tvalidation_0-mlogloss:1.04248\n",
      "[28]\tvalidation_0-mlogloss:1.04194\n",
      "[29]\tvalidation_0-mlogloss:1.04088\n",
      "[30]\tvalidation_0-mlogloss:1.03959\n",
      "[31]\tvalidation_0-mlogloss:1.03834\n",
      "[32]\tvalidation_0-mlogloss:1.03716\n",
      "[33]\tvalidation_0-mlogloss:1.03605\n",
      "[34]\tvalidation_0-mlogloss:1.03582\n",
      "[35]\tvalidation_0-mlogloss:1.03499\n",
      "[36]\tvalidation_0-mlogloss:1.03370\n",
      "[37]\tvalidation_0-mlogloss:1.03239\n",
      "[38]\tvalidation_0-mlogloss:1.03104\n",
      "[39]\tvalidation_0-mlogloss:1.02968\n",
      "[40]\tvalidation_0-mlogloss:1.02822\n",
      "[41]\tvalidation_0-mlogloss:1.02778\n",
      "[42]\tvalidation_0-mlogloss:1.02776\n",
      "[43]\tvalidation_0-mlogloss:1.02730\n",
      "[44]\tvalidation_0-mlogloss:1.02714\n",
      "[45]\tvalidation_0-mlogloss:1.02629\n",
      "[46]\tvalidation_0-mlogloss:1.02521\n",
      "[47]\tvalidation_0-mlogloss:1.02480\n",
      "[48]\tvalidation_0-mlogloss:1.02425\n",
      "[49]\tvalidation_0-mlogloss:1.02329\n",
      "[50]\tvalidation_0-mlogloss:1.02276\n",
      "[51]\tvalidation_0-mlogloss:1.02220\n",
      "[52]\tvalidation_0-mlogloss:1.02188\n",
      "[53]\tvalidation_0-mlogloss:1.02104\n",
      "[54]\tvalidation_0-mlogloss:1.02035\n",
      "[55]\tvalidation_0-mlogloss:1.02014\n",
      "[56]\tvalidation_0-mlogloss:1.01934\n",
      "[57]\tvalidation_0-mlogloss:1.01871\n",
      "[58]\tvalidation_0-mlogloss:1.01870\n",
      "[59]\tvalidation_0-mlogloss:1.01828\n",
      "[60]\tvalidation_0-mlogloss:1.01821\n",
      "[61]\tvalidation_0-mlogloss:1.01783\n",
      "[62]\tvalidation_0-mlogloss:1.01725\n",
      "[63]\tvalidation_0-mlogloss:1.01697\n",
      "[64]\tvalidation_0-mlogloss:1.01669\n",
      "[65]\tvalidation_0-mlogloss:1.01651\n",
      "[66]\tvalidation_0-mlogloss:1.01595\n",
      "[67]\tvalidation_0-mlogloss:1.01564\n",
      "[68]\tvalidation_0-mlogloss:1.01544\n",
      "[69]\tvalidation_0-mlogloss:1.01496\n",
      "[70]\tvalidation_0-mlogloss:1.01473\n",
      "[71]\tvalidation_0-mlogloss:1.01425\n",
      "[72]\tvalidation_0-mlogloss:1.01405\n",
      "[73]\tvalidation_0-mlogloss:1.01357\n",
      "[74]\tvalidation_0-mlogloss:1.01325\n",
      "[75]\tvalidation_0-mlogloss:1.01314\n",
      "[76]\tvalidation_0-mlogloss:1.01297\n",
      "[77]\tvalidation_0-mlogloss:1.01280\n",
      "[78]\tvalidation_0-mlogloss:1.01263\n",
      "[79]\tvalidation_0-mlogloss:1.01227\n",
      "[80]\tvalidation_0-mlogloss:1.01170\n",
      "[81]\tvalidation_0-mlogloss:1.01134\n",
      "[82]\tvalidation_0-mlogloss:1.01078\n",
      "[83]\tvalidation_0-mlogloss:1.01029\n",
      "[84]\tvalidation_0-mlogloss:1.01006\n",
      "[85]\tvalidation_0-mlogloss:1.00967\n",
      "[86]\tvalidation_0-mlogloss:1.00923\n",
      "[87]\tvalidation_0-mlogloss:1.00895\n",
      "[88]\tvalidation_0-mlogloss:1.00887\n",
      "[89]\tvalidation_0-mlogloss:1.00855\n",
      "[90]\tvalidation_0-mlogloss:1.00838\n",
      "[91]\tvalidation_0-mlogloss:1.00824\n",
      "[92]\tvalidation_0-mlogloss:1.00782\n",
      "[93]\tvalidation_0-mlogloss:1.00754\n",
      "[94]\tvalidation_0-mlogloss:1.00743\n",
      "[95]\tvalidation_0-mlogloss:1.00723\n",
      "[96]\tvalidation_0-mlogloss:1.00719\n",
      "[97]\tvalidation_0-mlogloss:1.00704\n",
      "[98]\tvalidation_0-mlogloss:1.00710\n",
      "[99]\tvalidation_0-mlogloss:1.00690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:12,978]\u001b[0m Trial 9 finished with value: 0.5263157894736842 and parameters: {'max_depth': 4, 'learning_rate': 0.03, 'gamma': 0.0, 'reg_lambda': 10, 'scale_pos_weight': 1}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09752\n",
      "[1]\tvalidation_0-mlogloss:1.09600\n",
      "[2]\tvalidation_0-mlogloss:1.09510\n",
      "[3]\tvalidation_0-mlogloss:1.09411\n",
      "[4]\tvalidation_0-mlogloss:1.09273\n",
      "[5]\tvalidation_0-mlogloss:1.09155\n",
      "[6]\tvalidation_0-mlogloss:1.09085\n",
      "[7]\tvalidation_0-mlogloss:1.08964\n",
      "[8]\tvalidation_0-mlogloss:1.08863\n",
      "[9]\tvalidation_0-mlogloss:1.08771\n",
      "[10]\tvalidation_0-mlogloss:1.08691\n",
      "[11]\tvalidation_0-mlogloss:1.08586\n",
      "[12]\tvalidation_0-mlogloss:1.08473\n",
      "[13]\tvalidation_0-mlogloss:1.08382\n",
      "[14]\tvalidation_0-mlogloss:1.08298\n",
      "[15]\tvalidation_0-mlogloss:1.08202\n",
      "[16]\tvalidation_0-mlogloss:1.08096\n",
      "[17]\tvalidation_0-mlogloss:1.08002\n",
      "[18]\tvalidation_0-mlogloss:1.07912\n",
      "[19]\tvalidation_0-mlogloss:1.07784\n",
      "[20]\tvalidation_0-mlogloss:1.07683\n",
      "[21]\tvalidation_0-mlogloss:1.07599\n",
      "[22]\tvalidation_0-mlogloss:1.07529\n",
      "[23]\tvalidation_0-mlogloss:1.07417\n",
      "[24]\tvalidation_0-mlogloss:1.07315\n",
      "[25]\tvalidation_0-mlogloss:1.07292\n",
      "[26]\tvalidation_0-mlogloss:1.07196\n",
      "[27]\tvalidation_0-mlogloss:1.07111\n",
      "[28]\tvalidation_0-mlogloss:1.07055\n",
      "[29]\tvalidation_0-mlogloss:1.06983\n",
      "[30]\tvalidation_0-mlogloss:1.06914\n",
      "[31]\tvalidation_0-mlogloss:1.06806\n",
      "[32]\tvalidation_0-mlogloss:1.06747\n",
      "[33]\tvalidation_0-mlogloss:1.06674\n",
      "[34]\tvalidation_0-mlogloss:1.06628\n",
      "[35]\tvalidation_0-mlogloss:1.06549\n",
      "[36]\tvalidation_0-mlogloss:1.06441\n",
      "[37]\tvalidation_0-mlogloss:1.06356\n",
      "[38]\tvalidation_0-mlogloss:1.06251\n",
      "[39]\tvalidation_0-mlogloss:1.06135\n",
      "[40]\tvalidation_0-mlogloss:1.06041\n",
      "[41]\tvalidation_0-mlogloss:1.05989\n",
      "[42]\tvalidation_0-mlogloss:1.05953\n",
      "[43]\tvalidation_0-mlogloss:1.05884\n",
      "[44]\tvalidation_0-mlogloss:1.05845\n",
      "[45]\tvalidation_0-mlogloss:1.05782\n",
      "[46]\tvalidation_0-mlogloss:1.05696\n",
      "[47]\tvalidation_0-mlogloss:1.05642\n",
      "[48]\tvalidation_0-mlogloss:1.05586\n",
      "[49]\tvalidation_0-mlogloss:1.05487\n",
      "[50]\tvalidation_0-mlogloss:1.05440\n",
      "[51]\tvalidation_0-mlogloss:1.05367\n",
      "[52]\tvalidation_0-mlogloss:1.05321\n",
      "[53]\tvalidation_0-mlogloss:1.05256\n",
      "[54]\tvalidation_0-mlogloss:1.05211\n",
      "[55]\tvalidation_0-mlogloss:1.05170\n",
      "[56]\tvalidation_0-mlogloss:1.05108\n",
      "[57]\tvalidation_0-mlogloss:1.05052\n",
      "[58]\tvalidation_0-mlogloss:1.05029\n",
      "[59]\tvalidation_0-mlogloss:1.04979\n",
      "[60]\tvalidation_0-mlogloss:1.04965\n",
      "[61]\tvalidation_0-mlogloss:1.04909\n",
      "[62]\tvalidation_0-mlogloss:1.04845\n",
      "[63]\tvalidation_0-mlogloss:1.04810\n",
      "[64]\tvalidation_0-mlogloss:1.04763\n",
      "[65]\tvalidation_0-mlogloss:1.04726\n",
      "[66]\tvalidation_0-mlogloss:1.04658\n",
      "[67]\tvalidation_0-mlogloss:1.04605\n",
      "[68]\tvalidation_0-mlogloss:1.04580\n",
      "[69]\tvalidation_0-mlogloss:1.04540\n",
      "[70]\tvalidation_0-mlogloss:1.04487\n",
      "[71]\tvalidation_0-mlogloss:1.04426\n",
      "[72]\tvalidation_0-mlogloss:1.04395\n",
      "[73]\tvalidation_0-mlogloss:1.04332\n",
      "[74]\tvalidation_0-mlogloss:1.04281\n",
      "[75]\tvalidation_0-mlogloss:1.04257\n",
      "[76]\tvalidation_0-mlogloss:1.04212\n",
      "[77]\tvalidation_0-mlogloss:1.04145\n",
      "[78]\tvalidation_0-mlogloss:1.04092\n",
      "[79]\tvalidation_0-mlogloss:1.04038\n",
      "[80]\tvalidation_0-mlogloss:1.03990\n",
      "[81]\tvalidation_0-mlogloss:1.03933\n",
      "[82]\tvalidation_0-mlogloss:1.03863\n",
      "[83]\tvalidation_0-mlogloss:1.03811\n",
      "[84]\tvalidation_0-mlogloss:1.03770\n",
      "[85]\tvalidation_0-mlogloss:1.03731\n",
      "[86]\tvalidation_0-mlogloss:1.03683\n",
      "[87]\tvalidation_0-mlogloss:1.03638\n",
      "[88]\tvalidation_0-mlogloss:1.03611\n",
      "[89]\tvalidation_0-mlogloss:1.03565\n",
      "[90]\tvalidation_0-mlogloss:1.03538\n",
      "[91]\tvalidation_0-mlogloss:1.03488\n",
      "[92]\tvalidation_0-mlogloss:1.03445\n",
      "[93]\tvalidation_0-mlogloss:1.03389\n",
      "[94]\tvalidation_0-mlogloss:1.03383\n",
      "[95]\tvalidation_0-mlogloss:1.03346\n",
      "[96]\tvalidation_0-mlogloss:1.03315\n",
      "[97]\tvalidation_0-mlogloss:1.03255\n",
      "[98]\tvalidation_0-mlogloss:1.03224\n",
      "[99]\tvalidation_0-mlogloss:1.03199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:14,551]\u001b[0m Trial 10 finished with value: 0.5271929824561403 and parameters: {'max_depth': 7, 'learning_rate': 0.01, 'gamma': 1.0, 'reg_lambda': 6, 'scale_pos_weight': 1}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09752\n",
      "[1]\tvalidation_0-mlogloss:1.09600\n",
      "[2]\tvalidation_0-mlogloss:1.09510\n",
      "[3]\tvalidation_0-mlogloss:1.09411\n",
      "[4]\tvalidation_0-mlogloss:1.09273\n",
      "[5]\tvalidation_0-mlogloss:1.09155\n",
      "[6]\tvalidation_0-mlogloss:1.09085\n",
      "[7]\tvalidation_0-mlogloss:1.08964\n",
      "[8]\tvalidation_0-mlogloss:1.08863\n",
      "[9]\tvalidation_0-mlogloss:1.08771\n",
      "[10]\tvalidation_0-mlogloss:1.08691\n",
      "[11]\tvalidation_0-mlogloss:1.08586\n",
      "[12]\tvalidation_0-mlogloss:1.08473\n",
      "[13]\tvalidation_0-mlogloss:1.08382\n",
      "[14]\tvalidation_0-mlogloss:1.08298\n",
      "[15]\tvalidation_0-mlogloss:1.08202\n",
      "[16]\tvalidation_0-mlogloss:1.08096\n",
      "[17]\tvalidation_0-mlogloss:1.08002\n",
      "[18]\tvalidation_0-mlogloss:1.07912\n",
      "[19]\tvalidation_0-mlogloss:1.07784\n",
      "[20]\tvalidation_0-mlogloss:1.07683\n",
      "[21]\tvalidation_0-mlogloss:1.07599\n",
      "[22]\tvalidation_0-mlogloss:1.07529\n",
      "[23]\tvalidation_0-mlogloss:1.07417\n",
      "[24]\tvalidation_0-mlogloss:1.07315\n",
      "[25]\tvalidation_0-mlogloss:1.07292\n",
      "[26]\tvalidation_0-mlogloss:1.07196\n",
      "[27]\tvalidation_0-mlogloss:1.07111\n",
      "[28]\tvalidation_0-mlogloss:1.07055\n",
      "[29]\tvalidation_0-mlogloss:1.06983\n",
      "[30]\tvalidation_0-mlogloss:1.06914\n",
      "[31]\tvalidation_0-mlogloss:1.06806\n",
      "[32]\tvalidation_0-mlogloss:1.06747\n",
      "[33]\tvalidation_0-mlogloss:1.06674\n",
      "[34]\tvalidation_0-mlogloss:1.06628\n",
      "[35]\tvalidation_0-mlogloss:1.06549\n",
      "[36]\tvalidation_0-mlogloss:1.06441\n",
      "[37]\tvalidation_0-mlogloss:1.06356\n",
      "[38]\tvalidation_0-mlogloss:1.06251\n",
      "[39]\tvalidation_0-mlogloss:1.06135\n",
      "[40]\tvalidation_0-mlogloss:1.06041\n",
      "[41]\tvalidation_0-mlogloss:1.05989\n",
      "[42]\tvalidation_0-mlogloss:1.05953\n",
      "[43]\tvalidation_0-mlogloss:1.05884\n",
      "[44]\tvalidation_0-mlogloss:1.05845\n",
      "[45]\tvalidation_0-mlogloss:1.05782\n",
      "[46]\tvalidation_0-mlogloss:1.05696\n",
      "[47]\tvalidation_0-mlogloss:1.05642\n",
      "[48]\tvalidation_0-mlogloss:1.05586\n",
      "[49]\tvalidation_0-mlogloss:1.05487\n",
      "[50]\tvalidation_0-mlogloss:1.05440\n",
      "[51]\tvalidation_0-mlogloss:1.05367\n",
      "[52]\tvalidation_0-mlogloss:1.05321\n",
      "[53]\tvalidation_0-mlogloss:1.05256\n",
      "[54]\tvalidation_0-mlogloss:1.05211\n",
      "[55]\tvalidation_0-mlogloss:1.05170\n",
      "[56]\tvalidation_0-mlogloss:1.05108\n",
      "[57]\tvalidation_0-mlogloss:1.05052\n",
      "[58]\tvalidation_0-mlogloss:1.05029\n",
      "[59]\tvalidation_0-mlogloss:1.04979\n",
      "[60]\tvalidation_0-mlogloss:1.04965\n",
      "[61]\tvalidation_0-mlogloss:1.04909\n",
      "[62]\tvalidation_0-mlogloss:1.04845\n",
      "[63]\tvalidation_0-mlogloss:1.04810\n",
      "[64]\tvalidation_0-mlogloss:1.04763\n",
      "[65]\tvalidation_0-mlogloss:1.04726\n",
      "[66]\tvalidation_0-mlogloss:1.04658\n",
      "[67]\tvalidation_0-mlogloss:1.04605\n",
      "[68]\tvalidation_0-mlogloss:1.04580\n",
      "[69]\tvalidation_0-mlogloss:1.04540\n",
      "[70]\tvalidation_0-mlogloss:1.04487\n",
      "[71]\tvalidation_0-mlogloss:1.04426\n",
      "[72]\tvalidation_0-mlogloss:1.04395\n",
      "[73]\tvalidation_0-mlogloss:1.04332\n",
      "[74]\tvalidation_0-mlogloss:1.04281\n",
      "[75]\tvalidation_0-mlogloss:1.04257\n",
      "[76]\tvalidation_0-mlogloss:1.04212\n",
      "[77]\tvalidation_0-mlogloss:1.04145\n",
      "[78]\tvalidation_0-mlogloss:1.04092\n",
      "[79]\tvalidation_0-mlogloss:1.04038\n",
      "[80]\tvalidation_0-mlogloss:1.03990\n",
      "[81]\tvalidation_0-mlogloss:1.03933\n",
      "[82]\tvalidation_0-mlogloss:1.03863\n",
      "[83]\tvalidation_0-mlogloss:1.03811\n",
      "[84]\tvalidation_0-mlogloss:1.03770\n",
      "[85]\tvalidation_0-mlogloss:1.03731\n",
      "[86]\tvalidation_0-mlogloss:1.03683\n",
      "[87]\tvalidation_0-mlogloss:1.03638\n",
      "[88]\tvalidation_0-mlogloss:1.03611\n",
      "[89]\tvalidation_0-mlogloss:1.03565\n",
      "[90]\tvalidation_0-mlogloss:1.03538\n",
      "[91]\tvalidation_0-mlogloss:1.03488\n",
      "[92]\tvalidation_0-mlogloss:1.03445\n",
      "[93]\tvalidation_0-mlogloss:1.03389\n",
      "[94]\tvalidation_0-mlogloss:1.03383\n",
      "[95]\tvalidation_0-mlogloss:1.03346\n",
      "[96]\tvalidation_0-mlogloss:1.03315\n",
      "[97]\tvalidation_0-mlogloss:1.03255\n",
      "[98]\tvalidation_0-mlogloss:1.03224\n",
      "[99]\tvalidation_0-mlogloss:1.03199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:16,235]\u001b[0m Trial 11 finished with value: 0.5271929824561403 and parameters: {'max_depth': 7, 'learning_rate': 0.01, 'gamma': 1.0, 'reg_lambda': 6, 'scale_pos_weight': 1}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09752\n",
      "[1]\tvalidation_0-mlogloss:1.09600\n",
      "[2]\tvalidation_0-mlogloss:1.09510\n",
      "[3]\tvalidation_0-mlogloss:1.09411\n",
      "[4]\tvalidation_0-mlogloss:1.09273\n",
      "[5]\tvalidation_0-mlogloss:1.09155\n",
      "[6]\tvalidation_0-mlogloss:1.09085\n",
      "[7]\tvalidation_0-mlogloss:1.08964\n",
      "[8]\tvalidation_0-mlogloss:1.08863\n",
      "[9]\tvalidation_0-mlogloss:1.08771\n",
      "[10]\tvalidation_0-mlogloss:1.08691\n",
      "[11]\tvalidation_0-mlogloss:1.08586\n",
      "[12]\tvalidation_0-mlogloss:1.08473\n",
      "[13]\tvalidation_0-mlogloss:1.08382\n",
      "[14]\tvalidation_0-mlogloss:1.08298\n",
      "[15]\tvalidation_0-mlogloss:1.08202\n",
      "[16]\tvalidation_0-mlogloss:1.08096\n",
      "[17]\tvalidation_0-mlogloss:1.08002\n",
      "[18]\tvalidation_0-mlogloss:1.07912\n",
      "[19]\tvalidation_0-mlogloss:1.07784\n",
      "[20]\tvalidation_0-mlogloss:1.07683\n",
      "[21]\tvalidation_0-mlogloss:1.07599\n",
      "[22]\tvalidation_0-mlogloss:1.07529\n",
      "[23]\tvalidation_0-mlogloss:1.07417\n",
      "[24]\tvalidation_0-mlogloss:1.07315\n",
      "[25]\tvalidation_0-mlogloss:1.07292\n",
      "[26]\tvalidation_0-mlogloss:1.07196\n",
      "[27]\tvalidation_0-mlogloss:1.07111\n",
      "[28]\tvalidation_0-mlogloss:1.07055\n",
      "[29]\tvalidation_0-mlogloss:1.06983\n",
      "[30]\tvalidation_0-mlogloss:1.06914\n",
      "[31]\tvalidation_0-mlogloss:1.06806\n",
      "[32]\tvalidation_0-mlogloss:1.06747\n",
      "[33]\tvalidation_0-mlogloss:1.06674\n",
      "[34]\tvalidation_0-mlogloss:1.06628\n",
      "[35]\tvalidation_0-mlogloss:1.06549\n",
      "[36]\tvalidation_0-mlogloss:1.06441\n",
      "[37]\tvalidation_0-mlogloss:1.06356\n",
      "[38]\tvalidation_0-mlogloss:1.06251\n",
      "[39]\tvalidation_0-mlogloss:1.06135\n",
      "[40]\tvalidation_0-mlogloss:1.06041\n",
      "[41]\tvalidation_0-mlogloss:1.05989\n",
      "[42]\tvalidation_0-mlogloss:1.05953\n",
      "[43]\tvalidation_0-mlogloss:1.05884\n",
      "[44]\tvalidation_0-mlogloss:1.05845\n",
      "[45]\tvalidation_0-mlogloss:1.05782\n",
      "[46]\tvalidation_0-mlogloss:1.05696\n",
      "[47]\tvalidation_0-mlogloss:1.05642\n",
      "[48]\tvalidation_0-mlogloss:1.05586\n",
      "[49]\tvalidation_0-mlogloss:1.05487\n",
      "[50]\tvalidation_0-mlogloss:1.05440\n",
      "[51]\tvalidation_0-mlogloss:1.05367\n",
      "[52]\tvalidation_0-mlogloss:1.05321\n",
      "[53]\tvalidation_0-mlogloss:1.05256\n",
      "[54]\tvalidation_0-mlogloss:1.05211\n",
      "[55]\tvalidation_0-mlogloss:1.05170\n",
      "[56]\tvalidation_0-mlogloss:1.05108\n",
      "[57]\tvalidation_0-mlogloss:1.05052\n",
      "[58]\tvalidation_0-mlogloss:1.05029\n",
      "[59]\tvalidation_0-mlogloss:1.04979\n",
      "[60]\tvalidation_0-mlogloss:1.04965\n",
      "[61]\tvalidation_0-mlogloss:1.04909\n",
      "[62]\tvalidation_0-mlogloss:1.04845\n",
      "[63]\tvalidation_0-mlogloss:1.04810\n",
      "[64]\tvalidation_0-mlogloss:1.04763\n",
      "[65]\tvalidation_0-mlogloss:1.04726\n",
      "[66]\tvalidation_0-mlogloss:1.04658\n",
      "[67]\tvalidation_0-mlogloss:1.04605\n",
      "[68]\tvalidation_0-mlogloss:1.04580\n",
      "[69]\tvalidation_0-mlogloss:1.04540\n",
      "[70]\tvalidation_0-mlogloss:1.04487\n",
      "[71]\tvalidation_0-mlogloss:1.04426\n",
      "[72]\tvalidation_0-mlogloss:1.04395\n",
      "[73]\tvalidation_0-mlogloss:1.04332\n",
      "[74]\tvalidation_0-mlogloss:1.04281\n",
      "[75]\tvalidation_0-mlogloss:1.04257\n",
      "[76]\tvalidation_0-mlogloss:1.04212\n",
      "[77]\tvalidation_0-mlogloss:1.04145\n",
      "[78]\tvalidation_0-mlogloss:1.04092\n",
      "[79]\tvalidation_0-mlogloss:1.04038\n",
      "[80]\tvalidation_0-mlogloss:1.03990\n",
      "[81]\tvalidation_0-mlogloss:1.03933\n",
      "[82]\tvalidation_0-mlogloss:1.03863\n",
      "[83]\tvalidation_0-mlogloss:1.03811\n",
      "[84]\tvalidation_0-mlogloss:1.03770\n",
      "[85]\tvalidation_0-mlogloss:1.03731\n",
      "[86]\tvalidation_0-mlogloss:1.03683\n",
      "[87]\tvalidation_0-mlogloss:1.03638\n",
      "[88]\tvalidation_0-mlogloss:1.03611\n",
      "[89]\tvalidation_0-mlogloss:1.03565\n",
      "[90]\tvalidation_0-mlogloss:1.03538\n",
      "[91]\tvalidation_0-mlogloss:1.03488\n",
      "[92]\tvalidation_0-mlogloss:1.03445\n",
      "[93]\tvalidation_0-mlogloss:1.03389\n",
      "[94]\tvalidation_0-mlogloss:1.03383\n",
      "[95]\tvalidation_0-mlogloss:1.03346\n",
      "[96]\tvalidation_0-mlogloss:1.03315\n",
      "[97]\tvalidation_0-mlogloss:1.03255\n",
      "[98]\tvalidation_0-mlogloss:1.03224\n",
      "[99]\tvalidation_0-mlogloss:1.03199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:17,718]\u001b[0m Trial 12 finished with value: 0.5271929824561403 and parameters: {'max_depth': 7, 'learning_rate': 0.01, 'gamma': 1.0, 'reg_lambda': 6, 'scale_pos_weight': 1}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09425\n",
      "[1]\tvalidation_0-mlogloss:1.08917\n",
      "[2]\tvalidation_0-mlogloss:1.08575\n",
      "[3]\tvalidation_0-mlogloss:1.08238\n",
      "[4]\tvalidation_0-mlogloss:1.07773\n",
      "[5]\tvalidation_0-mlogloss:1.07427\n",
      "[6]\tvalidation_0-mlogloss:1.07228\n",
      "[7]\tvalidation_0-mlogloss:1.06881\n",
      "[8]\tvalidation_0-mlogloss:1.06578\n",
      "[9]\tvalidation_0-mlogloss:1.06371\n",
      "[10]\tvalidation_0-mlogloss:1.06193\n",
      "[11]\tvalidation_0-mlogloss:1.05948\n",
      "[12]\tvalidation_0-mlogloss:1.05641\n",
      "[13]\tvalidation_0-mlogloss:1.05413\n",
      "[14]\tvalidation_0-mlogloss:1.05220\n",
      "[15]\tvalidation_0-mlogloss:1.04999\n",
      "[16]\tvalidation_0-mlogloss:1.04729\n",
      "[17]\tvalidation_0-mlogloss:1.04533\n",
      "[18]\tvalidation_0-mlogloss:1.04336\n",
      "[19]\tvalidation_0-mlogloss:1.04139\n",
      "[20]\tvalidation_0-mlogloss:1.03934\n",
      "[21]\tvalidation_0-mlogloss:1.03780\n",
      "[22]\tvalidation_0-mlogloss:1.03598\n",
      "[23]\tvalidation_0-mlogloss:1.03394\n",
      "[24]\tvalidation_0-mlogloss:1.03262\n",
      "[25]\tvalidation_0-mlogloss:1.03245\n",
      "[26]\tvalidation_0-mlogloss:1.03087\n",
      "[27]\tvalidation_0-mlogloss:1.02938\n",
      "[28]\tvalidation_0-mlogloss:1.02886\n",
      "[29]\tvalidation_0-mlogloss:1.02808\n",
      "[30]\tvalidation_0-mlogloss:1.02693\n",
      "[31]\tvalidation_0-mlogloss:1.02539\n",
      "[32]\tvalidation_0-mlogloss:1.02450\n",
      "[33]\tvalidation_0-mlogloss:1.02342\n",
      "[34]\tvalidation_0-mlogloss:1.02299\n",
      "[35]\tvalidation_0-mlogloss:1.02215\n",
      "[36]\tvalidation_0-mlogloss:1.02080\n",
      "[37]\tvalidation_0-mlogloss:1.01970\n",
      "[38]\tvalidation_0-mlogloss:1.01880\n",
      "[39]\tvalidation_0-mlogloss:1.01764\n",
      "[40]\tvalidation_0-mlogloss:1.01667\n",
      "[41]\tvalidation_0-mlogloss:1.01655\n",
      "[42]\tvalidation_0-mlogloss:1.01661\n",
      "[43]\tvalidation_0-mlogloss:1.01599\n",
      "[44]\tvalidation_0-mlogloss:1.01565\n",
      "[45]\tvalidation_0-mlogloss:1.01497\n",
      "[46]\tvalidation_0-mlogloss:1.01390\n",
      "[47]\tvalidation_0-mlogloss:1.01353\n",
      "[48]\tvalidation_0-mlogloss:1.01284\n",
      "[49]\tvalidation_0-mlogloss:1.01166\n",
      "[50]\tvalidation_0-mlogloss:1.01116\n",
      "[51]\tvalidation_0-mlogloss:1.01109\n",
      "[52]\tvalidation_0-mlogloss:1.01096\n",
      "[53]\tvalidation_0-mlogloss:1.01066\n",
      "[54]\tvalidation_0-mlogloss:1.01031\n",
      "[55]\tvalidation_0-mlogloss:1.00985\n",
      "[56]\tvalidation_0-mlogloss:1.00919\n",
      "[57]\tvalidation_0-mlogloss:1.00903\n",
      "[58]\tvalidation_0-mlogloss:1.00899\n",
      "[59]\tvalidation_0-mlogloss:1.00890\n",
      "[60]\tvalidation_0-mlogloss:1.00871\n",
      "[61]\tvalidation_0-mlogloss:1.00862\n",
      "[62]\tvalidation_0-mlogloss:1.00848\n",
      "[63]\tvalidation_0-mlogloss:1.00823\n",
      "[64]\tvalidation_0-mlogloss:1.00816\n",
      "[65]\tvalidation_0-mlogloss:1.00791\n",
      "[66]\tvalidation_0-mlogloss:1.00768\n",
      "[67]\tvalidation_0-mlogloss:1.00763\n",
      "[68]\tvalidation_0-mlogloss:1.00771\n",
      "[69]\tvalidation_0-mlogloss:1.00718\n",
      "[70]\tvalidation_0-mlogloss:1.00742\n",
      "[71]\tvalidation_0-mlogloss:1.00703\n",
      "[72]\tvalidation_0-mlogloss:1.00699\n",
      "[73]\tvalidation_0-mlogloss:1.00680\n",
      "[74]\tvalidation_0-mlogloss:1.00649\n",
      "[75]\tvalidation_0-mlogloss:1.00637\n",
      "[76]\tvalidation_0-mlogloss:1.00615\n",
      "[77]\tvalidation_0-mlogloss:1.00617\n",
      "[78]\tvalidation_0-mlogloss:1.00647\n",
      "[79]\tvalidation_0-mlogloss:1.00647\n",
      "[80]\tvalidation_0-mlogloss:1.00615\n",
      "[81]\tvalidation_0-mlogloss:1.00618\n",
      "[82]\tvalidation_0-mlogloss:1.00588\n",
      "[83]\tvalidation_0-mlogloss:1.00571\n",
      "[84]\tvalidation_0-mlogloss:1.00568\n",
      "[85]\tvalidation_0-mlogloss:1.00565\n",
      "[86]\tvalidation_0-mlogloss:1.00570\n",
      "[87]\tvalidation_0-mlogloss:1.00549\n",
      "[88]\tvalidation_0-mlogloss:1.00550\n",
      "[89]\tvalidation_0-mlogloss:1.00541\n",
      "[90]\tvalidation_0-mlogloss:1.00525\n",
      "[91]\tvalidation_0-mlogloss:1.00503\n",
      "[92]\tvalidation_0-mlogloss:1.00486\n",
      "[93]\tvalidation_0-mlogloss:1.00455\n",
      "[94]\tvalidation_0-mlogloss:1.00488\n",
      "[95]\tvalidation_0-mlogloss:1.00483\n",
      "[96]\tvalidation_0-mlogloss:1.00503\n",
      "[97]\tvalidation_0-mlogloss:1.00517\n",
      "[98]\tvalidation_0-mlogloss:1.00532\n",
      "[99]\tvalidation_0-mlogloss:1.00533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:19,014]\u001b[0m Trial 13 finished with value: 0.5078947368421053 and parameters: {'max_depth': 6, 'learning_rate': 0.04, 'gamma': 0.5, 'reg_lambda': 7, 'scale_pos_weight': 1}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09754\n",
      "[1]\tvalidation_0-mlogloss:1.09630\n",
      "[2]\tvalidation_0-mlogloss:1.09538\n",
      "[3]\tvalidation_0-mlogloss:1.09445\n",
      "[4]\tvalidation_0-mlogloss:1.09313\n",
      "[5]\tvalidation_0-mlogloss:1.09200\n",
      "[6]\tvalidation_0-mlogloss:1.09136\n",
      "[7]\tvalidation_0-mlogloss:1.09029\n",
      "[8]\tvalidation_0-mlogloss:1.08935\n",
      "[9]\tvalidation_0-mlogloss:1.08845\n",
      "[10]\tvalidation_0-mlogloss:1.08770\n",
      "[11]\tvalidation_0-mlogloss:1.08679\n",
      "[12]\tvalidation_0-mlogloss:1.08559\n",
      "[13]\tvalidation_0-mlogloss:1.08468\n",
      "[14]\tvalidation_0-mlogloss:1.08390\n",
      "[15]\tvalidation_0-mlogloss:1.08299\n",
      "[16]\tvalidation_0-mlogloss:1.08177\n",
      "[17]\tvalidation_0-mlogloss:1.08083\n",
      "[18]\tvalidation_0-mlogloss:1.07998\n",
      "[19]\tvalidation_0-mlogloss:1.07878\n",
      "[20]\tvalidation_0-mlogloss:1.07780\n",
      "[21]\tvalidation_0-mlogloss:1.07716\n",
      "[22]\tvalidation_0-mlogloss:1.07634\n",
      "[23]\tvalidation_0-mlogloss:1.07519\n",
      "[24]\tvalidation_0-mlogloss:1.07428\n",
      "[25]\tvalidation_0-mlogloss:1.07400\n",
      "[26]\tvalidation_0-mlogloss:1.07304\n",
      "[27]\tvalidation_0-mlogloss:1.07211\n",
      "[28]\tvalidation_0-mlogloss:1.07155\n",
      "[29]\tvalidation_0-mlogloss:1.07085\n",
      "[30]\tvalidation_0-mlogloss:1.07017\n",
      "[31]\tvalidation_0-mlogloss:1.06906\n",
      "[32]\tvalidation_0-mlogloss:1.06845\n",
      "[33]\tvalidation_0-mlogloss:1.06775\n",
      "[34]\tvalidation_0-mlogloss:1.06729\n",
      "[35]\tvalidation_0-mlogloss:1.06641\n",
      "[36]\tvalidation_0-mlogloss:1.06553\n",
      "[37]\tvalidation_0-mlogloss:1.06470\n",
      "[38]\tvalidation_0-mlogloss:1.06373\n",
      "[39]\tvalidation_0-mlogloss:1.06256\n",
      "[40]\tvalidation_0-mlogloss:1.06157\n",
      "[41]\tvalidation_0-mlogloss:1.06099\n",
      "[42]\tvalidation_0-mlogloss:1.06058\n",
      "[43]\tvalidation_0-mlogloss:1.05986\n",
      "[44]\tvalidation_0-mlogloss:1.05939\n",
      "[45]\tvalidation_0-mlogloss:1.05875\n",
      "[46]\tvalidation_0-mlogloss:1.05782\n",
      "[47]\tvalidation_0-mlogloss:1.05740\n",
      "[48]\tvalidation_0-mlogloss:1.05685\n",
      "[49]\tvalidation_0-mlogloss:1.05596\n",
      "[50]\tvalidation_0-mlogloss:1.05531\n",
      "[51]\tvalidation_0-mlogloss:1.05459\n",
      "[52]\tvalidation_0-mlogloss:1.05417\n",
      "[53]\tvalidation_0-mlogloss:1.05352\n",
      "[54]\tvalidation_0-mlogloss:1.05305\n",
      "[55]\tvalidation_0-mlogloss:1.05263\n",
      "[56]\tvalidation_0-mlogloss:1.05205\n",
      "[57]\tvalidation_0-mlogloss:1.05149\n",
      "[58]\tvalidation_0-mlogloss:1.05133\n",
      "[59]\tvalidation_0-mlogloss:1.05095\n",
      "[60]\tvalidation_0-mlogloss:1.05085\n",
      "[61]\tvalidation_0-mlogloss:1.05019\n",
      "[62]\tvalidation_0-mlogloss:1.04956\n",
      "[63]\tvalidation_0-mlogloss:1.04927\n",
      "[64]\tvalidation_0-mlogloss:1.04881\n",
      "[65]\tvalidation_0-mlogloss:1.04842\n",
      "[66]\tvalidation_0-mlogloss:1.04776\n",
      "[67]\tvalidation_0-mlogloss:1.04720\n",
      "[68]\tvalidation_0-mlogloss:1.04695\n",
      "[69]\tvalidation_0-mlogloss:1.04648\n",
      "[70]\tvalidation_0-mlogloss:1.04602\n",
      "[71]\tvalidation_0-mlogloss:1.04541\n",
      "[72]\tvalidation_0-mlogloss:1.04510\n",
      "[73]\tvalidation_0-mlogloss:1.04448\n",
      "[74]\tvalidation_0-mlogloss:1.04392\n",
      "[75]\tvalidation_0-mlogloss:1.04375\n",
      "[76]\tvalidation_0-mlogloss:1.04328\n",
      "[77]\tvalidation_0-mlogloss:1.04275\n",
      "[78]\tvalidation_0-mlogloss:1.04219\n",
      "[79]\tvalidation_0-mlogloss:1.04164\n",
      "[80]\tvalidation_0-mlogloss:1.04107\n",
      "[81]\tvalidation_0-mlogloss:1.04048\n",
      "[82]\tvalidation_0-mlogloss:1.03977\n",
      "[83]\tvalidation_0-mlogloss:1.03916\n",
      "[84]\tvalidation_0-mlogloss:1.03869\n",
      "[85]\tvalidation_0-mlogloss:1.03825\n",
      "[86]\tvalidation_0-mlogloss:1.03781\n",
      "[87]\tvalidation_0-mlogloss:1.03727\n",
      "[88]\tvalidation_0-mlogloss:1.03703\n",
      "[89]\tvalidation_0-mlogloss:1.03666\n",
      "[90]\tvalidation_0-mlogloss:1.03649\n",
      "[91]\tvalidation_0-mlogloss:1.03596\n",
      "[92]\tvalidation_0-mlogloss:1.03561\n",
      "[93]\tvalidation_0-mlogloss:1.03508\n",
      "[94]\tvalidation_0-mlogloss:1.03494\n",
      "[95]\tvalidation_0-mlogloss:1.03454\n",
      "[96]\tvalidation_0-mlogloss:1.03422\n",
      "[97]\tvalidation_0-mlogloss:1.03377\n",
      "[98]\tvalidation_0-mlogloss:1.03371\n",
      "[99]\tvalidation_0-mlogloss:1.03352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:20,327]\u001b[0m Trial 14 finished with value: 0.5271929824561403 and parameters: {'max_depth': 6, 'learning_rate': 0.01, 'gamma': 0.75, 'reg_lambda': 8, 'scale_pos_weight': 1}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09486\n",
      "[1]\tvalidation_0-mlogloss:1.09073\n",
      "[2]\tvalidation_0-mlogloss:1.08813\n",
      "[3]\tvalidation_0-mlogloss:1.08520\n",
      "[4]\tvalidation_0-mlogloss:1.08180\n",
      "[5]\tvalidation_0-mlogloss:1.07880\n",
      "[6]\tvalidation_0-mlogloss:1.07701\n",
      "[7]\tvalidation_0-mlogloss:1.07444\n",
      "[8]\tvalidation_0-mlogloss:1.07170\n",
      "[9]\tvalidation_0-mlogloss:1.06937\n",
      "[10]\tvalidation_0-mlogloss:1.06777\n",
      "[11]\tvalidation_0-mlogloss:1.06530\n",
      "[12]\tvalidation_0-mlogloss:1.06240\n",
      "[13]\tvalidation_0-mlogloss:1.06063\n",
      "[14]\tvalidation_0-mlogloss:1.05914\n",
      "[15]\tvalidation_0-mlogloss:1.05740\n",
      "[16]\tvalidation_0-mlogloss:1.05491\n",
      "[17]\tvalidation_0-mlogloss:1.05369\n",
      "[18]\tvalidation_0-mlogloss:1.05184\n",
      "[19]\tvalidation_0-mlogloss:1.04983\n",
      "[20]\tvalidation_0-mlogloss:1.04822\n",
      "[21]\tvalidation_0-mlogloss:1.04705\n",
      "[22]\tvalidation_0-mlogloss:1.04576\n",
      "[23]\tvalidation_0-mlogloss:1.04410\n",
      "[24]\tvalidation_0-mlogloss:1.04300\n",
      "[25]\tvalidation_0-mlogloss:1.04306\n",
      "[26]\tvalidation_0-mlogloss:1.04185\n",
      "[27]\tvalidation_0-mlogloss:1.04023\n",
      "[28]\tvalidation_0-mlogloss:1.03945\n",
      "[29]\tvalidation_0-mlogloss:1.03853\n",
      "[30]\tvalidation_0-mlogloss:1.03742\n",
      "[31]\tvalidation_0-mlogloss:1.03580\n",
      "[32]\tvalidation_0-mlogloss:1.03466\n",
      "[33]\tvalidation_0-mlogloss:1.03376\n",
      "[34]\tvalidation_0-mlogloss:1.03338\n",
      "[35]\tvalidation_0-mlogloss:1.03234\n",
      "[36]\tvalidation_0-mlogloss:1.03120\n",
      "[37]\tvalidation_0-mlogloss:1.03003\n",
      "[38]\tvalidation_0-mlogloss:1.02885\n",
      "[39]\tvalidation_0-mlogloss:1.02737\n",
      "[40]\tvalidation_0-mlogloss:1.02623\n",
      "[41]\tvalidation_0-mlogloss:1.02588\n",
      "[42]\tvalidation_0-mlogloss:1.02594\n",
      "[43]\tvalidation_0-mlogloss:1.02534\n",
      "[44]\tvalidation_0-mlogloss:1.02503\n",
      "[45]\tvalidation_0-mlogloss:1.02418\n",
      "[46]\tvalidation_0-mlogloss:1.02307\n",
      "[47]\tvalidation_0-mlogloss:1.02232\n",
      "[48]\tvalidation_0-mlogloss:1.02171\n",
      "[49]\tvalidation_0-mlogloss:1.02045\n",
      "[50]\tvalidation_0-mlogloss:1.01976\n",
      "[51]\tvalidation_0-mlogloss:1.01924\n",
      "[52]\tvalidation_0-mlogloss:1.01903\n",
      "[53]\tvalidation_0-mlogloss:1.01827\n",
      "[54]\tvalidation_0-mlogloss:1.01794\n",
      "[55]\tvalidation_0-mlogloss:1.01762\n",
      "[56]\tvalidation_0-mlogloss:1.01702\n",
      "[57]\tvalidation_0-mlogloss:1.01669\n",
      "[58]\tvalidation_0-mlogloss:1.01678\n",
      "[59]\tvalidation_0-mlogloss:1.01647\n",
      "[60]\tvalidation_0-mlogloss:1.01630\n",
      "[61]\tvalidation_0-mlogloss:1.01565\n",
      "[62]\tvalidation_0-mlogloss:1.01509\n",
      "[63]\tvalidation_0-mlogloss:1.01460\n",
      "[64]\tvalidation_0-mlogloss:1.01431\n",
      "[65]\tvalidation_0-mlogloss:1.01418\n",
      "[66]\tvalidation_0-mlogloss:1.01373\n",
      "[67]\tvalidation_0-mlogloss:1.01344\n",
      "[68]\tvalidation_0-mlogloss:1.01317\n",
      "[69]\tvalidation_0-mlogloss:1.01248\n",
      "[70]\tvalidation_0-mlogloss:1.01190\n",
      "[71]\tvalidation_0-mlogloss:1.01155\n",
      "[72]\tvalidation_0-mlogloss:1.01137\n",
      "[73]\tvalidation_0-mlogloss:1.01081\n",
      "[74]\tvalidation_0-mlogloss:1.01059\n",
      "[75]\tvalidation_0-mlogloss:1.01060\n",
      "[76]\tvalidation_0-mlogloss:1.01052\n",
      "[77]\tvalidation_0-mlogloss:1.01045\n",
      "[78]\tvalidation_0-mlogloss:1.01024\n",
      "[79]\tvalidation_0-mlogloss:1.01004\n",
      "[80]\tvalidation_0-mlogloss:1.00969\n",
      "[81]\tvalidation_0-mlogloss:1.00954\n",
      "[82]\tvalidation_0-mlogloss:1.00902\n",
      "[83]\tvalidation_0-mlogloss:1.00867\n",
      "[84]\tvalidation_0-mlogloss:1.00857\n",
      "[85]\tvalidation_0-mlogloss:1.00824\n",
      "[86]\tvalidation_0-mlogloss:1.00794\n",
      "[87]\tvalidation_0-mlogloss:1.00766\n",
      "[88]\tvalidation_0-mlogloss:1.00753\n",
      "[89]\tvalidation_0-mlogloss:1.00724\n",
      "[90]\tvalidation_0-mlogloss:1.00716\n",
      "[91]\tvalidation_0-mlogloss:1.00683\n",
      "[92]\tvalidation_0-mlogloss:1.00660\n",
      "[93]\tvalidation_0-mlogloss:1.00632\n",
      "[94]\tvalidation_0-mlogloss:1.00642\n",
      "[95]\tvalidation_0-mlogloss:1.00616\n",
      "[96]\tvalidation_0-mlogloss:1.00613\n",
      "[97]\tvalidation_0-mlogloss:1.00602\n",
      "[98]\tvalidation_0-mlogloss:1.00613\n",
      "[99]\tvalidation_0-mlogloss:1.00599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:21,512]\u001b[0m Trial 15 finished with value: 0.5201754385964912 and parameters: {'max_depth': 5, 'learning_rate': 0.03, 'gamma': 0.25, 'reg_lambda': 5, 'scale_pos_weight': 1}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09122\n",
      "[1]\tvalidation_0-mlogloss:1.08264\n",
      "[2]\tvalidation_0-mlogloss:1.07724\n",
      "[3]\tvalidation_0-mlogloss:1.07180\n",
      "[4]\tvalidation_0-mlogloss:1.06465\n",
      "[5]\tvalidation_0-mlogloss:1.05946\n",
      "[6]\tvalidation_0-mlogloss:1.05706\n",
      "[7]\tvalidation_0-mlogloss:1.05144\n",
      "[8]\tvalidation_0-mlogloss:1.04706\n",
      "[9]\tvalidation_0-mlogloss:1.04389\n",
      "[10]\tvalidation_0-mlogloss:1.04123\n",
      "[11]\tvalidation_0-mlogloss:1.03776\n",
      "[12]\tvalidation_0-mlogloss:1.03366\n",
      "[13]\tvalidation_0-mlogloss:1.03138\n",
      "[14]\tvalidation_0-mlogloss:1.03027\n",
      "[15]\tvalidation_0-mlogloss:1.02828\n",
      "[16]\tvalidation_0-mlogloss:1.02594\n",
      "[17]\tvalidation_0-mlogloss:1.02491\n",
      "[18]\tvalidation_0-mlogloss:1.02343\n",
      "[19]\tvalidation_0-mlogloss:1.02196\n",
      "[20]\tvalidation_0-mlogloss:1.02042\n",
      "[21]\tvalidation_0-mlogloss:1.01876\n",
      "[22]\tvalidation_0-mlogloss:1.01763\n",
      "[23]\tvalidation_0-mlogloss:1.01617\n",
      "[24]\tvalidation_0-mlogloss:1.01554\n",
      "[25]\tvalidation_0-mlogloss:1.01530\n",
      "[26]\tvalidation_0-mlogloss:1.01428\n",
      "[27]\tvalidation_0-mlogloss:1.01246\n",
      "[28]\tvalidation_0-mlogloss:1.01236\n",
      "[29]\tvalidation_0-mlogloss:1.01179\n",
      "[30]\tvalidation_0-mlogloss:1.01111\n",
      "[31]\tvalidation_0-mlogloss:1.01051\n",
      "[32]\tvalidation_0-mlogloss:1.00953\n",
      "[33]\tvalidation_0-mlogloss:1.00897\n",
      "[34]\tvalidation_0-mlogloss:1.00929\n",
      "[35]\tvalidation_0-mlogloss:1.00875\n",
      "[36]\tvalidation_0-mlogloss:1.00840\n",
      "[37]\tvalidation_0-mlogloss:1.00812\n",
      "[38]\tvalidation_0-mlogloss:1.00789\n",
      "[39]\tvalidation_0-mlogloss:1.00759\n",
      "[40]\tvalidation_0-mlogloss:1.00734\n",
      "[41]\tvalidation_0-mlogloss:1.00729\n",
      "[42]\tvalidation_0-mlogloss:1.00786\n",
      "[43]\tvalidation_0-mlogloss:1.00763\n",
      "[44]\tvalidation_0-mlogloss:1.00745\n",
      "[45]\tvalidation_0-mlogloss:1.00722\n",
      "[46]\tvalidation_0-mlogloss:1.00680\n",
      "[47]\tvalidation_0-mlogloss:1.00590\n",
      "[48]\tvalidation_0-mlogloss:1.00602\n",
      "[49]\tvalidation_0-mlogloss:1.00521\n",
      "[50]\tvalidation_0-mlogloss:1.00522\n",
      "[51]\tvalidation_0-mlogloss:1.00564\n",
      "[52]\tvalidation_0-mlogloss:1.00555\n",
      "[53]\tvalidation_0-mlogloss:1.00551\n",
      "[54]\tvalidation_0-mlogloss:1.00547\n",
      "[55]\tvalidation_0-mlogloss:1.00595\n",
      "[56]\tvalidation_0-mlogloss:1.00534\n",
      "[57]\tvalidation_0-mlogloss:1.00523\n",
      "[58]\tvalidation_0-mlogloss:1.00561\n",
      "[59]\tvalidation_0-mlogloss:1.00587\n",
      "[60]\tvalidation_0-mlogloss:1.00561\n",
      "[61]\tvalidation_0-mlogloss:1.00549\n",
      "[62]\tvalidation_0-mlogloss:1.00530\n",
      "[63]\tvalidation_0-mlogloss:1.00520\n",
      "[64]\tvalidation_0-mlogloss:1.00511\n",
      "[65]\tvalidation_0-mlogloss:1.00501\n",
      "[66]\tvalidation_0-mlogloss:1.00526\n",
      "[67]\tvalidation_0-mlogloss:1.00539\n",
      "[68]\tvalidation_0-mlogloss:1.00568\n",
      "[69]\tvalidation_0-mlogloss:1.00528\n",
      "[70]\tvalidation_0-mlogloss:1.00519\n",
      "[71]\tvalidation_0-mlogloss:1.00544\n",
      "[72]\tvalidation_0-mlogloss:1.00548\n",
      "[73]\tvalidation_0-mlogloss:1.00543\n",
      "[74]\tvalidation_0-mlogloss:1.00538\n",
      "[75]\tvalidation_0-mlogloss:1.00568\n",
      "[76]\tvalidation_0-mlogloss:1.00593\n",
      "[77]\tvalidation_0-mlogloss:1.00583\n",
      "[78]\tvalidation_0-mlogloss:1.00624\n",
      "[79]\tvalidation_0-mlogloss:1.00636\n",
      "[80]\tvalidation_0-mlogloss:1.00616\n",
      "[81]\tvalidation_0-mlogloss:1.00651\n",
      "[82]\tvalidation_0-mlogloss:1.00664\n",
      "[83]\tvalidation_0-mlogloss:1.00632\n",
      "[84]\tvalidation_0-mlogloss:1.00667\n",
      "[85]\tvalidation_0-mlogloss:1.00681\n",
      "[86]\tvalidation_0-mlogloss:1.00688\n",
      "[87]\tvalidation_0-mlogloss:1.00724\n",
      "[88]\tvalidation_0-mlogloss:1.00725\n",
      "[89]\tvalidation_0-mlogloss:1.00737\n",
      "[90]\tvalidation_0-mlogloss:1.00765\n",
      "[91]\tvalidation_0-mlogloss:1.00781\n",
      "[92]\tvalidation_0-mlogloss:1.00810\n",
      "[93]\tvalidation_0-mlogloss:1.00770\n",
      "[94]\tvalidation_0-mlogloss:1.00827\n",
      "[95]\tvalidation_0-mlogloss:1.00829\n",
      "[96]\tvalidation_0-mlogloss:1.00865\n",
      "[97]\tvalidation_0-mlogloss:1.00898\n",
      "[98]\tvalidation_0-mlogloss:1.00951\n",
      "[99]\tvalidation_0-mlogloss:1.00879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:22,921]\u001b[0m Trial 16 finished with value: 0.5070175438596491 and parameters: {'max_depth': 6, 'learning_rate': 0.06999999999999999, 'gamma': 1.0, 'reg_lambda': 7, 'scale_pos_weight': 3}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09647\n",
      "[1]\tvalidation_0-mlogloss:1.09355\n",
      "[2]\tvalidation_0-mlogloss:1.09191\n",
      "[3]\tvalidation_0-mlogloss:1.08992\n",
      "[4]\tvalidation_0-mlogloss:1.08720\n",
      "[5]\tvalidation_0-mlogloss:1.08493\n",
      "[6]\tvalidation_0-mlogloss:1.08348\n",
      "[7]\tvalidation_0-mlogloss:1.08129\n",
      "[8]\tvalidation_0-mlogloss:1.07936\n",
      "[9]\tvalidation_0-mlogloss:1.07734\n",
      "[10]\tvalidation_0-mlogloss:1.07614\n",
      "[11]\tvalidation_0-mlogloss:1.07413\n",
      "[12]\tvalidation_0-mlogloss:1.07207\n",
      "[13]\tvalidation_0-mlogloss:1.07037\n",
      "[14]\tvalidation_0-mlogloss:1.06878\n",
      "[15]\tvalidation_0-mlogloss:1.06724\n",
      "[16]\tvalidation_0-mlogloss:1.06549\n",
      "[17]\tvalidation_0-mlogloss:1.06385\n",
      "[18]\tvalidation_0-mlogloss:1.06248\n",
      "[19]\tvalidation_0-mlogloss:1.06109\n",
      "[20]\tvalidation_0-mlogloss:1.05936\n",
      "[21]\tvalidation_0-mlogloss:1.05840\n",
      "[22]\tvalidation_0-mlogloss:1.05735\n",
      "[23]\tvalidation_0-mlogloss:1.05575\n",
      "[24]\tvalidation_0-mlogloss:1.05417\n",
      "[25]\tvalidation_0-mlogloss:1.05366\n",
      "[26]\tvalidation_0-mlogloss:1.05227\n",
      "[27]\tvalidation_0-mlogloss:1.05101\n",
      "[28]\tvalidation_0-mlogloss:1.05021\n",
      "[29]\tvalidation_0-mlogloss:1.04910\n",
      "[30]\tvalidation_0-mlogloss:1.04812\n",
      "[31]\tvalidation_0-mlogloss:1.04671\n",
      "[32]\tvalidation_0-mlogloss:1.04603\n",
      "[33]\tvalidation_0-mlogloss:1.04508\n",
      "[34]\tvalidation_0-mlogloss:1.04445\n",
      "[35]\tvalidation_0-mlogloss:1.04330\n",
      "[36]\tvalidation_0-mlogloss:1.04178\n",
      "[37]\tvalidation_0-mlogloss:1.04057\n",
      "[38]\tvalidation_0-mlogloss:1.03935\n",
      "[39]\tvalidation_0-mlogloss:1.03811\n",
      "[40]\tvalidation_0-mlogloss:1.03688\n",
      "[41]\tvalidation_0-mlogloss:1.03600\n",
      "[42]\tvalidation_0-mlogloss:1.03591\n",
      "[43]\tvalidation_0-mlogloss:1.03516\n",
      "[44]\tvalidation_0-mlogloss:1.03497\n",
      "[45]\tvalidation_0-mlogloss:1.03422\n",
      "[46]\tvalidation_0-mlogloss:1.03315\n",
      "[47]\tvalidation_0-mlogloss:1.03249\n",
      "[48]\tvalidation_0-mlogloss:1.03187\n",
      "[49]\tvalidation_0-mlogloss:1.03055\n",
      "[50]\tvalidation_0-mlogloss:1.02989\n",
      "[51]\tvalidation_0-mlogloss:1.02934\n",
      "[52]\tvalidation_0-mlogloss:1.02881\n",
      "[53]\tvalidation_0-mlogloss:1.02802\n",
      "[54]\tvalidation_0-mlogloss:1.02761\n",
      "[55]\tvalidation_0-mlogloss:1.02744\n",
      "[56]\tvalidation_0-mlogloss:1.02695\n",
      "[57]\tvalidation_0-mlogloss:1.02649\n",
      "[58]\tvalidation_0-mlogloss:1.02607\n",
      "[59]\tvalidation_0-mlogloss:1.02565\n",
      "[60]\tvalidation_0-mlogloss:1.02570\n",
      "[61]\tvalidation_0-mlogloss:1.02535\n",
      "[62]\tvalidation_0-mlogloss:1.02478\n",
      "[63]\tvalidation_0-mlogloss:1.02421\n",
      "[64]\tvalidation_0-mlogloss:1.02399\n",
      "[65]\tvalidation_0-mlogloss:1.02391\n",
      "[66]\tvalidation_0-mlogloss:1.02332\n",
      "[67]\tvalidation_0-mlogloss:1.02299\n",
      "[68]\tvalidation_0-mlogloss:1.02286\n",
      "[69]\tvalidation_0-mlogloss:1.02236\n",
      "[70]\tvalidation_0-mlogloss:1.02189\n",
      "[71]\tvalidation_0-mlogloss:1.02144\n",
      "[72]\tvalidation_0-mlogloss:1.02131\n",
      "[73]\tvalidation_0-mlogloss:1.02049\n",
      "[74]\tvalidation_0-mlogloss:1.01987\n",
      "[75]\tvalidation_0-mlogloss:1.01976\n",
      "[76]\tvalidation_0-mlogloss:1.01936\n",
      "[77]\tvalidation_0-mlogloss:1.01898\n",
      "[78]\tvalidation_0-mlogloss:1.01868\n",
      "[79]\tvalidation_0-mlogloss:1.01823\n",
      "[80]\tvalidation_0-mlogloss:1.01765\n",
      "[81]\tvalidation_0-mlogloss:1.01721\n",
      "[82]\tvalidation_0-mlogloss:1.01656\n",
      "[83]\tvalidation_0-mlogloss:1.01609\n",
      "[84]\tvalidation_0-mlogloss:1.01575\n",
      "[85]\tvalidation_0-mlogloss:1.01541\n",
      "[86]\tvalidation_0-mlogloss:1.01495\n",
      "[87]\tvalidation_0-mlogloss:1.01487\n",
      "[88]\tvalidation_0-mlogloss:1.01465\n",
      "[89]\tvalidation_0-mlogloss:1.01449\n",
      "[90]\tvalidation_0-mlogloss:1.01421\n",
      "[91]\tvalidation_0-mlogloss:1.01395\n",
      "[92]\tvalidation_0-mlogloss:1.01369\n",
      "[93]\tvalidation_0-mlogloss:1.01317\n",
      "[94]\tvalidation_0-mlogloss:1.01301\n",
      "[95]\tvalidation_0-mlogloss:1.01280\n",
      "[96]\tvalidation_0-mlogloss:1.01253\n",
      "[97]\tvalidation_0-mlogloss:1.01217\n",
      "[98]\tvalidation_0-mlogloss:1.01213\n",
      "[99]\tvalidation_0-mlogloss:1.01212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:24,503]\u001b[0m Trial 17 finished with value: 0.5149122807017544 and parameters: {'max_depth': 7, 'learning_rate': 0.02, 'gamma': 1.0, 'reg_lambda': 4, 'scale_pos_weight': 1}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09372\n",
      "[1]\tvalidation_0-mlogloss:1.08833\n",
      "[2]\tvalidation_0-mlogloss:1.08497\n",
      "[3]\tvalidation_0-mlogloss:1.08138\n",
      "[4]\tvalidation_0-mlogloss:1.07694\n",
      "[5]\tvalidation_0-mlogloss:1.07346\n",
      "[6]\tvalidation_0-mlogloss:1.07171\n",
      "[7]\tvalidation_0-mlogloss:1.06838\n",
      "[8]\tvalidation_0-mlogloss:1.06537\n",
      "[9]\tvalidation_0-mlogloss:1.06282\n",
      "[10]\tvalidation_0-mlogloss:1.06095\n",
      "[11]\tvalidation_0-mlogloss:1.05861\n",
      "[12]\tvalidation_0-mlogloss:1.05550\n",
      "[13]\tvalidation_0-mlogloss:1.05361\n",
      "[14]\tvalidation_0-mlogloss:1.05185\n",
      "[15]\tvalidation_0-mlogloss:1.04965\n",
      "[16]\tvalidation_0-mlogloss:1.04681\n",
      "[17]\tvalidation_0-mlogloss:1.04511\n",
      "[18]\tvalidation_0-mlogloss:1.04292\n",
      "[19]\tvalidation_0-mlogloss:1.04065\n",
      "[20]\tvalidation_0-mlogloss:1.03908\n",
      "[21]\tvalidation_0-mlogloss:1.03780\n",
      "[22]\tvalidation_0-mlogloss:1.03631\n",
      "[23]\tvalidation_0-mlogloss:1.03456\n",
      "[24]\tvalidation_0-mlogloss:1.03345\n",
      "[25]\tvalidation_0-mlogloss:1.03316\n",
      "[26]\tvalidation_0-mlogloss:1.03211\n",
      "[27]\tvalidation_0-mlogloss:1.03054\n",
      "[28]\tvalidation_0-mlogloss:1.02984\n",
      "[29]\tvalidation_0-mlogloss:1.02898\n",
      "[30]\tvalidation_0-mlogloss:1.02775\n",
      "[31]\tvalidation_0-mlogloss:1.02612\n",
      "[32]\tvalidation_0-mlogloss:1.02493\n",
      "[33]\tvalidation_0-mlogloss:1.02386\n",
      "[34]\tvalidation_0-mlogloss:1.02341\n",
      "[35]\tvalidation_0-mlogloss:1.02248\n",
      "[36]\tvalidation_0-mlogloss:1.02122\n",
      "[37]\tvalidation_0-mlogloss:1.01994\n",
      "[38]\tvalidation_0-mlogloss:1.01880\n",
      "[39]\tvalidation_0-mlogloss:1.01767\n",
      "[40]\tvalidation_0-mlogloss:1.01681\n",
      "[41]\tvalidation_0-mlogloss:1.01648\n",
      "[42]\tvalidation_0-mlogloss:1.01680\n",
      "[43]\tvalidation_0-mlogloss:1.01632\n",
      "[44]\tvalidation_0-mlogloss:1.01601\n",
      "[45]\tvalidation_0-mlogloss:1.01525\n",
      "[46]\tvalidation_0-mlogloss:1.01444\n",
      "[47]\tvalidation_0-mlogloss:1.01380\n",
      "[48]\tvalidation_0-mlogloss:1.01346\n",
      "[49]\tvalidation_0-mlogloss:1.01251\n",
      "[50]\tvalidation_0-mlogloss:1.01189\n",
      "[51]\tvalidation_0-mlogloss:1.01149\n",
      "[52]\tvalidation_0-mlogloss:1.01103\n",
      "[53]\tvalidation_0-mlogloss:1.01034\n",
      "[54]\tvalidation_0-mlogloss:1.01003\n",
      "[55]\tvalidation_0-mlogloss:1.00918\n",
      "[56]\tvalidation_0-mlogloss:1.00852\n",
      "[57]\tvalidation_0-mlogloss:1.00838\n",
      "[58]\tvalidation_0-mlogloss:1.00852\n",
      "[59]\tvalidation_0-mlogloss:1.00844\n",
      "[60]\tvalidation_0-mlogloss:1.00813\n",
      "[61]\tvalidation_0-mlogloss:1.00771\n",
      "[62]\tvalidation_0-mlogloss:1.00744\n",
      "[63]\tvalidation_0-mlogloss:1.00723\n",
      "[64]\tvalidation_0-mlogloss:1.00700\n",
      "[65]\tvalidation_0-mlogloss:1.00670\n",
      "[66]\tvalidation_0-mlogloss:1.00652\n",
      "[67]\tvalidation_0-mlogloss:1.00630\n",
      "[68]\tvalidation_0-mlogloss:1.00640\n",
      "[69]\tvalidation_0-mlogloss:1.00600\n",
      "[70]\tvalidation_0-mlogloss:1.00600\n",
      "[71]\tvalidation_0-mlogloss:1.00557\n",
      "[72]\tvalidation_0-mlogloss:1.00541\n",
      "[73]\tvalidation_0-mlogloss:1.00500\n",
      "[74]\tvalidation_0-mlogloss:1.00473\n",
      "[75]\tvalidation_0-mlogloss:1.00464\n",
      "[76]\tvalidation_0-mlogloss:1.00470\n",
      "[77]\tvalidation_0-mlogloss:1.00473\n",
      "[78]\tvalidation_0-mlogloss:1.00470\n",
      "[79]\tvalidation_0-mlogloss:1.00465\n",
      "[80]\tvalidation_0-mlogloss:1.00424\n",
      "[81]\tvalidation_0-mlogloss:1.00416\n",
      "[82]\tvalidation_0-mlogloss:1.00364\n",
      "[83]\tvalidation_0-mlogloss:1.00319\n",
      "[84]\tvalidation_0-mlogloss:1.00302\n",
      "[85]\tvalidation_0-mlogloss:1.00290\n",
      "[86]\tvalidation_0-mlogloss:1.00281\n",
      "[87]\tvalidation_0-mlogloss:1.00254\n",
      "[88]\tvalidation_0-mlogloss:1.00241\n",
      "[89]\tvalidation_0-mlogloss:1.00235\n",
      "[90]\tvalidation_0-mlogloss:1.00227\n",
      "[91]\tvalidation_0-mlogloss:1.00221\n",
      "[92]\tvalidation_0-mlogloss:1.00189\n",
      "[93]\tvalidation_0-mlogloss:1.00161\n",
      "[94]\tvalidation_0-mlogloss:1.00182\n",
      "[95]\tvalidation_0-mlogloss:1.00185\n",
      "[96]\tvalidation_0-mlogloss:1.00204\n",
      "[97]\tvalidation_0-mlogloss:1.00204\n",
      "[98]\tvalidation_0-mlogloss:1.00215\n",
      "[99]\tvalidation_0-mlogloss:1.00222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:25,610]\u001b[0m Trial 18 finished with value: 0.519298245614035 and parameters: {'max_depth': 5, 'learning_rate': 0.04, 'gamma': 0.5, 'reg_lambda': 8, 'scale_pos_weight': 1}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:17:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09647\n",
      "[1]\tvalidation_0-mlogloss:1.09401\n",
      "[2]\tvalidation_0-mlogloss:1.09223\n",
      "[3]\tvalidation_0-mlogloss:1.09045\n",
      "[4]\tvalidation_0-mlogloss:1.08787\n",
      "[5]\tvalidation_0-mlogloss:1.08576\n",
      "[6]\tvalidation_0-mlogloss:1.08464\n",
      "[7]\tvalidation_0-mlogloss:1.08264\n",
      "[8]\tvalidation_0-mlogloss:1.08081\n",
      "[9]\tvalidation_0-mlogloss:1.07913\n",
      "[10]\tvalidation_0-mlogloss:1.07781\n",
      "[11]\tvalidation_0-mlogloss:1.07615\n",
      "[12]\tvalidation_0-mlogloss:1.07413\n",
      "[13]\tvalidation_0-mlogloss:1.07260\n",
      "[14]\tvalidation_0-mlogloss:1.07134\n",
      "[15]\tvalidation_0-mlogloss:1.06986\n",
      "[16]\tvalidation_0-mlogloss:1.06785\n",
      "[17]\tvalidation_0-mlogloss:1.06649\n",
      "[18]\tvalidation_0-mlogloss:1.06488\n",
      "[19]\tvalidation_0-mlogloss:1.06293\n",
      "[20]\tvalidation_0-mlogloss:1.06150\n",
      "[21]\tvalidation_0-mlogloss:1.06043\n",
      "[22]\tvalidation_0-mlogloss:1.05930\n",
      "[23]\tvalidation_0-mlogloss:1.05763\n",
      "[24]\tvalidation_0-mlogloss:1.05628\n",
      "[25]\tvalidation_0-mlogloss:1.05587\n",
      "[26]\tvalidation_0-mlogloss:1.05436\n",
      "[27]\tvalidation_0-mlogloss:1.05301\n",
      "[28]\tvalidation_0-mlogloss:1.05243\n",
      "[29]\tvalidation_0-mlogloss:1.05138\n",
      "[30]\tvalidation_0-mlogloss:1.05014\n",
      "[31]\tvalidation_0-mlogloss:1.04873\n",
      "[32]\tvalidation_0-mlogloss:1.04779\n",
      "[33]\tvalidation_0-mlogloss:1.04680\n",
      "[34]\tvalidation_0-mlogloss:1.04613\n",
      "[35]\tvalidation_0-mlogloss:1.04495\n",
      "[36]\tvalidation_0-mlogloss:1.04371\n",
      "[37]\tvalidation_0-mlogloss:1.04254\n",
      "[38]\tvalidation_0-mlogloss:1.04124\n",
      "[39]\tvalidation_0-mlogloss:1.03988\n",
      "[40]\tvalidation_0-mlogloss:1.03866\n",
      "[41]\tvalidation_0-mlogloss:1.03810\n",
      "[42]\tvalidation_0-mlogloss:1.03771\n",
      "[43]\tvalidation_0-mlogloss:1.03685\n",
      "[44]\tvalidation_0-mlogloss:1.03648\n",
      "[45]\tvalidation_0-mlogloss:1.03567\n",
      "[46]\tvalidation_0-mlogloss:1.03452\n",
      "[47]\tvalidation_0-mlogloss:1.03373\n",
      "[48]\tvalidation_0-mlogloss:1.03307\n",
      "[49]\tvalidation_0-mlogloss:1.03193\n",
      "[50]\tvalidation_0-mlogloss:1.03122\n",
      "[51]\tvalidation_0-mlogloss:1.03057\n",
      "[52]\tvalidation_0-mlogloss:1.03031\n",
      "[53]\tvalidation_0-mlogloss:1.02965\n",
      "[54]\tvalidation_0-mlogloss:1.02914\n",
      "[55]\tvalidation_0-mlogloss:1.02877\n",
      "[56]\tvalidation_0-mlogloss:1.02821\n",
      "[57]\tvalidation_0-mlogloss:1.02759\n",
      "[58]\tvalidation_0-mlogloss:1.02738\n",
      "[59]\tvalidation_0-mlogloss:1.02703\n",
      "[60]\tvalidation_0-mlogloss:1.02700\n",
      "[61]\tvalidation_0-mlogloss:1.02646\n",
      "[62]\tvalidation_0-mlogloss:1.02600\n",
      "[63]\tvalidation_0-mlogloss:1.02553\n",
      "[64]\tvalidation_0-mlogloss:1.02521\n",
      "[65]\tvalidation_0-mlogloss:1.02508\n",
      "[66]\tvalidation_0-mlogloss:1.02459\n",
      "[67]\tvalidation_0-mlogloss:1.02412\n",
      "[68]\tvalidation_0-mlogloss:1.02398\n",
      "[69]\tvalidation_0-mlogloss:1.02350\n",
      "[70]\tvalidation_0-mlogloss:1.02312\n",
      "[71]\tvalidation_0-mlogloss:1.02260\n",
      "[72]\tvalidation_0-mlogloss:1.02237\n",
      "[73]\tvalidation_0-mlogloss:1.02155\n",
      "[74]\tvalidation_0-mlogloss:1.02098\n",
      "[75]\tvalidation_0-mlogloss:1.02081\n",
      "[76]\tvalidation_0-mlogloss:1.02054\n",
      "[77]\tvalidation_0-mlogloss:1.02015\n",
      "[78]\tvalidation_0-mlogloss:1.01993\n",
      "[79]\tvalidation_0-mlogloss:1.01945\n",
      "[80]\tvalidation_0-mlogloss:1.01888\n",
      "[81]\tvalidation_0-mlogloss:1.01838\n",
      "[82]\tvalidation_0-mlogloss:1.01770\n",
      "[83]\tvalidation_0-mlogloss:1.01714\n",
      "[84]\tvalidation_0-mlogloss:1.01673\n",
      "[85]\tvalidation_0-mlogloss:1.01637\n",
      "[86]\tvalidation_0-mlogloss:1.01597\n",
      "[87]\tvalidation_0-mlogloss:1.01551\n",
      "[88]\tvalidation_0-mlogloss:1.01534\n",
      "[89]\tvalidation_0-mlogloss:1.01514\n",
      "[90]\tvalidation_0-mlogloss:1.01514\n",
      "[91]\tvalidation_0-mlogloss:1.01487\n",
      "[92]\tvalidation_0-mlogloss:1.01459\n",
      "[93]\tvalidation_0-mlogloss:1.01408\n",
      "[94]\tvalidation_0-mlogloss:1.01395\n",
      "[95]\tvalidation_0-mlogloss:1.01346\n",
      "[96]\tvalidation_0-mlogloss:1.01319\n",
      "[97]\tvalidation_0-mlogloss:1.01289\n",
      "[98]\tvalidation_0-mlogloss:1.01288\n",
      "[99]\tvalidation_0-mlogloss:1.01290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-20 04:17:26,925]\u001b[0m Trial 19 finished with value: 0.519298245614035 and parameters: {'max_depth': 6, 'learning_rate': 0.02, 'gamma': 0.75, 'reg_lambda': 8, 'scale_pos_weight': 3}. Best is trial 3 with value: 0.5298245614035088.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest value (rmse): 0.52982\n",
      "\tBest params:\n",
      "\t\tmax_depth: 5\n",
      "\t\tlearning_rate: 0.01\n",
      "\t\tgamma: 1.0\n",
      "\t\treg_lambda: 10\n",
      "\t\tscale_pos_weight: 1\n",
      "[04:17:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.09712\tvalidation_1-mlogloss:1.09652\n",
      "[1]\tvalidation_0-mlogloss:1.09576\tvalidation_1-mlogloss:1.09447\n",
      "[2]\tvalidation_0-mlogloss:1.09432\tvalidation_1-mlogloss:1.09244\n",
      "[3]\tvalidation_0-mlogloss:1.09290\tvalidation_1-mlogloss:1.09044\n",
      "[4]\tvalidation_0-mlogloss:1.09158\tvalidation_1-mlogloss:1.08849\n",
      "[5]\tvalidation_0-mlogloss:1.09016\tvalidation_1-mlogloss:1.08656\n",
      "[6]\tvalidation_0-mlogloss:1.08878\tvalidation_1-mlogloss:1.08467\n",
      "[7]\tvalidation_0-mlogloss:1.08750\tvalidation_1-mlogloss:1.08280\n",
      "[8]\tvalidation_0-mlogloss:1.08618\tvalidation_1-mlogloss:1.08098\n",
      "[9]\tvalidation_0-mlogloss:1.08485\tvalidation_1-mlogloss:1.07915\n",
      "[10]\tvalidation_0-mlogloss:1.08368\tvalidation_1-mlogloss:1.07736\n",
      "[11]\tvalidation_0-mlogloss:1.08245\tvalidation_1-mlogloss:1.07562\n",
      "[12]\tvalidation_0-mlogloss:1.08119\tvalidation_1-mlogloss:1.07386\n",
      "[13]\tvalidation_0-mlogloss:1.08012\tvalidation_1-mlogloss:1.07215\n",
      "[14]\tvalidation_0-mlogloss:1.07892\tvalidation_1-mlogloss:1.07044\n",
      "[15]\tvalidation_0-mlogloss:1.07785\tvalidation_1-mlogloss:1.06877\n",
      "[16]\tvalidation_0-mlogloss:1.07669\tvalidation_1-mlogloss:1.06714\n",
      "[17]\tvalidation_0-mlogloss:1.07544\tvalidation_1-mlogloss:1.06549\n",
      "[18]\tvalidation_0-mlogloss:1.07440\tvalidation_1-mlogloss:1.06389\n",
      "[19]\tvalidation_0-mlogloss:1.07344\tvalidation_1-mlogloss:1.06231\n",
      "[20]\tvalidation_0-mlogloss:1.07238\tvalidation_1-mlogloss:1.06073\n",
      "[21]\tvalidation_0-mlogloss:1.07130\tvalidation_1-mlogloss:1.05919\n",
      "[22]\tvalidation_0-mlogloss:1.07043\tvalidation_1-mlogloss:1.05766\n",
      "[23]\tvalidation_0-mlogloss:1.06944\tvalidation_1-mlogloss:1.05615\n",
      "[24]\tvalidation_0-mlogloss:1.06850\tvalidation_1-mlogloss:1.05466\n",
      "[25]\tvalidation_0-mlogloss:1.06759\tvalidation_1-mlogloss:1.05320\n",
      "[26]\tvalidation_0-mlogloss:1.06674\tvalidation_1-mlogloss:1.05174\n",
      "[27]\tvalidation_0-mlogloss:1.06587\tvalidation_1-mlogloss:1.05032\n",
      "[28]\tvalidation_0-mlogloss:1.06521\tvalidation_1-mlogloss:1.04888\n",
      "[29]\tvalidation_0-mlogloss:1.06470\tvalidation_1-mlogloss:1.04746\n",
      "[30]\tvalidation_0-mlogloss:1.06411\tvalidation_1-mlogloss:1.04605\n",
      "[31]\tvalidation_0-mlogloss:1.06344\tvalidation_1-mlogloss:1.04467\n",
      "[32]\tvalidation_0-mlogloss:1.06283\tvalidation_1-mlogloss:1.04331\n",
      "[33]\tvalidation_0-mlogloss:1.06217\tvalidation_1-mlogloss:1.04195\n",
      "[34]\tvalidation_0-mlogloss:1.06153\tvalidation_1-mlogloss:1.04061\n",
      "[35]\tvalidation_0-mlogloss:1.06102\tvalidation_1-mlogloss:1.03929\n",
      "[36]\tvalidation_0-mlogloss:1.06043\tvalidation_1-mlogloss:1.03800\n",
      "[37]\tvalidation_0-mlogloss:1.05993\tvalidation_1-mlogloss:1.03670\n",
      "[38]\tvalidation_0-mlogloss:1.05926\tvalidation_1-mlogloss:1.03543\n",
      "[39]\tvalidation_0-mlogloss:1.05869\tvalidation_1-mlogloss:1.03417\n",
      "[40]\tvalidation_0-mlogloss:1.05785\tvalidation_1-mlogloss:1.03289\n",
      "[41]\tvalidation_0-mlogloss:1.05716\tvalidation_1-mlogloss:1.03163\n",
      "[42]\tvalidation_0-mlogloss:1.05636\tvalidation_1-mlogloss:1.03038\n",
      "[43]\tvalidation_0-mlogloss:1.05564\tvalidation_1-mlogloss:1.02915\n",
      "[44]\tvalidation_0-mlogloss:1.05492\tvalidation_1-mlogloss:1.02794\n",
      "[45]\tvalidation_0-mlogloss:1.05419\tvalidation_1-mlogloss:1.02673\n",
      "[46]\tvalidation_0-mlogloss:1.05336\tvalidation_1-mlogloss:1.02553\n",
      "[47]\tvalidation_0-mlogloss:1.05268\tvalidation_1-mlogloss:1.02436\n",
      "[48]\tvalidation_0-mlogloss:1.05184\tvalidation_1-mlogloss:1.02320\n",
      "[49]\tvalidation_0-mlogloss:1.05123\tvalidation_1-mlogloss:1.02206\n",
      "[50]\tvalidation_0-mlogloss:1.05050\tvalidation_1-mlogloss:1.02092\n",
      "[51]\tvalidation_0-mlogloss:1.04990\tvalidation_1-mlogloss:1.01980\n",
      "[52]\tvalidation_0-mlogloss:1.04922\tvalidation_1-mlogloss:1.01869\n",
      "[53]\tvalidation_0-mlogloss:1.04843\tvalidation_1-mlogloss:1.01761\n",
      "[54]\tvalidation_0-mlogloss:1.04790\tvalidation_1-mlogloss:1.01651\n",
      "[55]\tvalidation_0-mlogloss:1.04716\tvalidation_1-mlogloss:1.01545\n",
      "[56]\tvalidation_0-mlogloss:1.04655\tvalidation_1-mlogloss:1.01438\n",
      "[57]\tvalidation_0-mlogloss:1.04595\tvalidation_1-mlogloss:1.01333\n",
      "[58]\tvalidation_0-mlogloss:1.04529\tvalidation_1-mlogloss:1.01230\n",
      "[59]\tvalidation_0-mlogloss:1.04487\tvalidation_1-mlogloss:1.01127\n",
      "[60]\tvalidation_0-mlogloss:1.04427\tvalidation_1-mlogloss:1.01025\n",
      "[61]\tvalidation_0-mlogloss:1.04362\tvalidation_1-mlogloss:1.00925\n",
      "[62]\tvalidation_0-mlogloss:1.04304\tvalidation_1-mlogloss:1.00826\n",
      "[63]\tvalidation_0-mlogloss:1.04237\tvalidation_1-mlogloss:1.00728\n",
      "[64]\tvalidation_0-mlogloss:1.04196\tvalidation_1-mlogloss:1.00630\n",
      "[65]\tvalidation_0-mlogloss:1.04142\tvalidation_1-mlogloss:1.00533\n",
      "[66]\tvalidation_0-mlogloss:1.04081\tvalidation_1-mlogloss:1.00438\n",
      "[67]\tvalidation_0-mlogloss:1.04031\tvalidation_1-mlogloss:1.00342\n",
      "[68]\tvalidation_0-mlogloss:1.03975\tvalidation_1-mlogloss:1.00251\n",
      "[69]\tvalidation_0-mlogloss:1.03926\tvalidation_1-mlogloss:1.00158\n",
      "[70]\tvalidation_0-mlogloss:1.03864\tvalidation_1-mlogloss:1.00067\n",
      "[71]\tvalidation_0-mlogloss:1.03823\tvalidation_1-mlogloss:0.99974\n",
      "[72]\tvalidation_0-mlogloss:1.03761\tvalidation_1-mlogloss:0.99883\n",
      "[73]\tvalidation_0-mlogloss:1.03691\tvalidation_1-mlogloss:0.99797\n",
      "[74]\tvalidation_0-mlogloss:1.03653\tvalidation_1-mlogloss:0.99706\n",
      "[75]\tvalidation_0-mlogloss:1.03611\tvalidation_1-mlogloss:0.99620\n",
      "[76]\tvalidation_0-mlogloss:1.03547\tvalidation_1-mlogloss:0.99532\n",
      "[77]\tvalidation_0-mlogloss:1.03506\tvalidation_1-mlogloss:0.99445\n",
      "[78]\tvalidation_0-mlogloss:1.03456\tvalidation_1-mlogloss:0.99363\n",
      "[79]\tvalidation_0-mlogloss:1.03395\tvalidation_1-mlogloss:0.99277\n",
      "[80]\tvalidation_0-mlogloss:1.03343\tvalidation_1-mlogloss:0.99193\n",
      "[81]\tvalidation_0-mlogloss:1.03297\tvalidation_1-mlogloss:0.99112\n",
      "[82]\tvalidation_0-mlogloss:1.03268\tvalidation_1-mlogloss:0.99028\n",
      "[83]\tvalidation_0-mlogloss:1.03205\tvalidation_1-mlogloss:0.98949\n",
      "[84]\tvalidation_0-mlogloss:1.03168\tvalidation_1-mlogloss:0.98870\n",
      "[85]\tvalidation_0-mlogloss:1.03120\tvalidation_1-mlogloss:0.98794\n",
      "[86]\tvalidation_0-mlogloss:1.03083\tvalidation_1-mlogloss:0.98714\n",
      "[87]\tvalidation_0-mlogloss:1.03036\tvalidation_1-mlogloss:0.98639\n",
      "[88]\tvalidation_0-mlogloss:1.02989\tvalidation_1-mlogloss:0.98560\n",
      "[89]\tvalidation_0-mlogloss:1.02939\tvalidation_1-mlogloss:0.98488\n",
      "[90]\tvalidation_0-mlogloss:1.02898\tvalidation_1-mlogloss:0.98411\n",
      "[91]\tvalidation_0-mlogloss:1.02857\tvalidation_1-mlogloss:0.98340\n",
      "[92]\tvalidation_0-mlogloss:1.02808\tvalidation_1-mlogloss:0.98267\n",
      "[93]\tvalidation_0-mlogloss:1.02764\tvalidation_1-mlogloss:0.98197\n",
      "[94]\tvalidation_0-mlogloss:1.02732\tvalidation_1-mlogloss:0.98123\n",
      "[95]\tvalidation_0-mlogloss:1.02697\tvalidation_1-mlogloss:0.98054\n",
      "[96]\tvalidation_0-mlogloss:1.02667\tvalidation_1-mlogloss:0.97983\n",
      "[97]\tvalidation_0-mlogloss:1.02616\tvalidation_1-mlogloss:0.97914\n",
      "[98]\tvalidation_0-mlogloss:1.02573\tvalidation_1-mlogloss:0.97847\n",
      "[99]\tvalidation_0-mlogloss:1.02544\tvalidation_1-mlogloss:0.97779\n",
      "Training accuracy 0.5656015037593985\n",
      "Testing accuracy 0.5245614035087719\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7, step=1),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, step=0.01),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 1, step=0.25),\n",
    "        \"reg_lambda\": trial.suggest_int(\"reg_lambda\", 0, 10, step=1),\n",
    "        \"scale_pos_weight\": trial.suggest_int(\"scale_pos_weight\", 1, 5, step=2),\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.5,\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(objective=\"multiclass\", **param_grid, use_label_encoder=False, eval_metric='mlogloss')\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)]\n",
    "        )\n",
    "    return model.score(X_val, y_val)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"XGBoost Classifier\")\n",
    "func = lambda trial: objective(trial, X, y)\n",
    "study.optimize(func, n_trials=20)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = xgb.XGBClassifier(objective=\"multiclass\", **study.best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
    "model.fit(X_train, y_train, eval_set=[(X_test,y_test), (X_train, y_train)])\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 AdaBoost\n",
    "<a name='section522'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimising hyperparameters for AdaBoost\n",
    "# First classified boosting algorithm\n",
    "\n",
    "# Sources used:\n",
    "# https://towardsdatascience.com/adaboost-from-scratch-37a936da3d50\n",
    "# https://analyticsindiamag.com/introduction-to-boosting-implementing-adaboost-in-python/\n",
    "# https://machinelearningmastery.com/adaboost-ensemble-in-python/\n",
    "\n",
    "# Hyperparameter types (Modified Y/N):\n",
    "# Num. of trees (Y)\n",
    "# Weak learner (N)\n",
    "# Learning rate (Y)\n",
    "# Alternate algorithm (Decision Tree/Logistic Regression)\n",
    "\n",
    "#First classified boosting algorithm\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define the model\n",
    "abc = AdaBoostClassifier()\n",
    "abc.fit(X_train, y_train)\n",
    "\n",
    "# grid = {\n",
    "#     \"n_estimators\": [10, 50, 100, 500],\n",
    "#     \"learning_rate\": [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "# }\n",
    "# abc_grid_cv = GridSearchCV(estimator=abc, param_grid = grid, n_jobs=-1, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "# abc_grid_cv.fit(X_train, y_train)\n",
    "\n",
    "# adc_pred = abc_grid_cv.predict(X_test)\n",
    "\n",
    "# print(\"Train: %f , Params: %s, Test: %f\" % (abc_grid_cv.best_score_, abc_grid_cv.best_params_, accuracy_score(y_test, adc_pred)))\n",
    "\n",
    "#Best score and parameters for decision tree round 1\n",
    "# Train: 0.525740 , Params: {'learning_rate': 0.001, 'n_estimators': 500}, Test: 0.521592\n",
    "\n",
    "# #Round 2\n",
    "# grid2 = {\n",
    "#     \"n_estimators\": [400, 500, 600, 800, 1000],\n",
    "#     \"learning_rate\": [0.0008, 0.001, 0.0012, 0.0014],\n",
    "# }\n",
    "# abc_grid_cv2 = GridSearchCV(estimator=abc, param_grid = grid2, n_jobs=-1, cv=3, scoring=\"accuracy\")\n",
    "# abc_grid_cv2.fit(X_train, y_train)\n",
    "# abc_pred2 = abc_grid_cv2.predict(X_test)\n",
    "# print(\"Train: %f , Params: %s, Test: %f\" % (abc_grid_cv2.best_score_, abc_grid_cv2.best_params_, accuracy_score(y_test, abc_pred2)))\n",
    "\n",
    "# #Best score and parameters for decision tree round 2\n",
    "# #Train: 0.525740 , Params: {'learning_rate': 0.0008, 'n_estimators': 400}, Test: 0.521592\n",
    "# #No improvement use as final values\n",
    "\n",
    "# # Try with logistic regression algorithm\n",
    "# # abc_lr = AdaBoostClassifier(base_estimator=LogisticRegression(solver = 'liblinear',penalty = 'l1', C = 0.01))\n",
    "# # abc_lr.fit(X_train, y_train)\n",
    "\n",
    "# # grid = {\n",
    "# #     \"n_estimators\": [10, 50, 100, 500],\n",
    "# #     \"learning_rate\": [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "# # }\n",
    "# # abc_lr_grid_cv = GridSearchCV(estimator=abc_lr, param_grid = grid, n_jobs=-1, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "# # abc_lr_grid_cv.fit(X_train, y_train)\n",
    "\n",
    "# # adc_lr_pred = abc_lr_grid_cv.predict(X_test)\n",
    "\n",
    "# # print(\"Train: %f , Params: %s, Test: %f\" % (abc_lr_grid_cv.best_score_, abc_lr_grid_cv.best_params_, accuracy_score(y_test, adc_lr_pred)))\n",
    "\n",
    "# #Output\n",
    "# # Train: 0.489301 , Params: {'learning_rate': 0.1, 'n_estimators': 500}, Test: 0.486505\n",
    "\n",
    "# #Use round 2 decision tree output as predictor\n",
    "# abc_final = AdaBoostClassifier(n_estimators=400, learning_rate=0.0008)\n",
    "\n",
    "#with optuna\n",
    "def objective_abc(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 1000, step=200),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.0008, 0.0014, step=0.0002),\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model = AdaBoostClassifier(**param_grid)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "        )\n",
    "        preds = model.predict_proba(X_test)\n",
    "        cv_scores[idx] = log_loss(y_test, preds)\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"AdaBoost\")\n",
    "func = lambda trial: objective_abc(trial, X, y)\n",
    "study.optimize(func, n_trials=8)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = AdaBoostClassifier(**study.best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))\n",
    "\n",
    "#model is slow - directly define model\n",
    "# \tBest value (rmse): 1.02205\n",
    "# \tBest params:\n",
    "# \t\tn_estimators: 400\n",
    "# \t\tlearning_rate: 0.0008\n",
    "# Training accuracy 0.5020676691729323\n",
    "# Testing accuracy 0.5184210526315789\n",
    "\n",
    "abc_final = AdaBoostClassifier(n_estimators=400, learning_rate=0.0008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 GradientBoost\n",
    "<a name='section523'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimising hyperparameters for GradientBoost\n",
    "\n",
    "# Sources used:\n",
    "# https://www.datasciencelearner.com/gradient-boosting-hyperparameters-tuning/\n",
    "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "#define the model\n",
    "gbc = GradientBoostingClassifier(subsample = 0.8)\n",
    "gbc.fit(X_train, y_train)\n",
    "gbc_pre_pred = gbc.predict(X_test)\n",
    "print(\"Pretuning Test: %f\" % accuracy_score(y_test, gbc_pre_pred))\n",
    "\n",
    "# grid = {\n",
    "#     \"n_estimators\":[5,50,250,500],\n",
    "#     \"max_depth\":[5,6],\n",
    "#     \"learning_rate\":[0.1],\n",
    "#     \"min_samples_split\":[40]\n",
    "# }\n",
    "\n",
    "# gbc_grid_cv = GridSearchCV(estimator=gbc, param_grid = grid, n_jobs=-1, cv=3, scoring=\"accuracy\")\n",
    "# gbc_grid_cv.fit(X_train, y_train)\n",
    "# gbc_pred = gbc_grid_cv.predict(X_test)\n",
    "# print(\"Train: %f , Params: %s, Test: %f\" % (gbc_grid_cv.best_score_, gbc_grid_cv.best_params_, accuracy_score(y_test, gbc_pred)))\n",
    "\n",
    "# Outputs\n",
    "# Train: 0.515040 , Params: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 40, 'n_estimators': 5}, Test: 0.522267\n",
    "\n",
    "#Round 2\n",
    "# grid2 = {\n",
    "#     \"n_estimators\":[3,5,7,9],\n",
    "#     \"max_depth\":[5],\n",
    "#     \"learning_rate\":[0.05,0.1,0.2],\n",
    "#     \"min_samples_split\":[30,40,50]\n",
    "# }\n",
    "\n",
    "# gbc_grid_cv2 = GridSearchCV(estimator=gbc, param_grid = grid2, n_jobs=-1, cv=3, scoring=\"accuracy\")\n",
    "# gbc_grid_cv2.fit(X_train, y_train)\n",
    "# gbc_pred2 = gbc_grid_cv2.predict(X_test)\n",
    "# print(\"Train: %f , Params: %s, Test: %f\" % (gbc_grid_cv2.best_score_, gbc_grid_cv2.best_params_, accuracy_score(y_test, gbc_pred2)))\n",
    "\n",
    "# # Outputs\n",
    "# # Train: 0.525448 , Params: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 50, 'n_estimators': 7}, Test: 0.528340\n",
    "\n",
    "# gbc_final = GradientBoostingClassifier(\n",
    "#     subsample = 0.8, \n",
    "#     learning_rate = 0.1, \n",
    "#     max_depth = 5, \n",
    "#     min_samples_split = 50, \n",
    "#     n_estimators = 7\n",
    "# )\n",
    "\n",
    "#with optuna\n",
    "def objective_gbc(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"subsample\": trial.suggest_categorical(\"subsample\", [0.8]),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 3, 9, step=3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 6),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.1]),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 30, 50, step=5),\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model = GradientBoostingClassifier(**param_grid)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "        )\n",
    "        preds = model.predict_proba(X_test)\n",
    "        cv_scores[idx] = log_loss(y_test, preds)\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"GradientBoost\")\n",
    "func = lambda trial: objective_gbc(trial, X, y)\n",
    "study.optimize(func, n_trials=8)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = GradientBoostingClassifier(**study.best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))\n",
    "\n",
    "gbc_final = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 LightGBM\n",
    "<a name='section524'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "# https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\n",
    "\n",
    "import lightgbm\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 0.2, 0.9, step=0.1\n",
    "        ),\n",
    "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 0.2, 0.9, step=0.1\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    model = lightgbm.LGBMClassifier(objective=\"multiclass\", **param_grid)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"multi_logloss\",\n",
    "        early_stopping_rounds=100,\n",
    "        )\n",
    "    return model.score(X_val, y_val)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\n",
    "func = lambda trial: objective(trial, X, y)\n",
    "study.optimize(func, n_trials=20)\n",
    "\n",
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "model = lightgbm.LGBMClassifier(objective=\"multiclass\", **study.best_params)\n",
    "model.fit(X_train, y_train, eval_set=[(X_test,y_test), (X_train, y_train)], eval_metric='multi_logloss')\n",
    "\n",
    "print('Training accuracy ' + str(model.score(X_train, y_train)))\n",
    "print('Testing accuracy ' + str(model.score(X_test, y_test)))\n",
    "\n",
    "#Rename for results graph\n",
    "lightgbm_final = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Neural Network Models\n",
    "<a name='section53'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Standard Neural Network\n",
    "<a name='section531'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Standard, baseline NN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#need to stop randomization\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "#Rename to vanilla\n",
    "model = Sequential()\n",
    "#need to replace input shape by X shape\n",
    "model.add(layers.Dense(128, input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation('linear'))\n",
    "model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val_categorical))\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Vanilla Neural Network\n",
    "<a name='section532'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparam tuning for vanilla nn\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(trial.suggest_categorical('n_nodes', [32, 64, 128, 256]), input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation', ['relu', 'linear', 'tanh'])))\n",
    "    model.add(Dropout(0.1), )\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train, y_train_categorical, batch_size=50, epochs=5, validation_split=0.1, shuffle=False)\n",
    "    return model.evaluate(X_val, y_val_categorical)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Standard, baseline NN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#need to stop randomization\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "#Rename to vanilla\n",
    "model = Sequential()\n",
    "#need to replace input shape by X shape\n",
    "model.add(layers.Dense(ridge_params['n_nodes'], input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation(ridge_params['activation']))\n",
    "model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=ridge_params['optimizer'], metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val_categorical))\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Deep Neural Network\n",
    "<a name='section533'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Deep NN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#need to stop randomization\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "#Rename to vanilla\n",
    "model = Sequential()\n",
    "#need to replace input shape by X shape\n",
    "model.add(layers.Dense(128, input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation('relu'))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(64, input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation('relu'))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(32, input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation('relu'))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparam tuning for deep nn\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(trial.suggest_categorical('n_nodes1', [32, 64, 128, 256]), input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation1', ['relu', 'linear', 'tanh'])))\n",
    "    model.add(layers.Dense(trial.suggest_categorical('n_nodes2', [32, 64, 128, 256]), input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation2', ['relu', 'linear', 'tanh'])))\n",
    "    model.add(layers.Dense(trial.suggest_categorical('n_nodes3', [32, 64, 128, 256]), input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation3', ['relu', 'linear', 'tanh'])))\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train, y_train_categorical, batch_size=50, epochs=5, validation_split=0.1, shuffle=False)\n",
    "    return model.evaluate(X_val, y_val_categorical)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Deep NN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#need to stop randomization\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "#Rename to vanilla\n",
    "model = Sequential()\n",
    "#need to replace input shape by X shape\n",
    "model.add(layers.Dense(ridge_params['n_nodes1'], input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation(ridge_params['activation1']))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(ridge_params['n_nodes2'], input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation(ridge_params['activation2']))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(ridge_params['n_nodes3'], input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Activation(ridge_params['activation3']))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=ridge_params['optimizer'], metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ROC curves\n",
    "\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "n_classes = 3\n",
    "\n",
    "y_score = model.predict_proba(X_test)\n",
    "y_test_categorical = keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_categorical[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_categorical.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Process of plotting roc-auc curve belonging to all classes.\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for all classes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Precision Recall curve\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test_categorical[:, i], y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test_categorical[:, i], y_score[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n",
    "    y_test_categorical.ravel(), y_score.ravel()\n",
    ")\n",
    "average_precision[\"micro\"] = average_precision_score(y_test_categorical, y_score, average=\"micro\")\n",
    "\n",
    "_, ax = plt.subplots(figsize=(17, 8))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines, labels = [], []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "    plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "display = PrecisionRecallDisplay(\n",
    "    recall=recall[\"micro\"],\n",
    "    precision=precision[\"micro\"],\n",
    "    average_precision=average_precision[\"micro\"],\n",
    ")\n",
    "display.plot(ax=ax, name=\"Micro-average precision-recall\", color=\"gold\")\n",
    "    \n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[i],\n",
    "        precision=precision[i],\n",
    "        average_precision=average_precision[i],\n",
    "    )\n",
    "    display.plot(ax=ax, name=f\"Precision-recall for class {i}\", color=color)\n",
    "    \n",
    "# add the legend for the iso-f1 curves\n",
    "handles, labels = display.ax_.get_legend_handles_labels()\n",
    "handles.extend([l])\n",
    "labels.extend([\"iso-f1 curves\"])\n",
    "# set the legend and the axes\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.legend(handles=handles, labels=labels, loc=\"best\")\n",
    "ax.set_title(\"Precision-Recall Curves for all classes\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 Recurrent Neural Network\n",
    "<a name='section534'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating RNN #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, SimpleRNN\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "# X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "# X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(128, input_shape = (10, 15), return_sequences = True))\n",
    "model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 20)\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "print(preds.shape)\n",
    "preds = preds.reshape(1140, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparam tuning for rnn\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(trial.suggest_categorical('n_nodes', [32, 64, 128, 256]), input_shape = (10, 15), return_sequences = True))\n",
    "    model.add(Dropout(trial.suggest_categorical('dropout', [0.2, 0.4, 0.6, 0.8])), )\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation', ['sigmoid', 'softmax'])))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer', ['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train_test, y_train_test, epochs = 20, validation_split=0.1, shuffle=False)\n",
    "    return model.evaluate(X_val_test, y_val_categorical_test)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating RNN #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, SimpleRNN\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "# X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "# X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(ridge_params['n_nodes'], input_shape = (10, 15), return_sequences = True))\n",
    "model.add(Dropout(ridge_params['dropout']), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation(ridge_params['activation']))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = ridge_params['optimizer'], metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 20)\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "print(preds.shape)\n",
    "preds = preds.reshape(1140, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.5 Gated Recurrent Neural Network\n",
    "<a name='section535'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating GRU #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GRU\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "# X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "# X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(GRU(128, input_shape = (10, 15), return_sequences = True))\n",
    "model.add(Dropout(0.4), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 20)\n",
    "\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "preds = preds.reshape(1140, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparam tuning for gru\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GRU(trial.suggest_categorical('n_nodes', [32, 64, 128, 256]), input_shape = (10, 15), return_sequences = True))\n",
    "    model.add(Dropout(trial.suggest_categorical('dropout', [0.2, 0.4, 0.6, 0.8])), )\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation', ['sigmoid', 'softmax'])))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train_test, y_train_test, epochs = 20, validation_split=0.1, shuffle=False)\n",
    "    return model.evaluate(X_val_test, y_val_categorical_test)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating GRU #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GRU\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "# X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "# X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(GRU(ridge_params['n_nodes'], input_shape = (10, 15), return_sequences = True))\n",
    "model.add(Dropout(ridge_params['dropout']), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation(ridge_params['activation']))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = ridge_params['optimizer'], metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 20)\n",
    "\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "preds = preds.reshape(1140, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.6 Long Short-Term Memory Neural Network\n",
    "<a name='section536'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating LSTM\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "\n",
    "# X_train_test = np.reshape(X_train, (266, 13, 15))\n",
    "\n",
    "# y_train_test = np.reshape(y_train_categorical, (266, 13, 3))\n",
    "\n",
    "# X_test_test = X_test.reshape(114, 13, 15)\n",
    "\n",
    "# X_train_lstm, y_train_categorical_lstm = lstm_data_transform(X_train, y_train_categorical, num_steps=5)\n",
    "# X_train_lstm = np.array(X_train_lstm).reshape(-1, 5, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical_lstm).reshape(-1, 5, y_train_categorical.shape[1])\n",
    "\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape = (10, 15), return_sequences = True))\n",
    "model.add(Dropout(0.4), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "#change fit to be like rnn and gru?\n",
    "#set shuffle to false\n",
    "# add validation set here?\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 20, batch_size = 100)\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "preds = preds.reshape(1140, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparam tuning for LSTM\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(trial.suggest_categorical('n_nodes', [32, 64, 128, 256]), input_shape = (10, 15), return_sequences = True))\n",
    "    model.add(Dropout(trial.suggest_categorical('dropout', [0.2, 0.4, 0.6, 0.8])), )\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation', ['sigmoid', 'softmax'])))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train_test, y_train_test, epochs = 50, batch_size = 100, shuffle=False)\n",
    "    return model.evaluate(X_val_test, y_val_categorical_test)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating LSTM\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "\n",
    "# X_train_test = np.reshape(X_train, (266, 13, 15))\n",
    "\n",
    "# y_train_test = np.reshape(y_train_categorical, (266, 13, 3))\n",
    "\n",
    "# X_test_test = X_test.reshape(114, 13, 15)\n",
    "\n",
    "# X_train_lstm, y_train_categorical_lstm = lstm_data_transform(X_train, y_train_categorical, num_steps=5)\n",
    "# X_train_lstm = np.array(X_train_lstm).reshape(-1, 5, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical_lstm).reshape(-1, 5, y_train_categorical.shape[1])\n",
    "\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(ridge_params['n_nodes'], input_shape = (10, 15), return_sequences = True))\n",
    "model.add(Dropout(ridge_params['dropout']), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation(ridge_params['activation']))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = ridge_params['optimizer'], metrics = ['accuracy'])\n",
    "\n",
    "#change fit to be like rnn and gru?\n",
    "#set shuffle to false\n",
    "# add validation set here?\n",
    "history = model.fit(X_train_test, y_train_test, epochs = 50, batch_size = 100)\n",
    "\n",
    "preds = model.predict(X_test_test)\n",
    "preds = preds.reshape(1140, 3)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.7 Convolutional Neural Network\n",
    "<a name='section537'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating CNN #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "# X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "# X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "# input_shape = (1, X_train.shape[1])\n",
    "model.add(Conv1D(128, kernel_size=X_train.shape[1], input_shape=(None, 1)))\n",
    "#should there be an activation here?\n",
    "# model.add(Conv1D(64, kernel_size=X_train.shape[1], input_shape=(None, 1)))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train_test_cnn, epochs = 20)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=2)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparam tuning for cnn\n",
    "\n",
    "#trial activation function\n",
    "#trial optimiser\n",
    "#trial number of neurons\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install optuna\n",
    "import optuna\n",
    "\n",
    "def tune(objective):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best score: {best_score}\\n\")\n",
    "    print(f\"Optimized parameters: {params}\\n\")\n",
    "    return params\n",
    "\n",
    "history = []\n",
    "\n",
    "def ridge_objective(trial):\n",
    "#     clear_session()\n",
    "    #learning rate?\n",
    "    #batch size?\n",
    "    #num epochs\n",
    "    #final activation\n",
    "    #dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(trial.suggest_categorical('n_nodes', [32, 64, 128, 256]), kernel_size=X_train.shape[1], input_shape=(None, 1)))\n",
    "    #add activation here\n",
    "    model.add(layers.Dense(3))\n",
    "    model.add(layers.Activation(trial.suggest_categorical('activation', ['sigmoid', 'softmax'])))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=trial.suggest_categorical('optimizer',['adam','rmsprop','adagrad', 'sgd']), metrics=['accuracy'])\n",
    "    \n",
    "#     stopping = EarlyStopping(monitor='val_acc', patience=50)\n",
    "    #what does using validation_split or validation_data do here exactly?\n",
    "    history = model.fit(X_train, y_train_test_cnn, epochs = 20 , validation_split=0.1, shuffle=False)\n",
    "    return model.evaluate(X_val, y_val_test_cnn)[1]\n",
    "\n",
    "ridge_params = tune(ridge_objective)\n",
    "#this is a dictionary : Optimized parameters: {'n_nodes': 128, 'activation': 'linear', 'optimizer': 'adagrad'}\n",
    "#can just extract params to make new model -> consider deleting old model or placing it after this\n",
    "# ridge = Ridge(**ridge_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating CNN #######rename variables appropriately\n",
    "np.random.seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D\n",
    "\n",
    "batch_size = 1\n",
    "# batch_size = 100\n",
    "# X_train_lstm = np.array(X_train).reshape(-1, batch_size, X_train.shape[1])\n",
    "# y_train_categorical_lstm = np.array(y_train_categorical).reshape(-1, batch_size, 3)\n",
    "# X_test_lstm = np.array(X_test).reshape(-1, batch_size, X_train.shape[1])\n",
    "#Rename to LSTM\n",
    "model = Sequential()\n",
    "# input_shape = (1, X_train.shape[1])\n",
    "model.add(Conv1D(ridge_params['n_nodes'], kernel_size=X_train.shape[1], input_shape=(None, 1)))\n",
    "#should there be an activation here?\n",
    "# model.add(Conv1D(64, kernel_size=X_train.shape[1], input_shape=(None, 1)))\n",
    "# model.add(Dropout(0.1), )\n",
    "model.add(layers.Dense(3))\n",
    "model.add(layers.Activation(ridge_params['activation']))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = ridge_params['optimizer'], metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train_test_cnn, epochs = 20)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=2)\n",
    "accuracy = accuracy_score(y_test, preds) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Time Series Models\n",
    "<a name='section54'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 Prophet\n",
    "<a name='section541'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet\n",
    "# https://machinelearningmastery.com/time-series-forecasting-with-prophet-in-python/\n",
    "\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
    "\n",
    "def objective(trial, df):\n",
    "    seasonality = ['additive', 'multiplicative']\n",
    "    param_grid = {\n",
    "        \"changepoint_prior_scale\": trial.suggest_uniform(\"changepoint_prior_scale\", 0.001, 0.5),\n",
    "        \"seasonality_prior_scale\": trial.suggest_uniform(\"seasonality_prior_scale\", 0.01, 10),\n",
    "        \"seasonality_mode\": seasonality[trial.suggest_int(\"seasonality_mode\", 0, 1)]\n",
    "    }\n",
    "\n",
    "    m = Prophet(**param_grid)\n",
    "    m.fit(df)\n",
    "    df_cv = cross_validation(m, \n",
    "                             initial='3458 days', \n",
    "                             period='13 days', \n",
    "                             horizon = '13 days',\n",
    "                             parallel=\"processes\")\n",
    "    \n",
    "    df_p = performance_metrics(df_cv, rolling_window=1)\n",
    "    \n",
    "    return df_p['rmse'].values[0]\n",
    "\n",
    "\n",
    "ds = pd.to_datetime(data[\"Date\"])\n",
    "ds = ds.iloc[380:] # chop off first season to match current data\n",
    "\n",
    "encoder = LabelEncoder().fit(y)\n",
    "\n",
    "prophet_df = pd.DataFrame()\n",
    "prophet_df[\"ds\"] = ds\n",
    "prophet_df[\"y\"] = encoder.transform(y)\n",
    "\n",
    "#study = optuna.create_study()\n",
    "#func = lambda trial: objective(trial, prophet_df)\n",
    "#study.optimize(func, n_trials=20)\n",
    "\n",
    "#print(f\"\\tBest value (rmse): {study.best_value}\")\n",
    "\n",
    "#print(f\"\\tBest params:\")\n",
    "#for key, value in study.best_params.items():\n",
    "#    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "#Best value (rmse): 0.8722181615529774\n",
    "#Best params:\n",
    "#changepoint_prior_scale: 0.3742218538948863\n",
    "#seasonality_prior_scale: 0.05051527961102975\n",
    "#seasonality_mode: 1 (multiplicative)\n",
    "\n",
    "best_params = {\n",
    "    \"changepoint_prior_scale\": 0.3742218538948863,\n",
    "    \"seasonality_prior_scale\": 0.05051527961102975,\n",
    "    \"seasonality_mode\": 'multiplicative'\n",
    "}\n",
    "\n",
    "model = Prophet(**best_params)\n",
    "\n",
    "# 3458 is the equivalent of 266 seasons if we define a season as 13 matches and reserve the remaining 114 seasons for testing\n",
    "ds_train = prophet_df.head(3458)\n",
    "ds_test = prophet_df.tail(len(prophet_df) - 3458)\n",
    "\n",
    "model.fit(ds_train)\n",
    "\n",
    "future = ds_test.drop(columns=[\"y\"])\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "#print(np.rint(forecast[\"yhat\"]))\n",
    "accuracy = accuracy_score(ds_test[\"y\"], np.rint(forecast[\"yhat\"]))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 Arima\n",
    "<a name='section542'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arima\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "import optuna\n",
    "import pmdarima as pm\n",
    "\n",
    "def objective(trial, X):\n",
    "    param_grid = (\n",
    "        trial.suggest_int(\"p\", 1, 3),\n",
    "        0,\n",
    "        trial.suggest_int(\"q\", 1, 3)\n",
    "    )\n",
    "    \n",
    "    param_grid_seasonal = (\n",
    "        trial.suggest_int(\"P\", 0, 3),\n",
    "        1,\n",
    "        trial.suggest_int(\"Q\", 0, 3),\n",
    "        12\n",
    "    )\n",
    "\n",
    "    train_size = int(len(X) * 0.70)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = SARIMAX(history, order=param_grid, seasonal_order=param_grid_seasonal)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    # calculate out of sample error\n",
    "    rmse = sqrt(mean_squared_error(test, predictions))\n",
    "    return rmse\n",
    "\n",
    "def evaluate_arima_model(X, arima_order):\n",
    "    # prepare training dataset\n",
    "    train_size = int(len(X) * 0.66)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    # calculate out of sample error\n",
    "    rmse = sqrt(mean_squared_error(test, predictions))\n",
    "    return rmse\n",
    "\n",
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    dataset = dataset.astype('float32')\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    rmse = evaluate_arima_model(dataset, order)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, order\n",
    "                    print('ARIMA%s RMSE=%.3f' % (order,rmse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "def get_accuracy(X):\n",
    "    train_size = int(len(X) * 0.70)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = SARIMAX(history)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(round(yhat))\n",
    "        history.append(test[t])\n",
    "    return accuracy_score(test, predictions)\n",
    "\n",
    "\n",
    "idx = data['Date']\n",
    "idx = idx.iloc[380:] # chop off first season to match current data\n",
    "encoder = LabelEncoder().fit(y)\n",
    "y = encoder.transform(y)\n",
    "ts = pd.Series(y, index=idx).astype('float32')\n",
    "\n",
    "#p_values = [0, 1, 2, 4, 6, 8, 10]\n",
    "#d_values = range(0, 3)\n",
    "#q_values = range(0, 3)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#evaluate_models(ts.values, p_values, d_values, q_values)\n",
    "\n",
    "#study = optuna.create_study()\n",
    "#func = lambda trial: objective(trial, ts.values)\n",
    "#study.optimize(func, n_trials=10)\n",
    "\n",
    "#print(f\"\\tBest value (rmse): {study.best_value}\")\n",
    "\n",
    "#print(f\"\\tBest params:\")\n",
    "#for key, value in study.best_params.items():\n",
    "#    print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "#smodel = pm.auto_arima(ts.values, start_p=1, start_q=1,\n",
    "#                         test='adf',\n",
    "#                         max_p=3, max_q=3, m=12,\n",
    "#                         start_P=0, seasonal=True,\n",
    "#                         d=None, D=1, trace=True,\n",
    "#                         error_action='ignore',  \n",
    "#                         suppress_warnings=True, \n",
    "#                         stepwise=True)\n",
    "\n",
    "#print(smodel.summary())\n",
    "\n",
    "# Summary:\n",
    "# Optuna - took 2 hours and didn't return any hyperparameters (never finished)\n",
    "# pmd.auto_arima - ran out of memory\n",
    "# Standard grid search - same as Optuna\n",
    "\n",
    "print(get_accuracy(ts))\n",
    "\n",
    "# Accuracy as of 2021-12-19 is 0.27149122807017545"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_aPyBUwqMAD"
   },
   "source": [
    "## 6. Results\n",
    "<a name='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "#### Standard Models \n",
    "##### LR\n",
    "```\n",
    "{'C': 1.0, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "```\n",
    "##### GNB\n",
    "```\n",
    "No Tuning\n",
    "```\n",
    "##### SVM\n",
    "```\n",
    "{'C': 1.0, 'gamma': 'scale', 'kernel': 'rbf'}\n",
    "```\n",
    "#### Boosting Models\n",
    "##### XGB\n",
    "```\n",
    "final_cl = xgb.XGBClassifier( \n",
    "    colsample_bytree=0.5,\n",
    "    subsample=0.8,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=1,\n",
    "    reg_lambda=0,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "```\n",
    "##### ABC\n",
    "```\n",
    "{'learning_rate': 0.0008, 'n_estimators': 400}\n",
    "```\n",
    "##### GBC\n",
    "```\n",
    "{'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 50, 'n_estimators': 7}\n",
    "```\n",
    "#### NN Models\n",
    "##### Vanilla NN\n",
    "##### DNN\n",
    "##### GRU\n",
    "##### LTSM\n",
    "##### RNN\n",
    "##### CNN\n",
    "#### Time Series Modules\n",
    " TO BE INVESTIGATED\n",
    "### Methodology\n",
    "1. Define Models\n",
    "2. Calculate Predictions for Test Data\n",
    "3. Extract Classification Report for each Model\n",
    "4. Plot results as bar charts\n",
    "5. 1 bar chart for each classification plot output\n",
    "### Definitions\n",
    "- The recall means \"how many of this class you find over the whole number of elements of this class\"\n",
    "- The precision will be \"how many are correctly classified among that class\"\n",
    "- The f1-score is the harmonic mean between precision & recall\n",
    "- The support is the number of occurence of the given class in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define NN models with hyperparams here\n",
    "#Follow naming scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Models Training Accuracy##\n",
    "gnb_vis_pred = gnb.fit(X_train, y_train).predict(X_train)\n",
    "svm_vis_pred = SVC(C=1.0, gamma='scale', kernel='rbf').fit(X_train, y_train).predict(X_train)\n",
    "lr_vis_pred = LogisticRegression(C=1.0, penalty='l1', solver='liblinear').fit(X_train, y_train).predict(X_train)\n",
    "\n",
    "## Boosting Models ##\n",
    "xgb_vis_pred = final_cl.predict(X_train)\n",
    "abc_vis_pred = abc_final.fit(X_train, y_train).predict(X_train)\n",
    "gbc_vis_pred = gbc_final.fit(X_train, y_train).predict(X_train)\n",
    "lightgbm_vis_pred = lightgbm_final.predict(X_train)\n",
    "\n",
    "## NN Models ##\n",
    "# nn_vis_pred = nn_final.predict(X_test)\n",
    "# vnn_vis_pred = vnn_final.predict(X_test)\n",
    "# dnn_vis_pred = dnn_final.predict(X_test)\n",
    "# rnn_vis_pred = rnn_final.predict(X_test)\n",
    "# gru_vis_pred = gru_final.predict(X_test)\n",
    "# ltsm_vis_pred = ltsm_final.predict(X_test)\n",
    "# cnn_vis_pred = cnn_final.predict(X_test)\n",
    "\n",
    "## Time Series Modules ##\n",
    "# prophet_vis_pred = prophet_final.predict(X_test)\n",
    "# arima_vis_pred = arima_final.predict(X_test)\n",
    "\n",
    "#Save results as dict\n",
    "lr_cr = classification_report(y_train, lr_vis_pred, output_dict=True)\n",
    "gnb_cr = classification_report(y_train, gnb_vis_pred, output_dict=True)\n",
    "svm_cr = classification_report(y_train, svm_vis_pred, output_dict=True)\n",
    "xgb_cr = classification_report(y_train, xgb_vis_pred, output_dict=True)\n",
    "abc_cr = classification_report(y_train, abc_vis_pred, output_dict=True)\n",
    "gbc_cr = classification_report(y_train, gbc_vis_pred, output_dict=True)\n",
    "lightgbm_cr = classification_report(y_train, lightgbm_vis_pred, output_dict=True)\n",
    "# nn_cr = classification_report(y_train, nn_vis_pred, output_dict=True)\n",
    "# vnn_cr = classification_report(y_train, vnn_vis_pred, output_dict=True)\n",
    "# dnn_cr = classification_report(y_train, dnn_vis_pred, output_dict=True)\n",
    "# rnn_cr = classification_report(y_train, rnn_vis_pred, output_dict=True)\n",
    "# gru_cr = classification_report(y_train, gru_vis_pred, output_dict=True)\n",
    "# ltsm_cr = classification_report(y_train, ltsm_vis_pred, output_dict=True)\n",
    "# cnn_cr = classification_report(y_train, cnn_vis_pred, output_dict=True)\n",
    "# prophet_cr = classification_report(y_train, prophet_vis_pred, output_dict=True)\n",
    "# arima_cr = classification_report(y_train, arima_vis_pred, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names of models for columns\n",
    "models = ['GNB','SVM','LR','XGB','ABC','GBC','L-GBM']\n",
    "# For after fix\n",
    "# models = ['GNB','SVM','LR','XGB','ABC','GBC','L-GBM','NN','VNN','DNN','RNN','GRU','LTSM','CNN','Prophet','Arima']\n",
    "\n",
    "#Plot of models by accuracy\n",
    "accuracy = [gnb_cr[\"accuracy\"],\n",
    "            svm_cr[\"accuracy\"],\n",
    "            lr_cr[\"accuracy\"],\n",
    "            xgb_cr[\"accuracy\"],\n",
    "            abc_cr[\"accuracy\"],\n",
    "            gbc_cr[\"accuracy\"],\n",
    "            lightgbm_cr[\"accuracy\"]]\n",
    "# For after fix\n",
    "# accuracy = [gnb_cr[\"accuracy\"],\n",
    "#             svm_cr[\"accuracy\"],\n",
    "#             lr_cr[\"accuracy\"],\n",
    "#             xgb_cr[\"accuracy\"],\n",
    "#             abc_cr[\"accuracy\"],\n",
    "#             gbc_cr[\"accuracy\"],\n",
    "#             lightgbm_cr[\"accuracy\"],\n",
    "#             nn_cr[\"accuracy\"],\n",
    "#             vnn_cr[\"accuracy\"],\n",
    "#             dnn_cr[\"accuracy\"],\n",
    "#             rnn_cr[\"accuracy\"],\n",
    "#             gru_cr[\"accuracy\"],\n",
    "#             ltsm_cr[\"accuracy\"],\n",
    "#             cnn_cr[\"accuracy\"],\n",
    "#             prophet_cr[\"accuracy\"],\n",
    "#             arima_cr[\"accuracy\"]]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "plt.bar(models, accuracy)\n",
    "ax.set_title('Training Accuracy of Different Models')\n",
    "ax.set_yticks([0.40, 0.5, 0.6])\n",
    "ax.set_ylim([0.40, 0.6])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Standard Models Test Accuracy##\n",
    "gnb_vis_pred = y_gnb\n",
    "svm_vis_pred = SVC(C=1.0, gamma='scale', kernel='rbf').fit(X_train, y_train).predict(X_test)\n",
    "lr_vis_pred = LogisticRegression(C=1.0, penalty='l1', solver='liblinear').fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# ## Boosting Models ##\n",
    "xgb_vis_pred = final_cl.predict(X_test)\n",
    "abc_vis_pred = abc_final.fit(X_train, y_train).predict(X_test)\n",
    "gbc_vis_pred = gbc_final.fit(X_train, y_train).predict(X_test)\n",
    "lightgbm_vis_pred = lightgbm_final.predict(X_test)\n",
    "\n",
    "## NN Models ##\n",
    "# nn_vis_pred = nn_final.predict(X_test)\n",
    "# vnn_vis_pred = vnn_final.predict(X_test)\n",
    "# dnn_vis_pred = dnn_final.predict(X_test)\n",
    "# rnn_vis_pred = rnn_final.predict(X_test)\n",
    "# gru_vis_pred = gru_final.predict(X_test)\n",
    "# ltsm_vis_pred = ltsm_final.predict(X_test)\n",
    "# cnn_vis_pred = cnn_final.predict(X_test)\n",
    "\n",
    "## Time Series Modules ##\n",
    "# prophet_vis_pred = prophet_final.predict(X_test)\n",
    "# arima_vis_pred = arima_final.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, lr_vis_pred, output_dict=True))\n",
    "\n",
    "#Save results as dict\n",
    "lr_cr = classification_report(y_test, lr_vis_pred, output_dict=True)\n",
    "gnb_cr = classification_report(y_test, gnb_vis_pred, output_dict=True)\n",
    "svm_cr = classification_report(y_test, svm_vis_pred, output_dict=True)\n",
    "xgb_cr = classification_report(y_test, xgb_vis_pred, output_dict=True)\n",
    "abc_cr = classification_report(y_test, abc_vis_pred, output_dict=True)\n",
    "gbc_cr = classification_report(y_test, gbc_vis_pred, output_dict=True)\n",
    "lightgbm_cr = classification_report(y_test, lightgbm_vis_pred, output_dict=True)\n",
    "# nn_cr = classification_report(y_test, nn_vis_pred, output_dict=True)\n",
    "# vnn_cr = classification_report(y_test, vnn_vis_pred, output_dict=True)\n",
    "# dnn_cr = classification_report(y_test, dnn_vis_pred, output_dict=True)\n",
    "# rnn_cr = classification_report(y_test, rnn_vis_pred, output_dict=True)\n",
    "# gru_cr = classification_report(y_test, gru_vis_pred, output_dict=True)\n",
    "# ltsm_cr = classification_report(y_test, ltsm_vis_pred, output_dict=True)\n",
    "# cnn_cr = classification_report(y_test, cnn_vis_pred, output_dict=True)\n",
    "# prophet_cr = classification_report(y_test, prophet_vis_pred, output_dict=True)\n",
    "# arima_cr = classification_report(y_test, arima_vis_pred, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names of models for columns\n",
    "models = ['GNB','SVM','LR','XGB','ABC','GBC','L-GBM']\n",
    "# For after fix\n",
    "# models = ['GNB','SVM','LR','XGB','ABC','GBC','L-GBM','NN','VNN','DNN','RNN','GRU','LTSM','CNN','Prophet','Arima']\n",
    "\n",
    "#Plot of models by accuracy\n",
    "accuracy = [gnb_cr[\"accuracy\"],\n",
    "            svm_cr[\"accuracy\"],\n",
    "            lr_cr[\"accuracy\"],\n",
    "            xgb_cr[\"accuracy\"],\n",
    "            abc_cr[\"accuracy\"],\n",
    "            gbc_cr[\"accuracy\"],\n",
    "            lightgbm_cr[\"accuracy\"]]\n",
    "# For after fix\n",
    "# accuracy = [gnb_cr[\"accuracy\"],\n",
    "#             svm_cr[\"accuracy\"],\n",
    "#             lr_cr[\"accuracy\"],\n",
    "#             xgb_cr[\"accuracy\"],\n",
    "#             abc_cr[\"accuracy\"],\n",
    "#             gbc_cr[\"accuracy\"],\n",
    "#             lightgbm_cr[\"accuracy\"],\n",
    "#             nn_cr[\"accuracy\"],\n",
    "#             vnn_cr[\"accuracy\"],\n",
    "#             dnn_cr[\"accuracy\"],\n",
    "#             rnn_cr[\"accuracy\"],\n",
    "#             gru_cr[\"accuracy\"],\n",
    "#             ltsm_cr[\"accuracy\"],\n",
    "#             cnn_cr[\"accuracy\"],\n",
    "#             prophet_cr[\"accuracy\"],\n",
    "#             arima_cr[\"accuracy\"]]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "plt.bar(models, accuracy)\n",
    "ax.set_title('Testing Accuracy of Different Models')\n",
    "ax.set_yticks([0.45, 0.5, 0.55])\n",
    "ax.set_ylim([0.45, 0.55])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "#Plot of models by weighted avg f1 score\n",
    "accuracy = [gnb_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            svm_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            lr_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            xgb_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            abc_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            gbc_cr[\"weighted avg\"][\"f1-score\"],\n",
    "            lightgbm_cr[\"weighted avg\"][\"f1-score\"]]\n",
    "# For after fix\n",
    "# accuracy = [gnb_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             svm_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             lr_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             xgb_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             abc_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             gbc_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             lightgbm_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             nn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             vnn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             dnn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             rnn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             gru_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             ltsm_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             cnn_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             prophet_cr[\"weighted avg\"][\"f1-score\"],\n",
    "#             arima_cr[\"weighted avg\"][\"f1-score\"]]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "plt.bar(models, accuracy)\n",
    "ax.set_title('Weighted F1-Score of Different Models')\n",
    "ax.set_yticks([0.35, 0.4, 0.45])\n",
    "ax.set_ylim([0.35, 0.45])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Weighted F1-Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Models Training Accuracy##\n",
    "gnb_vis_pred = gnb.fit(X_train, y_train).predict(X_train)\n",
    "svm_vis_pred = SVC(C=1.0, gamma='scale', kernel='rbf').fit(X_train, y_train).predict(X_train)\n",
    "lr_vis_pred = LogisticRegression(C=1.0, penalty='l1', solver='liblinear').fit(X_train, y_train).predict(X_train)\n",
    "\n",
    "## Boosting Models ##\n",
    "xgb_vis_pred = final_cl.predict(X_train)\n",
    "abc_vis_pred = abc_final.fit(X_train, y_train).predict(X_train)\n",
    "gbc_vis_pred = gbc_final.fit(X_train, y_train).predict(X_train)\n",
    "lightgbm_vis_pred = lightgbm_final.predict(X_train)\n",
    "\n",
    "#Save results as dict\n",
    "lr_cr = classification_report(y_train, lr_vis_pred, output_dict=True)\n",
    "gnb_cr = classification_report(y_train, gnb_vis_pred, output_dict=True)\n",
    "svm_cr = classification_report(y_train, svm_vis_pred, output_dict=True)\n",
    "xgb_cr = classification_report(y_train, xgb_vis_pred, output_dict=True)\n",
    "abc_cr = classification_report(y_train, abc_vis_pred, output_dict=True)\n",
    "gbc_cr = classification_report(y_train, gbc_vis_pred, output_dict=True)\n",
    "lightgbm_cr = classification_report(y_train, lightgbm_vis_pred, output_dict=True)\n",
    "\n",
    "#Collective Base Training Accuracy \n",
    "base_cr = lr_cr[\"accuracy\"] + gnb_cr[\"accuracy\"] + svm_cr[\"accuracy\"]\n",
    "base_cr = base_cr/3\n",
    "print(base_cr)\n",
    "\n",
    "#Collective Boost Training Accuracy \n",
    "boost_cr = xgb_cr[\"accuracy\"] + abc_cr[\"accuracy\"] + gbc_cr[\"accuracy\"] + lightgbm_cr[\"accuracy\"]\n",
    "boost_cr = boost_cr/4\n",
    "print(boost_cr)\n",
    "\n",
    "# #Collective NN Training Accuracy \n",
    "# boost_cr = xgb_cr[\"accuracy\"] + abc_cr[\"accuracy\"] + gbc_cr[\"accuracy\"] + lightgbm_cr[\"accuracy\"]\n",
    "# boost_cr = boost_cr/4\n",
    "# print(boost_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names of models for columns\n",
    "models = ['Base','Boost']\n",
    "# For after fix\n",
    "#models = ['Base','Boost','NN']\n",
    "\n",
    "\n",
    "#Plot of models by accuracy\n",
    "accuracy = [base_cr,\n",
    "            boost_cr]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "plt.bar(models, accuracy, width=0.8)\n",
    "ax.set_title('Cumulative Training Accuracy of Models')\n",
    "ax.set_yticks([0.40, 0.5, 0.60])\n",
    "ax.set_ylim([0.40, 0.60])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Models Training Accuracy##\n",
    "gnb_vis_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "svm_vis_pred = SVC(C=1.0, gamma='scale', kernel='rbf').fit(X_train, y_train).predict(X_test)\n",
    "lr_vis_pred = LogisticRegression(C=1.0, penalty='l1', solver='liblinear').fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "## Boosting Models ##\n",
    "xgb_vis_pred = final_cl.predict(X_test)\n",
    "abc_vis_pred = abc_final.fit(X_train, y_train).predict(X_test)\n",
    "gbc_vis_pred = gbc_final.fit(X_train, y_train).predict(X_test)\n",
    "lightgbm_vis_pred = lightgbm_final.predict(X_test)\n",
    "\n",
    "#Save results as dict\n",
    "lr_cr = classification_report(y_test, lr_vis_pred, output_dict=True)\n",
    "gnb_cr = classification_report(y_test, gnb_vis_pred, output_dict=True)\n",
    "svm_cr = classification_report(y_test, svm_vis_pred, output_dict=True)\n",
    "xgb_cr = classification_report(y_test, xgb_vis_pred, output_dict=True)\n",
    "abc_cr = classification_report(y_test, abc_vis_pred, output_dict=True)\n",
    "gbc_cr = classification_report(y_test, gbc_vis_pred, output_dict=True)\n",
    "lightgbm_cr = classification_report(y_test, lightgbm_vis_pred, output_dict=True)\n",
    "\n",
    "#Collective Base Training Accuracy \n",
    "base_cr = lr_cr[\"accuracy\"] + gnb_cr[\"accuracy\"] + svm_cr[\"accuracy\"]\n",
    "base_cr = base_cr/3\n",
    "print(base_cr)\n",
    "\n",
    "#Collective Boost Training Accuracy \n",
    "boost_cr = xgb_cr[\"accuracy\"] + abc_cr[\"accuracy\"] + gbc_cr[\"accuracy\"] + lightgbm_cr[\"accuracy\"]\n",
    "boost_cr = boost_cr/4\n",
    "print(boost_cr)\n",
    "\n",
    "# #Collective NN Training Accuracy \n",
    "# boost_cr = xgb_cr[\"accuracy\"] + abc_cr[\"accuracy\"] + gbc_cr[\"accuracy\"] + lightgbm_cr[\"accuracy\"]\n",
    "# boost_cr = boost_cr/4\n",
    "# print(boost_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names of models for columns\n",
    "models = ['Base','Boost']\n",
    "# For after fix\n",
    "#models = ['Base','Boost','NN']\n",
    "\n",
    "\n",
    "#Plot of models by accuracy\n",
    "accuracy = [base_cr,\n",
    "            boost_cr]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "plt.bar(models, accuracy, width=0.8)\n",
    "ax.set_title('Cumulative Testing Accuracy of Models')\n",
    "ax.set_yticks([0.40, 0.5, 0.60])\n",
    "ax.set_ylim([0.40, 0.60])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLot confusion matrix of all models\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def confusion_plt (model):\n",
    "    cm = confusion_matrix(y_test, model)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['A','D','H'])\n",
    "    disp.plot(cmap='Reds');\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# Plot all confusion plots choose best few models for display\n",
    "confusion_plt(lr_vis_pred)\n",
    "confusion_plt(gnb_vis_pred)\n",
    "confusion_plt(svm_vis_pred)\n",
    "confusion_plt(xgb_vis_pred)\n",
    "confusion_plt(abc_vis_pred)\n",
    "confusion_plt(gbc_vis_pred)\n",
    "confusion_plt(lightgbm_vis_pred)\n",
    "# confusion_plt(nn_vis_pred)\n",
    "# confusion_plt(vnn_vis_pred)\n",
    "# confusion_plt(dnn_vis_pred)\n",
    "# confusion_plt(rnn_vis_pred)\n",
    "# confusion_plt(gru_vis_pred)\n",
    "# confusion_plt(ltsm_vis_pred)\n",
    "# confusion_plt(cnn_vis_pred)\n",
    "# confusion_plt(prophet_vis_pred)\n",
    "# confusion_plt(arima_vis_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot best models from each category\n",
    "def output_model_comp (output):\n",
    "    best_model_comp = pd.DataFrame([['Precision', \n",
    "                                     gnb_cr[output]['precision'], \n",
    "                                     gnb_cr[output]['precision'], \n",
    "                                     gnb_cr[output]['precision'], \n",
    "                                     gnb_cr[output]['precision']], \n",
    "                                    ['Recall', \n",
    "                                     gnb_cr[output]['recall'], \n",
    "                                     gnb_cr[output]['recall'], \n",
    "                                     gnb_cr[output]['recall'], \n",
    "                                     gnb_cr[output]['recall']], \n",
    "                                    ['F1-Score',\n",
    "                                     gnb_cr[output][\"f1-score\"], \n",
    "                                     gnb_cr[output][\"f1-score\"], \n",
    "                                     gnb_cr[output][\"f1-score\"], \n",
    "                                     gnb_cr[output][\"f1-score\"]]], \n",
    "                  columns=['Model', 'Base', 'Boost', 'Neural Net','Time Series'])\n",
    "    return best_model_comp\n",
    "                                    \n",
    "output_model_comp('2').plot(x='Model', kind='bar', stacked=False, title='Grouped Bar Graph of Win')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2XDY1oOAyEZ"
   },
   "source": [
    "## 7. Final Predictions on Test Set\n",
    "<a name='section7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Data Cleaning\n",
    "<a name='section71'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up-to-date data of season 2021 is named as 'updatedData' \n",
    "# Test set is named as 'testData'\n",
    "\n",
    "# only pick the columns that are presented in the training set, drop the others\n",
    "updatedData = updatedData[data.columns.tolist()]\n",
    "\n",
    "# check if there are invalid data\n",
    "assert updatedData.shape[0] == removeInvalidData(updatedData).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Priors Derivation \n",
    "<a name='section72'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the up-to-date data of season 2021 and the test set\n",
    "final_data = pd.concat([updatedData,testData],ignore_index=True,sort=False)\n",
    "# convert different date formats into test set format\n",
    "convertDate(final_data)\n",
    "#construct priors for test set\n",
    "final_data = DerivePriors(final_data)\n",
    "final_data"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CW1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
